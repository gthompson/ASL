{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montserrat event selector for Machine Learning\n",
    "The aim of this code is to find the best N events of each type, and create a corresponding CSV file and data structure for entry into Alexis' and Marielle's AAA codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy.core import read\n",
    "import sys\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "LIBpath = os.path.join( os.getenv('HOME'),'src','kitchensinkGT', 'LIB')\n",
    "sys.path.append(LIBpath)\n",
    "from libseisGT import add_to_trace_history #, mulplt\n",
    "from modutils import yn_choice\n",
    "\n",
    "from obspy import read_inventory #, remove_response\n",
    "from libMVO import fix_trace_id, inventory_fix_id_mvo, load_mvo_inventory\n",
    "\"\"\"\n",
    "#%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "# Use 3 decimal places in output display\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "# Don't wrap repr(DataFrame) across additional lines\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "# Set max rows displayed in output to 25\n",
    "pd.set_option(\"display.max_rows\", 25)\n",
    "\n",
    "np.set_printoptions(linewidth=160)\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "\n",
    "def parse_STATION0HYP(station0hypfile):\n",
    "    \"\"\" file sample\n",
    "    012345678901234567890123456 \n",
    "      MWNH1644.53N 6211.45W 407\n",
    "      MWNL1644.53N 6211.45W 407\n",
    "      MWZH1644.53N 6211.45W 407\n",
    "      MWZL1644.53N 6211.45W 407\n",
    "      0123456789012345678901234 \n",
    "    \"\"\"\n",
    "    station_locations = []\n",
    "    if os.path.exists(station0hypfile):\n",
    "        fptr = open(station0hypfile,'r')\n",
    "        for line in fptr:\n",
    "            line = line.strip()\n",
    "            if len(line)==25:\n",
    "                station = line[0:4]\n",
    "                latdeg = line[4:6]\n",
    "                if latdeg != '16':\n",
    "                    print(line)\n",
    "                    continue\n",
    "                latmin = line[6:8]\n",
    "                latsec = line[9:11]\n",
    "                hemisphere = line[11]\n",
    "                if hemisphere == 'N':\n",
    "                    latsign = 1\n",
    "                elif hemisphere == 'S':\n",
    "                    latsign = -1 \n",
    "                else:\n",
    "                    print(line)\n",
    "                    continue\n",
    "                londeg = line[13:15]\n",
    "                lonmin = line[15:17]\n",
    "                lonsec = line[18:20]\n",
    "                lonsign = 1\n",
    "                if line[20]=='W':\n",
    "                    lonsign = -1\n",
    "                elev = line[21:25].strip()\n",
    "                station_dict = {}\n",
    "                station_dict['name'] = station\n",
    "                station_dict['lat'] = (float(latdeg) + float(latmin)/60 + float(latsec)/3600) * latsign\n",
    "                station_dict['lon'] = (float(londeg) + float(lonmin)/60 + float(lonsec)/3600) * lonsign\n",
    "                station_dict['elev'] = float(elev)\n",
    "                \n",
    "                station_locations.append(station_dict)\n",
    "        fptr.close()\n",
    "        return pd.DataFrame(station_locations)\n",
    "\n",
    "def add_station_locations(st, station_locationsDF):\n",
    "    for tr in st:\n",
    "        df = station_locationsDF[station_locationsDF['name'] == tr.stats.station]\n",
    "        df = df.reset_index()\n",
    "        if len(df.index)==1:\n",
    "            row = df.iloc[0]\n",
    "            tr.stats['lat'] = row['lat']\n",
    "            tr.stats['lon'] = row['lon']\n",
    "            tr.stats['elev'] = row['elev']\n",
    "        else:\n",
    "            print('Found %d matching stations' % len(index))\n",
    "            \n",
    "def plot_amplitude_locations(st):\n",
    "    plt.figure()\n",
    "    x = []\n",
    "    y = []\n",
    "    s = []\n",
    "    for tr in st:\n",
    "        if 'lon' in tr.stats:\n",
    "            x.append(tr.stats.lon)\n",
    "            y.append(tr.stats.lat)\n",
    "            s.append(np.max(np.absolute(tr.data)))\n",
    "    if len(s)>0:\n",
    "        maxs = np.max(s)\n",
    "        s = s/maxs * 50\n",
    "        plt.scatter(x, y, s);\n",
    "    \n",
    "        volcano_dict = {}\n",
    "        volcano_dict['name']='VOLC'\n",
    "        volcano_dict['lat']=16.7166638\n",
    "        volcano_dict['lon']=-62.1833326\n",
    "        volcano_dict['elev']=-62.1833326\n",
    "        plt.scatter(volcano_dict['lon'], volcano_dict['lat'], 200, marker='*')\n",
    "        plt.show();\n",
    "\n",
    "def deconvolve_instrument_response(st):\n",
    "    for tr in st:\n",
    "        this_inv = None\n",
    "        need_to_correct = False\n",
    "        if 'units' in tr.stats:\n",
    "            if 'units'=='Counts':\n",
    "                need_to_correct = True\n",
    "        else:\n",
    "            need_to_correct = True\n",
    "        if need_to_correct:           \n",
    "            if bool_correct_data: # try to find corresponding station XML\n",
    "                this_inv = load_mvo_inventory(tr, '/Users/thompsong/DATA/MVO/CAL')\n",
    "        if this_inv and tr.stats['units'] == 'Counts':\n",
    "            tr.remove_response(inventory=this_inv, output=\"VEL\")\n",
    "            tr.stats['units'] = 'm/s'\n",
    "            add_to_trace_history(tr, 'deconvolved')\n",
    "        #else:\n",
    "        #    st.remove(tr)\n",
    "    \n",
    "\n",
    "def read_volcano_def():\n",
    "    filepath = './volcano_def.csv'\n",
    "    subclass_df = pd.read_csv(filepath)\n",
    "    subclass_df.columns = subclass_df.columns.str.lstrip()\n",
    "    return subclass_df\n",
    "    \n",
    "def build_master_event_catalog(csvdir, seisandbname, catalogfile, subclasses_for_ML, max_duration = 60):\n",
    "    # load all the year/month CSV files\n",
    "    csvfiles = glob(os.path.join(csvdir, 'reawav_%s??????.csv' % seisandbname))\n",
    "    frames = []\n",
    "    for csvfile in csvfiles:\n",
    "        df = pd.read_csv(csvfile)\n",
    "        frames.append(df) \n",
    "    dfall = pd.concat(frames, sort=True)\n",
    "    dfall.set_index('filetime', inplace=True) # we will need this later to remerge\n",
    "    dfall.sort_index(inplace=True)\n",
    "    \"\"\"\n",
    "    for index, row in dfall.iterrows():\n",
    "        \n",
    "        # For simplicity, copy 'D' and 'R' mainclass to subclass\n",
    "        if row['mainclass'] in ['D', 'R']: # Do I need L here too?\n",
    "            dfall.loc[index, 'subclass']=row['mainclass']\n",
    "    \"\"\" \n",
    "    # replace loop above\n",
    "    for mainclass in ['R', 'D']:\n",
    "        dfall.loc[dfall['mainclass'] == mainclass, 'subclass'] = mainclass\n",
    "    \n",
    "    # Drop the mainclass column, as it is now superfluous.\n",
    "    dfall.drop(columns=['mainclass'], inplace=True)\n",
    "    \n",
    "    # Add an etype column\n",
    "    #dfall['etype'] = dfall['subclass'].replace(subclasses, etypes)\n",
    "    \n",
    "    # Add columns to assign a percentage for each subclass\n",
    "    for subclass in subclasses_for_ML:\n",
    "        dfall[subclass] = 0\n",
    "    \n",
    "    # But set column for actual subclass to 100%  \n",
    "    \"\"\"\n",
    "    for index, row in dfall.iterrows():            \n",
    "        # Set dfall['h'] to 100 if dfall['subclass']=='h', etc\n",
    "        dfall.loc[index, row['subclass']] = 100        \n",
    "    \"\"\"\n",
    "    # replace row operations above\n",
    "    for subclass in subclasses_for_ML:\n",
    "        dfall.loc[dfall['subclass'] == subclass, subclass] = 100\n",
    "        \n",
    "    # Add a new_subclass column\n",
    "    dfall['new_subclass'] = dfall['subclass']\n",
    "\n",
    "    # Add weight column. I will give really clear events higher weight when I process them\n",
    "    dfall['weight']=3 # weight for events I have not visually checked\n",
    "    \n",
    "    # Add column that records if event is checked\n",
    "    dfall['checked']=False\n",
    "    \n",
    "    # Add column that records if event is marked for splitting\n",
    "    dfall['split']=False    \n",
    "    \n",
    "    # Add column that records if event is marked for deletion\n",
    "    dfall['delete']=False\n",
    "    \n",
    "    # Add column that records if event should be ignored\n",
    "    # Ignore any events longer than 1-minute, as they are likely to contain multiple events \n",
    "    # or just be unhelpful for classifying short signals which are more common\n",
    "    dfall['ignore'] = dfall['twin']>max_duration\n",
    "    \n",
    "    # Now we have a catalog dataframe we can work with. Let's save this.\n",
    "    dfall.to_csv(catalogfile)\n",
    "    \n",
    "    return dfall\n",
    "\n",
    "def _count_by_subclass(df):\n",
    "    checked = df[df['checked']==True]\n",
    "    unchecked = df[df['checked']==False]\n",
    "    \n",
    "    print('Event counts:')\n",
    "    \n",
    "    if len(checked.index)>0:\n",
    "        print('Checked events: %d' % len(checked.index) )\n",
    "        checked_by_subclass = checked.groupby(\"new_subclass\")\n",
    "        print(checked_by_subclass['path'].count())\n",
    "        \n",
    "    if len(unchecked.index)>0:\n",
    "        print('Unchecked events: %d' % len(unchecked.index) )\n",
    "        unchecked_by_subclass = unchecked.groupby(\"new_subclass\")\n",
    "        print(unchecked_by_subclass['path'].count()) \n",
    "\n",
    "    print('Events by weight / quality threshold')\n",
    "    print(df.groupby('weight')['path'].count())\n",
    "\n",
    "        \n",
    "def _select_best_events(df, allowed_subclasses, N=100, exclude_checked=True):\n",
    "    # When we are iterating, we want to exclude the checked events, else we are repeating our work.\n",
    "    # When we return our final list, we do not want to exclude checked events\n",
    "    all_subclasses = df['new_subclass'].unique()\n",
    "    best_events_dict = {}\n",
    "    for subclass in all_subclasses:\n",
    "        if subclass in allowed_subclasses:\n",
    "            #print('\\nProcessing subclass=%s' % subclass)\n",
    "            is_subclass =  df['new_subclass']==subclass\n",
    "            df_subclass = df[is_subclass]\n",
    "            \n",
    "            # mechanism to weight out checked events\n",
    "            df_subclass['include'] = 1 - df_subclass['ignore']\n",
    "            if exclude_checked:\n",
    "                print('Excluding checked events')\n",
    "                df_subclass['include'] = df_subclass['include'] * (1 - df_subclass['checked'])\n",
    "            \n",
    "            if len(df_subclass.index)>0:\n",
    "                # we use three criteria for ranking events. detection_quality has the largest magnitude, but can be missing, so we also add snr, \n",
    "                # and finally quality as a tie-braker, since it has a small range for events that have made it this far\n",
    "                df_subclass['sortcol'] = df_subclass['quality'] # always present\n",
    "                if 'snr' in df_subclass.columns:\n",
    "                    df_subclass['sortcol'] = df_subclass['sortcol'] + df_subclass['snr']\n",
    "                if 'detection_quality' in df_subclass.columns:\n",
    "                    df_subclass['sortcol'] = df_subclass['sortcol'] + df_subclass['detection_quality']                    \n",
    "                df_subclass['sortcol'] = df_subclass['sortcol'] * df_subclass['weight'] * df_subclass['include']\n",
    "                df_subclass.drop(columns=['include'], inplace=True)\n",
    "\n",
    "                L = len(df_subclass.index)\n",
    "                H = int(min([L, N]))\n",
    "                print('Selecting %d events of type %s from a total of %d' % (H, subclass, L))\n",
    "                df_subclass.sort_values(by=['sortcol'], ascending=False, inplace=True)\n",
    "                df_subclass.drop(columns=['sortcol'], inplace=True)\n",
    "                df_subclass = df_subclass.head(H)\n",
    "                if 'bandratio_[1.0_6.0_11.0]' in df_subclass.columns:\n",
    "                    df.rename(columns = {'bandratio_[1.0_6.0_11.0]':'band_ratio'}, inplace = True)\n",
    "                best_events_dict[subclass]=df_subclass\n",
    "\n",
    "    return best_events_dict \n",
    "\n",
    "\n",
    "\n",
    "def get_weighted_fingerprints(dfall, subclasses_for_ML, N=300, exclude_checked=False):\n",
    "\n",
    "    fingerprints = {}\n",
    "    best_events_dict = _select_best_events(dfall, subclasses_for_ML, N=N, exclude_checked=exclude_checked)\n",
    "    for subclass in subclasses_for_ML:\n",
    "        if subclass in best_events_dict.keys(): \n",
    "            print('Computing fingerprint for subclass ',subclass)\n",
    "            thisdf = best_events_dict[subclass]\n",
    "            if len(thisdf.index)>30:\n",
    "                statsdf = pd.DataFrame()\n",
    "                statsdf['statistic'] = ['mean', 'std', '25%', '50%', '75%']\n",
    "                statsdf.set_index(['statistic'], inplace = True)\n",
    "\n",
    "                for col in [ 'peaktime', 'kurtosis', 'medianF', 'peakF', 'bw_min', 'bw_max', 'band_ratio']:\n",
    "                    # compute mean, std, median, 25% percentile, 75% percentile\n",
    "                    wdf = DescrStatsW(thisdf[col], weights=thisdf['weight'].astype(float)*thisdf[subclass].astype(float), ddof=1)\n",
    "                    p = [0.25,0.50,0.75]\n",
    "                    q  = wdf.quantile(p) \n",
    "                    statsdf.loc['mean', col] = wdf.mean\n",
    "                    statsdf.loc['std', col] = wdf.std\n",
    "                    statsdf.loc['50%', col] = q[p[1]]\n",
    "                    statsdf.loc['25%', col] = q[p[0]]\n",
    "                    statsdf.loc['75%', col] = q[p[2]]\n",
    "\n",
    "                fingerprints[subclass] = statsdf\n",
    "                #print(fingerprints[subclass])\n",
    "    return fingerprints    \n",
    "\n",
    "def get_fingerprints(dfall, allowed_subclasses, N=300, exclude_checked=True):\n",
    "    \"\"\"\n",
    "    All we do right now is a dataframe describe, so we return the stats of each column.\n",
    "    \n",
    "    wdf = DescrStatsW(df.x, weights=df.wt, ddof=1) \n",
    "    \n",
    "    \"\"\"\n",
    "    #df = best_events_dict.groupby(\"subclass\")\n",
    "    fingerprints = {}\n",
    "    best_events_dict = _select_best_events(dfall, allowed_subclasses, N=N, exclude_checked=exclude_checked)\n",
    "    for subclass in allowed_subclasses:\n",
    "        if subclass in best_events_dict.keys(): \n",
    "            print('Computing fingerprint for subclass ',subclass)\n",
    "            #df_subclass = df.get_group(subclass)\n",
    "            df_subclass = best_events_dict[subclass]     \n",
    "            fingerprints[subclass] = df_subclass[[ 'peaktime', \n",
    "                'kurtosis', 'medianF', 'peakF', 'bw_min', 'bw_max', 'band_ratio']].describe()\n",
    "            #print(fingerprints[subclass])\n",
    "    return fingerprints\n",
    "\n",
    "def save_fingerprints(fingerprints, allowed_subclasses):\n",
    "    for subclass in allowed_subclasses:\n",
    "        if subclass in fingerprints.keys(): \n",
    "            fingerprints[subclass].to_csv('fingerprint_%s.csv' % subclass)\n",
    "  \n",
    "def _merge_dataframes(df_dict, accepted_subclasses):\n",
    "    frames = []\n",
    "    for subclass in accepted_subclasses:\n",
    "        if subclass in df_dict.keys():\n",
    "            frames.append(df_dict[subclass]) \n",
    "    return pd.concat(frames, sort=True)     \n",
    "\n",
    "def _guess_subclass(row, fingerprints, subclasses_for_ML):\n",
    "    chance_of = {}\n",
    "    for item in subclasses_for_ML:\n",
    "        chance_of[item]=0.0\n",
    "    \n",
    "    params = ['peaktime', 'kurtosis', 'medianF', 'peakF', 'bw_min', 'bw_max', 'band_ratio']\n",
    "    for subclass in fingerprints.keys():\n",
    "        fingerprint = fingerprints[subclass]\n",
    "        #fingerprint.reset_index(inplace=True)\n",
    "        #print(fingerprint.columns)\n",
    "        for param in params:\n",
    "            thisval = row[param]\n",
    "            \n",
    "            # test against mean+/-std\n",
    "            meanval = fingerprint[param]['mean']\n",
    "            stdval = fingerprint[param]['std']\n",
    "            minus1sigma = meanval - stdval\n",
    "            plus1sigma = meanval + stdval\n",
    "            \n",
    "            if thisval > minus1sigma and thisval < plus1sigma:\n",
    "                weight = 1.0 - abs(thisval-meanval)/stdval\n",
    "                chance_of[subclass] += weight\n",
    "                \n",
    "            # test against 25-75% percentile\n",
    "            medianval = fingerprint[param]['50%']\n",
    "            val25 = fingerprint[param]['25%']\n",
    "            val75 = fingerprint[param]['75%']\n",
    "            if thisval > val25 and thisval < val75:\n",
    "                if thisval < medianval:\n",
    "                    weight = 1.0 - (medianval-thisval)/(medianval-val25)\n",
    "                else:\n",
    "                    weight = 1.0 - (thisval-medianval)/(val75-medianval)\n",
    "                chance_of[subclass] += weight\n",
    "    \n",
    "    print('The event is classified as %s, but here are our guesses:' % row['subclass'])\n",
    "    total = 0\n",
    "    for subclass in chance_of.keys():\n",
    "        total += chance_of[subclass]\n",
    "    for subclass in chance_of.keys():\n",
    "        if total>0:\n",
    "            print('subclass: %s, points = %f, probability = %3.0f%%' % (subclass, chance_of[subclass], 100*chance_of[subclass]/total))\n",
    "\n",
    "\n",
    "def qc_best_events(best_events_dict, seisan_subclasses, subclasses_for_ML, fingerprints):\n",
    "    \n",
    "    for subclass in subclasses_for_ML:\n",
    "        #if subclass=='u' or subclass=='n': # we don't care about these events\n",
    "        #    continue\n",
    "        #print('Processing ',subclass)\n",
    "        if subclass in best_events_dict.keys():\n",
    "            df = best_events_dict[subclass]\n",
    "            print('Processing %d events of type %s' % (len(df.index), subclass) )\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                #picklepath = os.path.join(SEISAN_DATA, row.path.replace('WAV','PICKLE') + '.pickle')\n",
    "                picklepath = os.path.join(SEISAN_DATA, row.path.replace('./WAV','PICKLE') + '.pickle')\n",
    "                print('Loading %s' % picklepath)\n",
    "                if os.path.exists(picklepath):\n",
    "                    \n",
    "                    # plot whole event file\n",
    "                    st = read(picklepath) #.select(station='MBWH')\n",
    "                    st.filter('bandpass', freqmin=0.5, freqmax=25.0, corners=4)\n",
    "                    deconvolve_instrument_response(st)\n",
    "                    \n",
    "                    # compute ampeng to show later\n",
    "                    traceids = []\n",
    "                    energies = []\n",
    "                    amplitudes = []\n",
    "                    for tr in st:\n",
    "                        amp = np.max(np.absolute(tr.data))\n",
    "                        energy = np.sum(np.square(tr.data))/tr.stats.sampling_rate\n",
    "                        traceids.append(tr.id)\n",
    "                        amplitudes.append(amp)\n",
    "                        energies.append(energy)\n",
    "                    dfenergy = pd.DataFrame()\n",
    "                    dfenergy['traceID']=traceids\n",
    "                    dfenergy['amplitude']=amplitudes\n",
    "                    dfenergy['energy']=energies\n",
    "                    dfenergy.sort_values(by=['amplitude'],ascending=False,inplace=True)                    \n",
    "                    \n",
    "                    \n",
    "                    # plot all\n",
    "                    st.plot(equal_scale=False);\n",
    "                                      \n",
    "                    # also plot a fixed 30-seconds around the peaktime\n",
    "                    starttime = st[0].stats.starttime + row['peaktime']-10\n",
    "                    endtime = starttime + 20\n",
    "                    st.trim(starttime=starttime, endtime=endtime, pad=True, fill_value=None)\n",
    "                    st.plot(equal_scale=False)                  \n",
    "                    \n",
    "                    # Show a map of amplitude distribution\n",
    "                    add_station_locations(st, station_locationsDF)\n",
    "                    plot_amplitude_locations(st)\n",
    "                        \n",
    "                    # station amplitudes and energies\n",
    "                    print(' ')\n",
    "                    print(dfenergy)\n",
    "                    \n",
    "                    # pickle file\n",
    "                    csvpath = picklepath.replace('.pickle', '.csv')\n",
    "                    tracedf = pd.read_csv(csvpath)\n",
    "                    if 'bandratio_[1.0_6.0_11.0]' in tracedf.columns:\n",
    "                        tracedf.rename(columns = {'bandratio_[1.0_6.0_11.0]':'band_ratio'}, inplace = True)\n",
    "                    tracedf.sort_values(by=['peakamp'], ascending=False, inplace=True)\n",
    "                    print(tracedf[['id', 'medianF', 'bw_min', 'peakF', 'bw_max', 'band_ratio', 'kurtosis']])\n",
    "                    _guess_subclass(row, fingerprints, subclasses_for_ML)\n",
    "                    \n",
    "                    # Input\n",
    "                    checked = False\n",
    "                    print('Please reclassify the event.')\n",
    "                    print('Valid subclasses are: ', seisan_subclasses )\n",
    "                    print('To enter percentage probabilities, e.g. 75% l, 25%h, enter l, 75, h, 25')\n",
    "                    print('Optionally add a weight [0-9] too with a trailing integer, e.g. l, 75,  h, 25, 5')\n",
    "                    print('Or:\\n\\ts = mark event for splitting')\n",
    "                    print('\\td = mark event for deletion')\n",
    "                    print('\\ti = ignore event')\n",
    "                    print('\\tq = quit reclassifying this subclass %s' % subclass)\n",
    "                    \n",
    "                    try:                       \n",
    "                        new_subclass = input('\\t ?') \n",
    "                        if not new_subclass:\n",
    "                            new_subclass = subclass\n",
    "                        if new_subclass == 'q': \n",
    "                            break\n",
    "                        if new_subclass == 's':\n",
    "                              df.loc[index, 'split'] = True\n",
    "                              checked = True\n",
    "                        if new_subclass == 'i':\n",
    "                              df.loc[index, 'ignore'] = True \n",
    "                              checked = True\n",
    "                        if new_subclass == 'd':\n",
    "                              df.loc[index, 'delete'] = True \n",
    "                              checked = True\n",
    "                        if not checked:                         \n",
    "                            if not ',' in new_subclass: # convert to a subclass, percentage string\n",
    "                                new_subclass = new_subclass + ', 100'\n",
    "                            spl = new_subclass.split(',') # split string to subclass probability list \n",
    "                            if len(spl) % 2 == 1:\n",
    "                                df.loc[index, 'weight'] = int(spl.pop())\n",
    "                            spd = {spl[a]:spl[a + 1] for a in range(0, len(spl), 2)} # subclass probability dict\n",
    "                            for key in subclasses_for_ML:\n",
    "                                if key in spd.keys():\n",
    "                                    df.loc[index, key] = int(spd[key])\n",
    "                                else:\n",
    "                                    df.loc[index, key] = 0\n",
    "                            keymax = max(spd, key=spd.get)\n",
    "                            df.loc[index, 'new_subclass']=keymax  \n",
    "                            checked = True\n",
    "                        if checked:\n",
    "                            df.loc[index, 'checked']=True\n",
    "                    except:\n",
    "                        print('Input may have been faulty. Skipping event')\n",
    "                        pass\n",
    "                        \n",
    "                    \n",
    "            best_events_dict[subclass] = df\n",
    "    #return _merge_dataframes(best_events_dict, best_events_dict.keys())\n",
    "    return best_events_dict\n",
    "\n",
    "def remove_marked_events(df):  \n",
    "    dfsubset = df\n",
    "    n_all = len(dfsubset.index)\n",
    "    dfsubset = dfsubset[dfsubset['delete']==False]\n",
    "    n_after_delete = len(dfsubset.index)\n",
    "    dfsubset = dfsubset[dfsubset['ignore']==False]\n",
    "    n_after_ignore = len(dfsubset.index)\n",
    "    dfsubset = dfsubset[dfsubset['split']==False]\n",
    "    n_after_split = len(dfsubset.index)\n",
    "    n_delete = n_all - n_after_delete\n",
    "    n_ignore = n_after_delete - n_after_ignore\n",
    "    n_split = n_after_ignore - n_after_split\n",
    "    print('Removed events: Marked to:')\n",
    "    print('- split ', n_split)\n",
    "    print('- delete ', n_delete)\n",
    "    print('- ignore ', n_ignore)\n",
    "    print('Catalog down from %d to %d events' % (n_all, n_after_split))\n",
    "    print(' ')  \n",
    "    return dfsubset\n",
    "\n",
    "def to_AAA(df, subclasses_for_ML, outfile, ignore_extra_columns=True, make_local_archive=True):\n",
    "    \"\"\"\n",
    "    create output file for AAA\n",
    "    \"\"\" \n",
    "    print(' ')\n",
    "    print('Here is the updated list of events by subclass and whether they have been checked:')\n",
    "    _count_by_subclass(df)\n",
    "    print(' ')    \n",
    "\n",
    "    print('*** Choose labelled events for input to supervised machine learning. ***')\n",
    "    minweight = int(input('Minimum weight (signal quality, 0-9): '))\n",
    "    if minweight>0:\n",
    "        df = df[df['weight']>=minweight]\n",
    "    #choice = input('Include only checked events ? (y/n) ')\n",
    "    if yn_choice('Include only checked events ? (y/n)', default='y'):\n",
    "        print('Removing unchecked events')\n",
    "        df = df[df['checked']==True]\n",
    "    \n",
    "    print('Now we have the following number of events by subclass:')\n",
    "    L = []\n",
    "    for i, subclass in enumerate(subclasses_for_ML):\n",
    "        df_subclass = df[df['new_subclass']==subclass]\n",
    "        L.append(len(df_subclass.index))\n",
    "        print('- %s: %d' % (subclass, L[i]))\n",
    "    maxnumevents = int(input('What is the maximum number of events of each subclass you want to use for supervised learning (e.g. %d) ? ' % max(L)))\n",
    "    best_events_dict = _select_best_events(df, subclasses_for_ML, N=maxnumevents, exclude_checked=False)\n",
    "    df = _merge_dataframes(best_events_dict, subclasses_for_ML)\n",
    "    \n",
    "    minnumevents = int(input('What is the minimum number of events of each subclass you want to use for supervised learning, (e.g. %d) ?\\nSubclasses with less than this number of events will be removed. ' % int(max(L)/10)))\n",
    "    for subclass in df['new_subclass'].unique():\n",
    "        df_subclass = df[df['new_subclass']==subclass]\n",
    "        if len(df_subclass.index) < minnumevents:\n",
    "            print('Eliminating subclass %s' % subclass)\n",
    "            subclasses_for_ML.remove(subclass)\n",
    "            \n",
    "    print('The subclasses for machine learning are %s. Removing other subclasses.' % ''.join(subclasses_for_ML) )\n",
    "    df_list = []\n",
    "    df.sort_values(by='filetime',inplace=True)\n",
    "    for i, row in df.iterrows():\n",
    "        row['new_subclass'] = row['new_subclass'].strip()\n",
    "        if row['new_subclass'] in subclasses_for_ML:\n",
    "            df_list.append(row)\n",
    "    df = pd.DataFrame(df_list) \n",
    "    \n",
    "    print('Here is the FINAL list of events by subclass and whether they have been checked:')\n",
    "    _count_by_subclass(df)    \n",
    "    \n",
    "    #df.rename(columns = {'twin':'duration'}, inplace = True)\n",
    "    df.rename(columns = {'twin':'length'}, inplace = True)\n",
    "    #df['f0']=None\n",
    "    #df['f1']=None\n",
    "    df['f0']=0.5\n",
    "    df['f1']=25.0   \n",
    "    \n",
    "    \"\"\"\n",
    "    confidence_threshold = int(input('What is the minimum confidence percentage (e.g. 50) you wish to use ? This should be an integer.'))\n",
    "    if confidence_threshold:\n",
    "        good_indices = []\n",
    "        for subclass in subclasses_for_ML:\n",
    "            good_indices.expand(df[subclass] >= confidence_threshold)\n",
    "        df = df.iloc[good_indices]\n",
    "    print(df[['filetime', 'new_subclass']])\n",
    "    \"\"\"\n",
    "    \n",
    "    # subset and rename columns for output\n",
    "    if ignore_extra_columns:\n",
    "        df=df[['new_subclass','year','month','day','hour','minute','second','length','path']]     \n",
    "    df.rename(columns = {'new_subclass':'class'}, inplace = True)\n",
    "    \n",
    "    if make_local_archive:\n",
    "        for i, row in df.iterrows():\n",
    "            subclass = row['class']\n",
    "            oldpath = row['path'].replace('./', '/Users/thompsong/DATA/MVO/')\n",
    "            newpath = os.path.join(subclass,os.path.basename(oldpath))\n",
    "            df.loc[i, 'path']=newpath\n",
    "            if not os.path.exists(subclass):\n",
    "                os.mkdir(subclass)\n",
    "            print('cp %s %s' % (oldpath, newpath))\n",
    "            os.system('cp %s %s' % (oldpath, newpath))\n",
    "            \n",
    "    \n",
    "    df.to_csv(outfile)\n",
    "    print('Saved to ',outfile)\n",
    "    \n",
    "def report_checked_events(dfall, subclasses_for_ML):\n",
    "    # Sanity check against AAA writer\n",
    "    df = dfall.copy()\n",
    "    df = df[df['checked']==True]\n",
    "    print('total checked events = %d' % len(df.index))\n",
    "    df = df[df['ignore']==False]\n",
    "    df = df[df['delete']==False]\n",
    "    df = df[df['split']==False]\n",
    "    print('total classified events = %d' % len(df.index))\n",
    "    frames = []\n",
    "    for subclass in subclasses_for_ML:\n",
    "        dfs = df[df['new_subclass']==subclass]\n",
    "        print(subclass, len(dfs.index))\n",
    "        frames.append(dfs)\n",
    "    newdf = pd.concat(frames)\n",
    "\n",
    "    L0 = len(newdf.index)\n",
    "    print('total events matching ML subclasses = %d' % L0)\n",
    "\n",
    "    differentdf = newdf[newdf['subclass']!=newdf['new_subclass']]\n",
    "    L1 = len(differentdf.index)\n",
    "    print('total reclassified events = %d' % L1)\n",
    "\n",
    "    samedf = newdf[newdf['subclass']==newdf['new_subclass']]\n",
    "    L2 = len(samedf.index)\n",
    "    print('total already correctly classified events = %d' % L2)\n",
    "\n",
    "    print('Error rate = %.1f%%' % (L1*100/L0))\n",
    "\n",
    "    print(L0, L1+L2)\n",
    "\n",
    "    newdf.sort_values(by=['weight'],inplace=True,ascending=False)\n",
    "    print(newdf[['subclass', 'R', 'r', 'e', 'l', 'h', 't', 'new_subclass', 'weight']].to_string())\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "SEISAN_DATA = os.path.join( os.getenv('HOME'),'DATA','MVO')\n",
    "DB = 'MVOE_'\n",
    "subclass_mapping = read_volcano_def() # subclasses allowed for classification\n",
    "#print(subclass_mapping.columns)\n",
    "seisan_subclasses = subclass_mapping['subclass'].values.tolist() # append('g') as needed, it is not an allowed subclass\n",
    "#seisan_etypes = subclass_mapping['etype'].values.tolist()\n",
    "subclasses_for_ML = ['D', 'R', 'r', 'e', 'l', 'h', 't'] # subclasses allowed for Machine Learning\n",
    "outfile = 'catalog_all.csv'\n",
    "\n",
    "if os.path.exists(outfile):\n",
    "    dfall = pd.read_csv(outfile) # how do i ignore the index?\n",
    "    # do the following until I learn how to ignore index. otherwise it adds a new column on each load.\n",
    "    dfall = dfall[['filetime', 'Fs', 'RSAM_high',\n",
    "       'RSAM_low', 'band_ratio', 'bw_max', 'bw_min', 'calib', 'cft_peak_wmean',\n",
    "       'cft_std_wmean', 'coincidence_sum', 'day', 'detection_quality',\n",
    "       'energy', 'hour', 'kurtosis', 'medianF', 'minute', 'month', 'num_gaps',\n",
    "       'num_traces', 'offtime', 'ontime', 'path', 'peakA', 'peakF', 'peakamp',\n",
    "       'peaktime', 'percent_availability', 'quality', 'sample_lower_quartile',\n",
    "       'sample_max', 'sample_mean', 'sample_median', 'sample_min',\n",
    "       'sample_rms', 'sample_stdev', 'sample_upper_quartile', 'second',\n",
    "       'sfile', 'skewness', 'starttime', 'subclass', 'trigger_duration',\n",
    "       'twin', 'year', 'D', 'R', 'r', 'e', 'l', 'h', 't', 'new_subclass',\n",
    "       'weight', 'checked', 'split', 'delete', 'ignore']]\n",
    "else:\n",
    "    master_event_catalog = 'catalog_all_original.csv'\n",
    "    dfall = build_master_event_catalog(SEISAN_DATA, DB, master_event_catalog, subclasses_for_ML)\n",
    "\n",
    "station0hypfile = os.path.join(SEISAN_DATA, 'DAT', 'STATION0_MVO.HYP')\n",
    "station_locationsDF = parse_STATION0HYP(station0hypfile)\n",
    "\n",
    "iterate_again = True # changed this back to do the loop\n",
    "while iterate_again:\n",
    "    \n",
    "    # how many events of each type do we want?\n",
    "    _count_by_subclass(dfall)\n",
    "    choice = input('How many (more) events of each subclass do you want to reclassify ? ')\n",
    "    N = int(choice)\n",
    "    if not N>0:\n",
    "        iterate_again = False\n",
    "        break\n",
    "\n",
    "    # get/update the fingerprints of each event class\n",
    "    #fingerprints = get_fingerprints(dfall, SUBCLASSES, N=300, exclude_checked=False)  \n",
    "    fingerprints = get_weighted_fingerprints(dfall, subclasses_for_ML, N=300, exclude_checked=False)\n",
    "    save_fingerprints(fingerprints, subclasses_for_ML)\n",
    "\n",
    "    # for further classification select best unchecked events of each subclass based on quality_index\n",
    "    best_events_dict = _select_best_events(dfall, subclasses_for_ML, N=N, exclude_checked=True)\n",
    "    \n",
    "    # manually QC the data\n",
    "    best_events_dict = qc_best_events(best_events_dict, seisan_subclasses, subclasses_for_ML, fingerprints)\n",
    "    \n",
    "    # save and summarize the data\n",
    "    allbest = _merge_dataframes(best_events_dict, best_events_dict.keys())\n",
    "    allbest.sort_index(inplace=True)\n",
    "    dfall.sort_index(inplace=True)\n",
    "    dfall.update(allbest)    \n",
    "    dfall.to_csv(outfile)\n",
    "    _count_by_subclass(dfall)\n",
    "    \n",
    "    choice = input('Do you want to iterate again (y/n)?')\n",
    "    if choice[0]=='n':\n",
    "        iterate_again=False\n",
    " \n",
    "# remove events we marked for deletion, splitting or to ignore\n",
    "dfsubset = remove_marked_events(dfall)\n",
    "\n",
    "aaa_infile = 'aaa_labelled_events.csv' \n",
    "to_AAA(dfsubset, subclasses_for_ML, aaa_infile, make_local_archive=True)\n",
    "report_checked_events(dfall, subclasses_for_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we plot the first 1-minute of each selected event file, padding if needed\n",
    "# We do this mainly to see the variable onset time of signals within event files\n",
    "import pandas as pd\n",
    "aaa_infile = 'aaa_labelled_events.csv' \n",
    "dfAAA = pd.read_csv(aaa_infile)\n",
    "for i, row in dfAAA.iterrows():\n",
    "    st = read(row['path'])\n",
    "    st = st.select(station='MBLG', component='Z')\n",
    "    if len(st):\n",
    "        stime = st[0].stats.starttime\n",
    "        st.trim(starttime=stime,endtime=stime+60,pad=True,fill_value=0)\n",
    "        st.plot();\n",
    "        print('subclass = %s' % row['class'])\n",
    "        #dummy = input('ENTER to see next signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if Pickle files are corrected\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from obspy.core import read, UTCDateTime\n",
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "df = dfall.copy()\n",
    "df.sort_values(by=['trigger_duration'],ascending=False)\n",
    "for i,row in df.iterrows():\n",
    "    abpath =row['path'].replace('./WAV', '/Users/thompsong/DATA/MVO/PICKLE') + '.pickle'\n",
    "    st = read(abpath)\n",
    "    for tr in st:\n",
    "        print(tr.stats)\n",
    "    st.plot()\n",
    "    dummy = input('ENTER to see next event, or q to quit')  \n",
    "    if dummy=='q':\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if there is a relationship between detection window length and length of file for different subclasses\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from obspy.core import read, UTCDateTime\n",
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "df = dfall.copy()\n",
    "for subclass in ['r','e','l','h','t']:\n",
    "    df0 = df[df['new_subclass']==subclass]\n",
    "    df0 = df0[df0['trigger_duration']>0]\n",
    "    #print(df_subclass.columns)\n",
    "    print(subclass)\n",
    "    print(df0[['twin','trigger_duration']].describe())\n",
    "    for i,row in df0.iterrows():\n",
    "        abpath =row['path'].replace('./', '/Users/thompsong/DATA/MVO/')\n",
    "        st = read(abpath)\n",
    "        st = st.select(station='MBLG', component='Z')\n",
    "        \n",
    "        if len(st)==0:\n",
    "            st = st.select(station='MBWH', component='Z')\n",
    "        if len(st)==0:\n",
    "            tr = st[0]\n",
    "            st = Stream()\n",
    "            st.append(tr)\n",
    "        #st.plot(equal_scale=False)\n",
    "        st.normalize()\n",
    "        plt.plot(st[0].times(), st[0].data)\n",
    "        ontime = UTCDateTime.strptime(row['ontime'], format='%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "        offtime = UTCDateTime.strptime(row['offtime'], format='%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "        filetime = UTCDateTime.strptime(row['filetime'], format='%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "        plt.vlines([ontime-filetime, offtime-filetime],-1,1,'r')\n",
    "        plt.ylabel(st[0].id)\n",
    "        plt.title('subclass = %s ' % subclass)\n",
    "        plt.xlabel('Time (s)')\n",
    "        \n",
    "        plt.show()\n",
    "        dummy = input('ENTER to see next event, or q to quit')  \n",
    "        if dummy=='q':\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the events that I have marked for splitting\n",
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "df = dfall.copy()\n",
    "df = df[df['split']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "fingerprint_files = glob('fingerprint*.csv')\n",
    "print(fingerprint_files)\n",
    "for file in fingerprint_files:\n",
    "    df = pd.read_csv(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.tips()\n",
    "fig = px.box(df, x='time', y='total_bill')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "df = dfall.copy()\n",
    "df = df[df['checked']==True]\n",
    "df = df[df['ignore']==False]\n",
    "df = df[df['delete']==False]\n",
    "df = df[df['split']==False]\n",
    "frames = []\n",
    "for subclass in ['R', 'r', 'e', 'l', 'h', 't']:\n",
    "    dfs = df[df['new_subclass']==subclass]\n",
    "    print(subclass, len(dfs.index))\n",
    "    frames.append(dfs)\n",
    "newdf = pd.concat(frames)\n",
    "for index, row in newdf.iterrows():\n",
    "    print(row[['filetime', 'subclass', 'R', 'r', 'e', 'l', 'h', 't', 'new_subclass']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = 'catalog_all.csv'\n",
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(infile)\n",
    "for index, row in dfall.iterrows():\n",
    "    if row['new_subclass'] != row['subclass']:\n",
    "        dfall.loc[index, row['subclass']]=0\n",
    "dfall.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subclasses_for_ML = ['D', 'R', 'r', 'e', 'l', 'h', 't'] # subclasses allowed for Machine Learning\n",
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "dfall = dfall[['filetime', 'Fs', 'RSAM_high',\n",
    "       'RSAM_low', 'band_ratio', 'bw_max', 'bw_min', 'calib', 'cft_peak_wmean',\n",
    "       'cft_std_wmean', 'coincidence_sum', 'day', 'detection_quality',\n",
    "       'energy', 'hour', 'kurtosis', 'medianF', 'minute', 'month', 'num_gaps',\n",
    "       'num_traces', 'offtime', 'ontime', 'path', 'peakA', 'peakF', 'peakamp',\n",
    "       'peaktime', 'percent_availability', 'quality', 'sample_lower_quartile',\n",
    "       'sample_max', 'sample_mean', 'sample_median', 'sample_min',\n",
    "       'sample_rms', 'sample_stdev', 'sample_upper_quartile', 'second',\n",
    "       'sfile', 'skewness', 'starttime', 'subclass', 'trigger_duration',\n",
    "       'twin', 'year', 'D', 'R', 'r', 'e', 'l', 'h', 't', 'new_subclass',\n",
    "       'weight', 'checked', 'split', 'delete', 'ignore']]\n",
    "df = dfall[dfall['new_subclass']=='e']\n",
    "#print(df)\n",
    "df.to_csv('test1.csv')\n",
    "df2 = pd.read_csv('test1.csv',ignore_index=True) # how do i ignore index?\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in dfall.iterrows():\n",
    "    ns = row['new_subclass']\n",
    "    if len(ns)!=1:\n",
    "        if not ns in subclasses_for_ML:\n",
    "            print(\"%d, *%s*\"%(i,ns))\n",
    "            ns=ns.strip()\n",
    "            dfall.loc[i,'new_subclass']=ns\n",
    "            dfall.loc[i,ns]=100\n",
    "print(len(dfall.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfall.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load without index\n",
    "# on map, add station names then remove the ampeng stats\n",
    "# plot open, not closed, circles\n",
    "# figure out why R does not get saved, and no D at all. And why e do not all appear to save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
