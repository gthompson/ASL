{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from:  /media/sdd1/backups/seismo\n"
     ]
    }
   ],
   "source": [
    "# count Sfiles by year/month\n",
    "import os\n",
    "import pandas as pd\n",
    "SEISAN_TOP = os.getenv('SEISAN_TOP')\n",
    "# Uncomment following line to use local archive on hal, rather than newton\n",
    "SEISAN_TOP = '/media/sdd1/backups/seismo'\n",
    "DB = 'MVOE_'\n",
    "print('Reading from: ',SEISAN_TOP)\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     year month  sfiles\n",
      "0    1996    10     424\n",
      "1    1996    11    2091\n",
      "2    1996    12    5570\n",
      "3    1997    01    1729\n",
      "4    1997    02    2695\n",
      "..    ...   ...     ...\n",
      "142  2008    08     247\n",
      "143  2008    09     125\n",
      "144  2008    10      85\n",
      "145  2008    11       0\n",
      "146  2008    12       0\n",
      "\n",
      "[147 rows x 3 columns]\n",
      "235813\n"
     ]
    }
   ],
   "source": [
    "yeardirs = glob.glob(os.path.join(SEISAN_TOP, 'REA', DB, '[12]???'))\n",
    "rows = []\n",
    "for yeardir in sorted(yeardirs):\n",
    "    yearmonthdirs = glob.glob(os.path.join(yeardir, '[01][0-9]'))\n",
    "    for yearmonthdir in sorted(yearmonthdirs):\n",
    "        sfiles = glob.glob(os.path.join(yearmonthdir, '*[LRD].S*'))\n",
    "        thisrow = { 'year':yeardir[-4:],'month':yearmonthdir[-2:], 'sfiles':len(sfiles) } \n",
    "        rows.append(thisrow)\n",
    "readf = pd.DataFrame(rows)\n",
    "print(readf)\n",
    "\n",
    "print(readf['sfiles'].sum())\n",
    "readf.to_csv('count_seisan_files_REA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     year month  sfiles  DSNfiles  ASNfiles\n",
      "0    1996    10     424       483         0\n",
      "1    1996    11    2091      2425         0\n",
      "2    1996    12    5570      5549         0\n",
      "3    1997    01    1729      1736         0\n",
      "4    1997    02    2695      2749         0\n",
      "..    ...   ...     ...       ...       ...\n",
      "138  2008    04     117       116         0\n",
      "139  2008    05     153       153         0\n",
      "140  2008    06     139       139         0\n",
      "141  2008    07    1961      1963         0\n",
      "142  2008    08     247       247         0\n",
      "\n",
      "[143 rows x 5 columns]\n",
      "sfiles      235603\n",
      "DSNfiles    217861\n",
      "ASNfiles     38664\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "yeardirs = glob.glob(os.path.join(SEISAN_TOP, 'WAV', DB, '[12]???'))\n",
    "rows = []\n",
    "for yeardir in sorted(yeardirs):\n",
    "    yearmonthdirs = glob.glob(os.path.join(yeardir, '[01][0-9]'))\n",
    "    for yearmonthdir in sorted(yearmonthdirs):\n",
    "        sfiles = glob.glob(os.path.join(yearmonthdir.replace('WAV','REA'), '*[LRD].S*'))\n",
    "        dsnfiles = glob.glob(os.path.join(yearmonthdir, '*S.MVO*'))\n",
    "        asnfiles = glob.glob(os.path.join(yearmonthdir, '*S.SPN*'))\n",
    "        thisrow = { 'year':yeardir[-4:],\n",
    "                   'month':yearmonthdir[-2:], \n",
    "                   'sfiles':len(sfiles),\n",
    "                   'DSNfiles':len(dsnfiles),\n",
    "                   'ASNfiles':len(asnfiles)\n",
    "                  } \n",
    "        rows.append(thisrow)\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n",
    "\n",
    "print(df[['sfiles','DSNfiles','ASNfiles']].sum())\n",
    "df.to_csv('count_seisan_files_WAV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Fs', 'bandratio_[0.8_4.0_16.0]',\n",
      "       'bandratio_[1.0_6.0_11.0]', 'bw_max', 'bw_min', 'calib',\n",
      "       'cft_peak_wmean', 'cft_std_wmean', 'coincidence_sum', 'day',\n",
      "       'detection_quality', 'energy', 'filetime', 'hour', 'kurtosis',\n",
      "       'mainclass', 'medianF', 'minute', 'month', 'noise_level', 'num_gaps',\n",
      "       'num_traces', 'offtime', 'ontime', 'path', 'peakA', 'peakF', 'peakamp',\n",
      "       'peaktime', 'percent_availability', 'quality', 'sample_lower_quartile',\n",
      "       'sample_max', 'sample_mean', 'sample_median', 'sample_min',\n",
      "       'sample_rms', 'sample_stdev', 'sample_upper_quartile', 'second',\n",
      "       'sfile', 'signal_level', 'skewness', 'snr', 'subclass',\n",
      "       'trigger_duration', 'year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Let's now read in reawav_MVOE_all.csv and see if there are duplicate S-files or WAV-files still\n",
    "reawavdf = pd.read_csv('~/DATA/MVO/reawav_MVOE_all.csv')\n",
    "print(reawavdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208229"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reawavdf.sfile.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208186"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reawavdf.filetime.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208187"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reawavdf.path.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = reawavdf.sort_values('filetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400977\n"
     ]
    }
   ],
   "source": [
    "df2.drop_duplicates()\n",
    "print(len(df2.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208230\n"
     ]
    }
   ],
   "source": [
    "# so with vi, i see that there are many almost duplicate rows except for rounding errors\n",
    "# let's drop duplicates based on just sfile and path (path is wavfile)\n",
    "df3 = df2.drop_duplicates(\n",
    "    subset=['sfile','path'],\n",
    "    keep = 'last')\n",
    "print(len(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now examine any rows of df3 with non-unique path\n",
    "dups = df3[df3.duplicated(subset=['path'])]\n",
    "\n",
    "for thispath in dups['path']:\n",
    "    df4 = df3[df3['path']==thispath]\n",
    "    wavbase = os.path.basename(thispath)\n",
    "    df4.to_csv('%s.csv' % wavbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
