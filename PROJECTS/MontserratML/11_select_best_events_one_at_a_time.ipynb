{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montserrat event selector for Machine Learning\n",
    "The aim of this code is to find the best N events of each type, and create a corresponding CSV file and data structure for entry into Alexis' and Marielle's AAA codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "## SCAFFOLD. It keeps giving me the same Regional over and over.\n",
    "# Is the CSV file corrupt? Or is there a bug in the code?\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "LIBpath = os.path.join( os.getenv('HOME'),'src','kitchensinkGT', 'LIB')\n",
    "sys.path.append(LIBpath)\n",
    "from libseisGT import add_to_trace_history #, mulplt\n",
    "from modutils import yn_choice\n",
    "\n",
    "from obspy import read_inventory #, remove_response\n",
    "from libMVO import fix_trace_id, inventory_fix_id_mvo, load_mvo_inventory\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(cwd)\n",
    "from libMontyML import read_volcano_def, build_master_event_catalog, parse_STATION0HYP, qc_event, \\\n",
    "     get_weighted_fingerprints, save_fingerprints, remove_marked_events, to_AAA, report_checked_events\n",
    "\n",
    "SEISAN_DATA = os.path.join( os.getenv('HOME'),'DATA','MVO')\n",
    "DB = 'MVOE_'\n",
    "\n",
    "subclass_mapping = read_volcano_def() # subclasses allowed for classification\n",
    "seisan_subclasses = subclass_mapping['subclass'].values.tolist() # append('g') as needed, it is not an allowed subclass\n",
    "#seisan_etypes = subclass_mapping['etype'].values.tolist()\n",
    "subclasses_for_ML = ['D', 'R', 'r', 'e', 'l', 'h', 't'] # subclasses allowed for Machine Learning\n",
    "outfile = 'MVOE_catalog.csv'\n",
    "\n",
    "if os.path.exists(outfile):\n",
    "    dfall = pd.read_csv(outfile) # how do i ignore the index?\n",
    "    # do the following until I learn how to ignore index. otherwise it adds a new column on each load.\n",
    "    dfall = dfall[['filetime', 'Fs', 'bandratio_[0.8_4.0_16.0]',\n",
    "       'bandratio_[1.0_6.0_11.0]', 'bw_max', 'bw_min', 'calib',\n",
    "       'cft_peak_wmean', 'cft_std_wmean', 'coincidence_sum', 'day',\n",
    "       'detection_quality', 'energy', 'hour', 'kurtosis', 'medianF', 'minute',\n",
    "       'month', 'noise_level', 'num_gaps', 'num_traces', 'offtime', 'ontime',\n",
    "       'path', 'peakA', 'peakF', 'peakamp', 'peaktime', 'percent_availability',\n",
    "       'quality', 'sample_lower_quartile', 'sample_max', 'sample_mean',\n",
    "       'sample_median', 'sample_min', 'sample_rms', 'sample_stdev',\n",
    "       'sample_upper_quartile', 'second', 'sfile', 'signal_level', 'skewness',\n",
    "       'snr', 'starttime', 'subclass', 'trigger_duration', 'year', 'D', 'R',\n",
    "       'r', 'e', 'l', 'h', 't', 'new_subclass', 'weight', 'checked', 'split',\n",
    "       'delete', 'ignore']]\n",
    "    # removed twin as it is missing\n",
    "else:\n",
    "    master_event_catalog = 'MVOE_catalog_original.csv'\n",
    "    \n",
    "    # SCAFFOLD - the twin column no longer seems to exist\n",
    "    dfall = build_master_event_catalog(SEISAN_DATA, DB, master_event_catalog, subclasses_for_ML)\n",
    "\n",
    "station0hypfile = os.path.join(SEISAN_DATA, 'DAT', 'STATION0_MVO.HYP')\n",
    "station_locationsDF = parse_STATION0HYP(station0hypfile)\n",
    "\n",
    "###\n",
    "#fingerprints = get_weighted_fingerprints(dfall, subclasses_for_ML, N=300, exclude_checked=False)\n",
    "#one_event_df, quit = qc_event(dfall, subclasses_for_ML, seisan_subclasses, fingerprints, SEISAN_DATA, station_locationsDF)\n",
    "\n",
    "iterate_again = True # changed this back to do the loop\n",
    "while iterate_again:\n",
    "\n",
    "    # get/update the fingerprints of each event class\n",
    "    fingerprints = get_weighted_fingerprints(dfall, subclasses_for_ML, N=100, exclude_checked=False)\n",
    "    save_fingerprints(fingerprints, subclasses_for_ML)\n",
    "    \n",
    "    # manually QC the next event. each time we choose the class with least checked examples\n",
    "    one_event_df, quit = qc_event(dfall, subclasses_for_ML, seisan_subclasses, fingerprints, SEISAN_DATA, station_locationsDF)\n",
    "    if isinstance(one_event_df, pd.DataFrame):\n",
    "        # now we must merge this back into dfall\n",
    "        dfall.sort_index(inplace=True)\n",
    "        dfall.update(one_event_df)  \n",
    "    \n",
    "        # save the data  \n",
    "        dfall.to_csv(outfile, index=False)\n",
    "    else:\n",
    "        iterate_again=False\n",
    "    if quit:\n",
    "        iterate_again=False \n",
    "# remove events we marked for deletion, splitting or to ignore\n",
    "dfsubset = remove_marked_events(dfall)\n",
    "\n",
    "aaa_infile = 'MVOE_catalog_reclassified.csv' \n",
    "to_AAA(dfsubset, subclasses_for_ML, aaa_infile, SEISAN_DATA, ignore_extra_columns=False)\n",
    "report_checked_events(dfall, subclasses_for_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When I work on hal, I will want to build a full catalog \n",
    "# from all the event CSV files. But I would lose my checked events.\n",
    "# So this is how I took care of that before.\n",
    "# 1. Move the catalog file to a new name, e.g. MVOE_catalog_previous.csv\n",
    "# 2. Move the original catalog file also, e.g. MVOE_catalog_original_previous.csv\n",
    "# 3. Run the code above but with iterate=False\n",
    "# 4. Now we should have a new MVOE_catalog_original.csv. \n",
    "#    Copy that to MVOE_catalog.csv\n",
    "# 5. Now we need to update the newest catalog using the oldest. \n",
    "#    Any rows with matching file times should be replaced\n",
    "print(dfall.path)\n",
    "\n",
    "oldcatfile = 'MVOE_catalog_previous.csv'\n",
    "newcatfile = 'MVOE_catalog.csv'\n",
    "newcat = pd.read_csv(newcatfile)\n",
    "newcat.update(pd.read_csv(oldcatfile))\n",
    "newcat.to_csv('MVOE_catalog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'MVOE_catalog.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "#df = dfall[dfall['filetime']=='2003-05-14T06:04:14.000000Z']\n",
    "#df = dfall[dfall['filetime']=='1901-03-06T21:13:28.040000Z']\n",
    "df = dfall[dfall['filetime']=='2001-11-06T21:13:28.040000Z']\n",
    "print(len(dfall))\n",
    "#df = dfall.iloc[0]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfall['filetime'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows 17489\n",
      "Number of unique filetimes 17489\n",
      "Number of unique paths 17489\n",
      "Number of rows 17489\n"
     ]
    }
   ],
   "source": [
    "# it turns out that multiple Sfiles sometimes point to the same WAVfile\n",
    "# this leads to non-unique filetime entries\n",
    "\n",
    "# Indeed we now see there is a many-to-many relationship between (WAVfile) path and Sfile.\n",
    "\n",
    "# reawav_MVOE_YYYYDD.csv: fields include sfile, (WAV) path and (WAV) filetime\n",
    "# ^ this already establishes a link between every Sfile and DSN WavFile\n",
    "\n",
    "# Better might be...\n",
    "# Mapping.CSV: Sfile, SfileTime, DSNWavfile, DSNWavFileTime, ASNWavfile, ASNWavFileTime\n",
    "\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "LIBpath = os.path.join( os.getenv('HOME'),'src','kitchensinkGT', 'LIB')\n",
    "sys.path.append(LIBpath)\n",
    "import seisan_classes\n",
    "SEISAN_DATA = os.path.join( os.getenv('HOME'),'DATA','MVO')\n",
    "DB = 'MVOE_'\n",
    "\n",
    "def sfile2spath(sfile):\n",
    "    parts = sfile.split('.S')\n",
    "    YYYY = parts[1][0:4]\n",
    "    MM = parts[1][4:6]\n",
    "    spath = os.path.join(SEISAN_DATA, 'REA', DB, YYYY, MM, sfile) \n",
    "    return spath    \n",
    "\n",
    "catfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(catfile)\n",
    "\n",
    "print('Number of rows %d' % len(dfall))\n",
    "print('Number of unique filetimes %d' % len(dfall['filetime'].unique()))\n",
    "print('Number of unique paths %d' % len(dfall['path'].unique()))\n",
    "\n",
    "for i,row in dfall.iterrows():\n",
    "    if i>0:\n",
    "        if row['delete']==True:\n",
    "            continue\n",
    "        if row['filetime']==lastrow['filetime']:\n",
    "            rows=[lastrow, row]\n",
    "            \n",
    "            # Fix the S-file path. Sometimes we have two S-files that point to same DSN MVO WAVfile.\n",
    "            # We have to examine both S-files, and determine what name they should have based on the WAVfiles\n",
    "            # they point to. The key is the first WAVfile in time. Then we ultimately drop the later S-file.\n",
    "            predicted_sfilepath = \"\"\n",
    "            first_wavfile = None\n",
    "            for r in rows:\n",
    "                sfilepath = sfile2spath(r['sfile'])\n",
    "                sfileobj = seisan_classes.Sfile(sfilepath, use_mvo_parser=True)\n",
    "                for wavfile in sfileobj.wavfiles:\n",
    "                    if not first_wavfile:\n",
    "                        first_wavfile = wavfile\n",
    "                    if first_wavfile.filetime > wavfile.filetime:\n",
    "                        first_wavfile = wavfile\n",
    "            predicted_sfilepath, was_found = first_wavfile.find_sfile(mainclass=sfileobj.mainclass[0])  \n",
    "            dfall.loc[lasti, 'sfile'] = os.path.basename(predicted_sfilepath)\n",
    "            \n",
    "            # Fix the subclass\n",
    "            if row['subclass']!=lastrow['subclass']:\n",
    "                sfileobj = seisan_classes.Sfile(sfilepath, use_mvo_parser=True)\n",
    "                dfall.loc[lasti, 'mainclass'] = sfileobj.mainclass\n",
    "                dfall.loc[lasti, 'subclass'] = sfileobj.subclass\n",
    "                \n",
    "            # If either event was checked, assume the new_subclass is correct\n",
    "            keep = lasti\n",
    "            if dfall.loc[lasti, 'checked'] == False:\n",
    "                dfall.loc[lasti, 'new_subclass'] = sfileobj.subclass\n",
    "            if dfall.loc[i, 'checked'] == True:\n",
    "                dfall.loc[lasti, 'new_subclass'] = dfall.loc[i, 'new_subclass']\n",
    "                for subclass in ['R', 'r', 'e', 'l', 'h', 't']:\n",
    "                    dfall.loc[lasti, subclass] = dfall.loc[i, subclass]\n",
    "                dfall.loc[lasti, 'checked'] = True\n",
    "                dfall.loc[lasti, 'ignore'] = dfall.loc[i, 'ignore']             \n",
    "\n",
    "            # Mark the duplicate for removal    \n",
    "            dfall.loc[i, 'delete'] = 2\n",
    "            \n",
    "            # Print the modified rows of the dataframe  \n",
    "            print(dfall.loc[[lasti, i], ['sfile','subclass','new_subclass', 'checked','delete']])\n",
    "            \n",
    "    lastrow=row\n",
    "    lasti=i\n",
    "    \n",
    "# Remove duplicated rows previously marked for deletion\n",
    "dfall = dfall[dfall['delete']!=2]\n",
    "print('Number of rows %d' % len(dfall))\n",
    "\n",
    "outfile = catfile.replace('all', 'unique')\n",
    "#dfall.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we want to figure out how to do this for the new, bigger catalog. And then merge the two catalogs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
