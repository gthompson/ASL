{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montserrat Seisan database conversion to CSV files\n",
    "The aim of this code is to read through an entire Seisan database of S-files (under REA/DB/yyyy/mm) and the corresponding WAV-files (under WAV/DB/yyyy/mm). WAV-files that do not appear in an S-file are ignored.\n",
    "\n",
    "Glenn Thompson, 2021/07/22\n",
    "\n",
    "For each linked WAV-file, we perform the following steps:\n",
    "\n",
    "## function WAV2picklefile\n",
    "1. Load the WAV-file into an ObsPy Stream object. \n",
    "2. Fix trace IDs.\n",
    "3. Process each Trace object in the Stream object using various functions in metrics.py that:\n",
    "4. - compute standard Miniseed QC metrics. Stored in tr.stats.metrics.\n",
    "5. - clean the trace, which consists of:\n",
    "6. - - clipping trace to remove absurd values\n",
    "7. - - padding the trace by at least 10-seconds on each end, with a reflection of data on each end\n",
    "8. - - detrending the trace\n",
    "9. - - tapering the trace (just the padded parts)\n",
    "10. - - filtering the trace from 0.1 to 40 Hz.\n",
    "11. - - optionally correcting for instrument response (if corresponding StationXML file found). This will also change tr.stats.units from 'Counts' to 'm/s'.\n",
    "12. - - unpadding the trace\n",
    "13. - computing signal to noise ratio. This is done by breaking the signal into seconds, and computing the ratio of the highest amplitude 1-second to the lowest amplitude 1-second. Stored in tr.stats.metrics.\n",
    "14. - computing SciPy.stats metrics including kurtosis. These are stored in tr.stats.scipy.\n",
    "15. - recomputing the Miniseed QC metrics now that traces have been detrended, filtered and (optionally) instrument corrected.\n",
    "16. Computing a quality_factor for each trace, based on the above metrics.\n",
    "17. Removing any traces with quality_factor = 0.0 from the Stream object.\n",
    "18. Saving the Stream object as a Pickle file. This goes into PICKLE/DB/YYYY/MM.\n",
    "\n",
    "## function Stream2logfile\n",
    "19. Dump all the metrics for each Trace object to a log file. This helps for debugging.\n",
    "\n",
    "## function Stream2png\n",
    "\n",
    "20. Optionally plot the Stream object and save as a PNG file. If the Stream contains a mixture of seismic and infrasound traces, one PNG will be created for seismic traces, another for infrasound traces, because they have different units (m/s or Pa).\n",
    "\n",
    "## function add_ampengfft_metrics\n",
    "21. Compute spectrograms for each trace using icewebPy.\n",
    "22. Compute amplitude spectrum from the spectrograms.\n",
    "23. Compute amplitude, energy and frequency metrics including peakf, medianf, RSAM_high, RSAM_low, band_ratio. These are added to tr.stats.metrics.\n",
    "\n",
    "## function plot_spectrograms\n",
    "24. Optionally plot spectrograms that were computed at step 9.\n",
    "\n",
    "## function metrics2tracedf\n",
    "25. Translate the metrics stored in tr.stats into a pandas DataFrame, one row per Trace.\n",
    "26. Save this to a CSV file in the PICKLE directory.\n",
    "\n",
    "## function tracedf2eventdf\n",
    "27. Compute the median of each numeric column in the DataFrame, to create a 1-row summary for each event.\n",
    "28. Optionally attempt to detect the event within the Trace using an STA/LTA. This is controlled by the bool_detect_event. Additional metrics from the detection result are added to the 1-row summary.\n",
    "\n",
    "## function processWAV\n",
    "This is a wrapper around all of the above, to apply each function to a single WAV-file.\n",
    "\n",
    "## function processSeisanYearMonth\n",
    "This is a wrapper around processWAV.\n",
    "1. calls function get_sfile_list to make a list of all S-file within a particular REA/DB/YYYY/MM directory.\n",
    "2. reads each S-file.\n",
    "3. calls processWAV on any WAV-files linked from the S-file (if they can be found). There can be up to 2 - one from the analog network, one from the digital network (there were two networks from 10/1996 - 12/2004).\n",
    "4. For each WAV-file, the output from processWAV is a 1-row event summary. To this the sfile path, main event classification and volcano-seismic event subclassification are added.\n",
    "5. Each 1-row event summary is appended to a DataFrame which is stored as reawav_(DB)(YYYY)(MM).csv. This contains a summary of each WAV file corresponding to an S-file for the YYYY/MM in question.\n",
    "\n",
    "## function main\n",
    "This loops over every YYYY/MM directory in the given Seisan database, calling processSeisanYearMonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from obspy import read, read_inventory, Stream\n",
    "#from obspy.io.xseed.core import _read_resp\n",
    "#from obspy.imaging.cm import obspy_sequential\n",
    "\n",
    "LIBpath = os.path.join( os.getenv('HOME'),'src','kitchensinkGT', 'LIB')\n",
    "sys.path.append(LIBpath)\n",
    "from libMVO import fix_trace_id, inventory_fix_id_mvo, load_mvo_inventory\n",
    "from metrics import process_trace, choose_best_traces, select_by_index_list, ampengfft\n",
    "from libseisGT import Stream_min_starttime, detect_network_event\n",
    "from seisan_classes import spath2datetime, Sfile #, printEvents\n",
    "\n",
    "sys.path.append(os.path.join( os.getenv('HOME'),'src', 'icewebPy') )\n",
    "import IceWeb\n",
    "\n",
    "\n",
    "def get_sfile_list(SEISAN_DATA, DB, startdate, enddate): \n",
    "    \"\"\"\n",
    "    make a list of Sfiles between 2 dates\n",
    "    \"\"\"\n",
    "\n",
    "    event_list=[]\n",
    "    reapath = os.path.join(SEISAN_DATA, 'REA', DB)\n",
    "    years=list(range(startdate.year,enddate.year+1))\n",
    "    for year in years:\n",
    "        if year==enddate.year and year==startdate.year:\n",
    "            months=list(range(startdate.month,enddate.month+1))\n",
    "        elif year==startdate.year:\n",
    "            months=list(range(startdate.month,13))\n",
    "        elif year==enddate.year:\n",
    "            months=list(range(1,enddate.month+1))\n",
    "        else:\n",
    "            months=list(range(1,13))\n",
    "        for month in months:\n",
    "            #print month\n",
    "            yearmonthdir=os.path.join(reapath, \"%04d\" % year, \"%02d\" % month)\n",
    "            flist=sorted(glob(os.path.join(yearmonthdir,\"*L.S*\")))\n",
    "            for f in flist:\n",
    "                #fdt = sfilename2datetime(f)\n",
    "                fdt = spath2datetime(f)\n",
    "                #print(f, fdt)\n",
    "                if fdt>=startdate and fdt<enddate:\n",
    "                    event_list.append(f)\n",
    "    return event_list \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def WAV2picklefile(paths, shortperiod, correct_data):\n",
    "    if os.path.exists(paths['picklefile']):\n",
    "        st = read(paths['picklefile'])\n",
    "    else:\n",
    "        # We need to create the pickle file\n",
    "        # Try to read the WAV file\n",
    "        st = Stream()\n",
    "        print('Processing %s.' % paths['wavbase'], end=' ')\n",
    "        print('Reading.', end = ' ')\n",
    "        try:               \n",
    "            st = read(paths['wavfile'])\n",
    "        except:\n",
    "            print('ERROR. Could not load.')\n",
    "            return st\n",
    "\n",
    "        if len(st)==0:\n",
    "            print('ERROR. No traces.')\n",
    "            return st\n",
    "        else: \n",
    "            print('Success.')\n",
    "        \n",
    "        ######################### START OF CLEAN/CORRECT BLOCK ######################\n",
    "        print('Cleaning/correcting')\n",
    "        fix_trace_id(st, shortperiod=shortperiod) \n",
    "        #st=st.select(component='Z')\n",
    "\n",
    "        for tr in st:\n",
    "            this_inv = None            \n",
    "            if correct_data: # try to find corresponding station XML\n",
    "                this_inv = load_mvo_inventory(tr, paths['CALDIR'])\n",
    "            process_trace(tr, inv=None)\n",
    "        \n",
    "        # remove bad traces\n",
    "        for tr in st:    \n",
    "            if tr.stats.quality_factor <= 0.0:\n",
    "                st.remove(tr)  \n",
    "                \n",
    "        ######################### END OF CLEAN/CORRECT BLOCK ######################\n",
    "            \n",
    "        # Write pickle file. This is now the best place to load stream data from in future, so replaces\n",
    "        # seisan/WAV directory with seisan/PICKLE\n",
    "        print('Writing ',paths['picklefile'])\n",
    "        st.write(paths['picklefile'], format='PICKLE') \n",
    "    return st\n",
    "\n",
    "def Stream2logfile(st, paths):        \n",
    "    # save log file\n",
    "    if not os.path.exists(paths['logfile']):\n",
    "        print('Writing %s' % paths['logfile'])\n",
    "        with open(paths['logfile'],'w') as fout:\n",
    "            for tr in st:\n",
    "                # print trace history\n",
    "                pprint(tr.stats, stream=fout)\n",
    "                fout.write('\\n')\n",
    "                \n",
    "def Stream2png(st, paths):\n",
    "\n",
    "    if not os.path.exists(paths['seismicpngfile']):\n",
    "        print('Writing ',paths['seismicpngfile'])                \n",
    "        chosen = choose_best_traces(st, MAX_TRACES=99, include_seismic=True, \n",
    "                                    include_infrasound=False, include_uncorrected=False)\n",
    "        if len(chosen)>0:\n",
    "            st2 = select_by_index_list(st, chosen)\n",
    "            st2.plot(equal_scale=False, outfile=paths['seismicpngfile'])   \n",
    "            \n",
    "    if not os.path.exists(paths['infrasoundpngfile']):\n",
    "        print('Writing ',paths['infrasoundpngfile'])                \n",
    "        chosen = choose_best_traces(st, MAX_TRACES=99, include_seismic=False, \n",
    "                                    include_infrasound=True, include_uncorrected=False)\n",
    "        if len(chosen)>0:\n",
    "            st2 = select_by_index_list(st, chosen)\n",
    "            st2.plot(equal_scale=False, outfile=paths['infrasoundpngfile'])    \n",
    "            \n",
    "def add_ampengfft_metrics(st, paths):\n",
    "    print('Computing spectrogram data')        \n",
    "    iwsobj = IceWeb.icewebSpectrogram(stream=st)\n",
    "    iwsobj = iwsobj.precompute() # spectrograms data added\n",
    "    print('Computing spectrum.', end = ' ')  \n",
    "    iwsobj.compute_amplitude_spectrum(compute_bandwidth=True) # adds tr.stats.spectrum\n",
    "    for tr in iwsobj.stream:\n",
    "        ampengfft(tr, paths['PICKLEDIR']) # add peaktime, peakamp, energy\n",
    "    return iwsobj\n",
    "\n",
    "\n",
    "def plot_spectrograms(paths, iwsobj):\n",
    "    st = iwsobj.stream\n",
    "\n",
    "    # free scale  \n",
    "    print('Creating %s.' % paths['sgramfile'], end = ' ')            \n",
    "    titlestr = os.path.basename(paths['sgramfile']) \n",
    "    chosen = choose_best_traces(st, MAX_TRACES=10)            \n",
    "    iwsobj.plot(outfile=paths['sgramfile'], log=False, equal_scale=False, add_colorbar=True, dbscale=True, title=titlestr, trace_indexes=chosen);\n",
    "\n",
    "    # fixed scale \n",
    "    print('Creating %s.' % paths['sgramfixed'], end = ' ')\n",
    "    titlestr = os.path.basename(paths['sgramfixed'])\n",
    "    clim_in_dB = [-160, -100] # only works for corrected volcano-seismic data\n",
    "    clim_in_units = [ IceWeb.dB2amp(clim_in_dB[0]),  IceWeb.dB2amp(clim_in_dB[1]) ]\n",
    "    iwsobj.plot(outfile=paths['sgramfixed'], log=False, clim=clim_in_units, add_colorbar=True, dbscale=True, title=titlestr, trace_indexes=chosen);           \n",
    "    # need separate plots for any infrasound channels or uncorrected data\n",
    "\n",
    "def metrics2tracedf(traceCSVfile, st):\n",
    "    if os.path.exists(traceCSVfile):\n",
    "        tracedf = pd.read_csv(traceCSVfile)\n",
    "    else: \n",
    "        print('- Building metrics dataframe.', end = ' ') \n",
    "        tracedf = pd.DataFrame()\n",
    "        list_of_tracerows = []\n",
    "        for tr in st:\n",
    "            s = tr.stats\n",
    "            tracerow = {'id':tr.id, 'starttime':s.starttime, \n",
    "                   'Fs':s.sampling_rate, \n",
    "                   'calib':s.calib, 'units':s.units, \n",
    "                   'quality':s.quality_factor}\n",
    "            if 'spectrum' in s: \n",
    "                for item in ['medianF', 'peakF', 'peakA', 'bw_min', 'bw_max']:\n",
    "                    try:\n",
    "                        tracerow[item] = s.spectrum[item]\n",
    "                    except:\n",
    "                        pass\n",
    "            if 'metrics' in s:\n",
    "                m = s.metrics\n",
    "                for item in ['snr', 'signal_level', 'noise_level', 'twin',\n",
    "                             'peakamp', 'peaktime', 'energy', 'RSAM_high', 'RSAM_low',\n",
    "                             'sample_min', 'sample_max', 'sample_mean', 'sample_median', \n",
    "                             'sample_lower_quartile', 'sample_upper_quartile', 'sample_rms', \n",
    "                             'sample_stdev', 'percent_availability', 'num_gaps', 'skewness', 'kurtosis']:\n",
    "                             #'start_gap', 'num_gaps', 'end_gap', 'sum_gaps', 'max_gap', \n",
    "                             #'num_overlaps', 'sum_overlaps', 'num_records', 'record_length', \n",
    "                    try:\n",
    "                        tracerow[item] = m[item]\n",
    "                    except:\n",
    "                        pass \n",
    "            if 'bandratio' in s:\n",
    "                for dictitem in s['bandratio']:\n",
    "                    label = 'bandratio_' +  \"\".join(str(dictitem['freqlims'])).replace(', ','_')\n",
    "                    tracerow[label] = dictitem['RSAM_ratio']\n",
    "\n",
    "            list_of_tracerows.append(tracerow)\n",
    "        tracedf = pd.DataFrame(list_of_tracerows)\n",
    "        tracedf = tracedf.round({'Fs': 2, 'secs': 2, 'quality':2, 'medianF':1, 'peakF':1, 'bw_max':1, 'bw_min':1, 'peaktime':2, 'twin':2, 'skewness':2, 'kurtosis':2})\n",
    "        print('Saving to CSV.')\n",
    "        tracedf.set_index('id')\n",
    "        tracedf.to_csv(traceCSVfile)\n",
    "    return tracedf\n",
    "\n",
    "def tracedf2eventdf(tracedf, st, paths, correct_data, detect_event, make_png_files):\n",
    "    # Summarize event\n",
    "    print('Create a summary row for whole event')\n",
    "    numOfRows = tracedf.shape[0]\n",
    "    if correct_data:\n",
    "        df = tracedf[tracedf[\"units\"] == 'm/s']\n",
    "        if len(df.index)==0:\n",
    "            df = tracedf\n",
    "    else:\n",
    "        df = tracedf\n",
    "    df.sort_values(by=['quality'], inplace=True)\n",
    "    df = df.head(10) # get median of 10 best rows    \n",
    "    wavrow = df.median(axis = 0, skipna = True).to_dict()        \n",
    "    wavrow['path']=paths['wavfile']\n",
    "    wavrow['num_traces']=numOfRows\n",
    "    filetime=df.iloc[0]['starttime']\n",
    "    wavrow['filetime']=filetime\n",
    "    try:\n",
    "        wavrow['year']=filetime[0:4]\n",
    "        wavrow['month']=filetime[5:7]\n",
    "        wavrow['day']=filetime[8:10]\n",
    "        wavrow['hour']=filetime[11:13]\n",
    "        wavrow['minute']=filetime[14:16]\n",
    "        wavrow['second']=filetime[17:23]\n",
    "    except:\n",
    "        wavrow['year']=filetime.year\n",
    "        wavrow['month']=filetime.month\n",
    "        wavrow['day']=filetime.day\n",
    "        wavrow['hour']=filetime.hour\n",
    "        wavrow['minute']=filetime.minute\n",
    "        wavrow['second']=filetime.second\n",
    "    \n",
    "    if detect_event:   \n",
    "        trig, ontimes, offtimes = detect_network_event(st, sta=0.4, lta=5.0, threshon=4.0, threshoff=0.24, pad=5.0)\n",
    "        print('%s: %d events detected' % (paths['picklefile'], len(ontimes)))\n",
    "        durations = [t['duration'] for t in trig]\n",
    "        if len(durations)>0:\n",
    "            bestevent = np.argmax(durations)\n",
    "            thistrig=trig[int(np.argmax(durations))]\n",
    "            wavrow['ontime'] = thistrig['time']\n",
    "            wavrow['offtime']=thistrig['time']+thistrig['duration']  \n",
    "            wavrow['trigger_duration']=thistrig['duration']\n",
    "            for item in ['coincidence_sum', 'cft_peak_wmean', 'cft_std_wmean']:\n",
    "                wavrow[item]=thistrig[item]\n",
    "            wavrow['detection_quality']=thistrig['coincidence_sum']*thistrig['cft_peak_wmean']*thistrig['cft_std_wmean']\n",
    "            \n",
    "        if make_png_files:\n",
    "            chosen = choose_best_traces(st, MAX_TRACES=1, include_seismic=True, \n",
    "                                include_infrasound=False, include_uncorrected=False)\n",
    "            tr = st[chosen[0]] \n",
    "            plt.figure()\n",
    "            plt.plot(tr.times(), tr.data)\n",
    "            plt.ylabel(tr.id)   \n",
    "            t0 = tr.stats.starttime\n",
    "            bottom, top = plt.ylim()\n",
    "            plt.vlines([wavrow['ontime']-t0, wavrow['offtime']-t0], bottom, top )\n",
    "            plt.savefig(paths['detectionfile'])            \n",
    "\n",
    "    return wavrow\n",
    "\n",
    "def wavfile2paths(wavfile):\n",
    "    paths={}\n",
    "    paths['WAVDIR'] = os.path.dirname(wavfile)\n",
    "    paths['wavfile'] = wavfile\n",
    "    paths['wavbase'] = os.path.basename(wavfile)\n",
    "    parts = paths['WAVDIR'].split('WAV')\n",
    "    paths['CALDIR'] = os.path.join(parts[0],'CAL')    \n",
    "    paths['HTMLDIR'] = paths['WAVDIR'].replace('WAV', 'HTML')\n",
    "    paths['PICKLEDIR'] = paths['WAVDIR'].replace('WAV', 'PICKLE')           \n",
    "    paths['seismicpngfile'] = os.path.join(paths['HTMLDIR'], paths['wavbase'] + '_seismic.png')\n",
    "    paths['infrasoundpngfile'] = os.path.join(paths['HTMLDIR'], paths['wavbase'] + '_infrasound.png')\n",
    "    paths['picklefile'] = os.path.join(paths['PICKLEDIR'], paths['wavbase'] + '.pickle')\n",
    "    paths['logfile'] = paths['picklefile'].replace('.pickle', '.log')\n",
    "    paths['sgramfile'] = os.path.join(paths['HTMLDIR'], paths['wavbase'] + '_sgram.png')\n",
    "    paths['sgramfixed'] = paths['sgramfile'].replace('_sgram.png', '_sgram_fixed.png')  \n",
    "    paths['traceCSVfile'] = paths['picklefile'].replace('.pickle', '.csv')\n",
    "    paths['detectionfile'] = os.path.join(paths['HTMLDIR'], paths['wavbase'] + '_detection.png')\n",
    "    return paths\n",
    "    \n",
    "    \n",
    "def processWAV(wavfile, shortperiod=False, correct_data=False, make_png_files=False, detect_event=False, compute_aef=True):\n",
    "    paths = wavfile2paths(wavfile)\n",
    "    if not os.path.exists(paths['HTMLDIR']) and make_png_files:\n",
    "        os.makedirs(paths['HTMLDIR'])\n",
    "    if not os.path.exists(paths['PICKLEDIR']):\n",
    "        os.makedirs(paths['PICKLEDIR'])  \n",
    "    wavrow={}\n",
    "    \n",
    "    st = WAV2picklefile(paths, shortperiod, correct_data)\n",
    "    st.select(component='[ENZ]') # subset to seismic components only\n",
    "    if len(st)==0:\n",
    "        return wavrow\n",
    "    Stream2logfile(st, paths)\n",
    "    if make_png_files:\n",
    "        Stream2png(st, paths)\n",
    "        \n",
    "    if compute_aef and not 'energy' in st[0].stats.metrics: \n",
    "        iwsobj = add_ampengfft_metrics(st, paths)\n",
    "        if not os.path.exists(paths['sgramfile']) and make_png_files:\n",
    "            plot_spectrograms(paths, iwsobj)\n",
    "        for tr in iwsobj.stream:\n",
    "            tr.stats.pop('spectrogramdata', None) # Remove spectrogramdata as it is too large for picklefile\n",
    "            #tr.stats.spectrogramdata = {}\n",
    "        print('Writing enhanced pickle file.')     \n",
    "        st = iwsobj.stream\n",
    "        st.write(paths['picklefile'], 'PICKLE') # Rewrite pickle file with extra attributes     \n",
    "        \n",
    "    tracedf = metrics2tracedf(paths['traceCSVfile'], st)\n",
    "    wavrow = tracedf2eventdf(tracedf, st, paths, correct_data, detect_event, make_png_files)        \n",
    "    return wavrow\n",
    "\n",
    "\n",
    "def processSeisanYearMonth(SEISAN_DATA, DB, YYYY, MM, filesdone, MAXFILES=999999):\n",
    "    failedWAVfiles=[]\n",
    "    LoD = []\n",
    "    \n",
    "    # We aim to add a couple of columns from the S-file, and save to this\n",
    "    reawavCSVfile=os.path.join(SEISAN_DATA, 'reawav_%s%s%s.csv' % (DB, YYYY, MM) )\n",
    "    if os.path.exists(reawavCSVfile):\n",
    "        return failedWAVfiles, filesdone\n",
    "    \n",
    "    # Get s-file list\n",
    "    startdate = dt.datetime(int(YYYY), int(MM), 1)\n",
    "    if int(MM)<12:\n",
    "        enddate = dt.datetime(int(YYYY), int(MM)+1, 1)\n",
    "    else:\n",
    "        enddate = dt.datetime(int(YYYY)+1, 1, 1)\n",
    "    slist = sorted(get_sfile_list(SEISAN_DATA, DB, startdate, enddate))\n",
    "    \n",
    "    for i,sfile in enumerate(slist):\n",
    "        print('Processing %d of %d: %s' % (i, len(slist), sfile) )\n",
    "        if i==MAXFILES:\n",
    "            break\n",
    "            \n",
    "        s = Sfile(sfile, use_mvo_parser=True)\n",
    "        #s.cat()\n",
    "        #s.printEvents()\n",
    "        d = s.to_dict()\n",
    "        #pprint(d)\n",
    "        \n",
    "        for item in ['wavfile1', 'wavfile2']:\n",
    "            if d[item]:\n",
    "                if os.path.exists(d[item]):\n",
    "                    wavbase = os.path.basename(d[item])\n",
    "                    if 'MVO' in wavbase:\n",
    "                        print('Processing ',d[item])\n",
    "                        eventrow=[]\n",
    "                        eventrow = processWAV(d[item], shortperiod=bool_shortperiod, correct_data=bool_correct_data, make_png_files=bool_make_png_files, detect_event=bool_detect_event)\n",
    "                        if eventrow:\n",
    "                            eventrow['sfile']=os.path.basename(s.path)\n",
    "                            eventrow['mainclass']=s.mainclass\n",
    "                            eventrow['subclass']=s.subclass\n",
    "                            LoD.append(eventrow)\n",
    "                            filesdone += 1\n",
    "                            if filesdone >= MAXFILES:\n",
    "                                break\n",
    "    if LoD:\n",
    "        df = pd.DataFrame(LoD)\n",
    "        print('Writing ',reawavCSVfile)\n",
    "        df.drop(df.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "        df = df.set_index('filetime')       \n",
    "        df.to_csv(reawavCSVfile, index=True) \n",
    "    return failedWAVfiles, filesdone\n",
    "\n",
    "def main(DB, MAXFILES=999999):\n",
    "    filesdone = 0\n",
    "    yeardirs = sorted(glob(os.path.join('REA',DB,'[12]???')))\n",
    "    for yeardir in yeardirs:\n",
    "        YYYY = os.path.basename(yeardir)\n",
    "        monthsdirs = sorted(glob(os.path.join(yeardir,'[01]?')))\n",
    "        for monthdir in monthsdirs:\n",
    "            if filesdone>=MAXFILES:\n",
    "                break\n",
    "            MM = os.path.basename(monthdir)\n",
    "            print('**** Processing %s ****' % monthdir)\n",
    "            failedWAVfiles, filesdone = processSeisanYearMonth('.', DB, YYYY, MM, filesdone, MAXFILES=999999)\n",
    "            if len(failedWAVfiles)>0:\n",
    "                fptr=open('failedWAVfiles.txt','a')\n",
    "                for element in failedWAVfiles:\n",
    "                    fptr.write(element)\n",
    "                    fptr.write('\\n')\n",
    "                fptr.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEISAN_DATA = os.path.join( os.getenv('HOME'),'DATA','MVO')\n",
    "os.chdir(SEISAN_DATA)\n",
    "SEISAN_DB = 'MVOE_'\n",
    "bool_shortperiod=False\n",
    "bool_correct_data=True\n",
    "bool_make_png_files=False\n",
    "bool_detect_event=True\n",
    "main(SEISAN_DB, 999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stationxml(paths, shortperiod, correct_data):\n",
    "    #if os.path.exists(paths['picklefile']): # removed\n",
    "    if 0: # added\n",
    "        st = read(paths['picklefile'])\n",
    "    else:\n",
    "        # We need to create the pickle file\n",
    "        # Try to read the WAV file\n",
    "        st = Stream()\n",
    "        print('Processing %s.' % paths['wavbase'], end=' ')\n",
    "        print('Reading.', end = ' ')\n",
    "        try:               \n",
    "            st = read(paths['wavfile'])\n",
    "        except:\n",
    "            print('ERROR. Could not load.')\n",
    "            return st\n",
    "\n",
    "        if len(st)==0:\n",
    "            print('ERROR. No traces.')\n",
    "            return st\n",
    "        else: \n",
    "            print('Success.')\n",
    "        \n",
    "        ######################### START OF CLEAN/CORRECT BLOCK ######################\n",
    "        print('Cleaning/correcting')\n",
    "        print(st) # added\n",
    "        fix_trace_id(st, shortperiod=shortperiod) \n",
    "        print(st) # added\n",
    "        #st=st.select(component='Z')\n",
    "\n",
    "        for tr in st:\n",
    "\n",
    "            this_inv = None\n",
    "            \n",
    "            if correct_data: # try to find corresponding station XML\n",
    "                #tr.stats.network = 'MV' \n",
    "                #xmlfile = os.path.join(caldir, \"station.%s.%s.xml\" % (tr.stats.network, tr.stats.station) )\n",
    "                if tr.stats.channel[0] in 'ES':\n",
    "                    matchcode = '[ES]'\n",
    "                elif tr.stats.channel[0] in 'BH':\n",
    "                    matchcode = '[BH]'\n",
    "                xmlfilepattern = os.path.join(paths['CALDIR'], \"station.MV.%s..%s*%s.xml\" % (tr.stats.station, matchcode, tr.stats.channel[2]) )\n",
    "                #print(xmlfilepattern)\n",
    "                xmlfiles = glob(os.path.join(paths['CALDIR'], \"station.MV.%s..%s*%s.xml\" % (tr.stats.station, matchcode, tr.stats.channel[2]) ))\n",
    "\n",
    "                N = len(xmlfiles)\n",
    "                if N==1:\n",
    "                    xmlfile = xmlfiles[0]\n",
    "                    this_inv = read_inventory(xmlfile)   \n",
    "                    print('Processing %s with %s' % (tr.id, xmlfile) )\n",
    "                    process_trace(tr, inv=this_inv)\n",
    "                    print('Done')\n",
    "    return st\n",
    " \n",
    "\n",
    "correct_data = True\n",
    "shortperiod = False\n",
    "wavfile = '/Users/thompsong/DATA/MVO/WAV/MVOE_/2002/08/2002-08-01-0148-43S.MVO___014'\n",
    "paths = wavfile2paths(wavfile)\n",
    "st = find_stationxml(paths, shortperiod, correct_data)\n",
    "st.plot(equal_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.select(component='[ENZ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
