{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montserrat event selector for Machine Learning\n",
    "The aim of this code is to find the best N events of each type, and create a corresponding CSV file and data structure for entry into Alexis' and Marielle's AAA codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "## SCAFFOLD. It keeps giving me the same Regional over and over.\n",
    "# Is the CSV file corrupt? Or is there a bug in the code?\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "LIBpath = os.path.join( os.getenv('HOME'),'src','kitchensinkGT', 'LIB')\n",
    "sys.path.append(LIBpath)\n",
    "from libseisGT import add_to_trace_history #, mulplt\n",
    "from modutils import yn_choice\n",
    "\n",
    "from obspy import read_inventory #, remove_response\n",
    "from libMVO import fix_trace_id, inventory_fix_id_mvo, load_mvo_inventory\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(cwd)\n",
    "\n",
    "\n",
    "\n",
    "def read_volcano_def(volcanodefcsv):\n",
    "    subclass_df = pd.read_csv(volcanodefcsv)\n",
    "    subclass_df.columns = subclass_df.columns.str.lstrip()\n",
    "    return subclass_df\n",
    "\n",
    "\n",
    "\n",
    "def build_master_event_catalog(csvdir, seisandbname, catalogfile, subclasses_for_ML, max_duration = 300):\n",
    "    # load all the year/month CSV files\n",
    "    csvfiles = glob(os.path.join(csvdir, 'reawav_%s??????.csv' % seisandbname))\n",
    "    frames = []\n",
    "    for csvfile in csvfiles:\n",
    "        df = pd.read_csv(csvfile)\n",
    "        frames.append(df) \n",
    "    dfall = pd.concat(frames, sort=True)\n",
    "    dfall.set_index('filetime', inplace=True) # we will need this later to remerge\n",
    "    dfall.sort_index(inplace=True)\n",
    "    \"\"\"\n",
    "    for index, row in dfall.iterrows():\n",
    "        \n",
    "        # For simplicity, copy 'D' and 'R' mainclass to subclass\n",
    "        if row['mainclass'] in ['D', 'R']: # Do I need L here too?\n",
    "            dfall.loc[index, 'subclass']=row['mainclass']\n",
    "    \"\"\" \n",
    "    # replace loop above\n",
    "    for mainclass in ['R', 'D']:\n",
    "        dfall.loc[dfall['mainclass'] == mainclass, 'subclass'] = mainclass\n",
    "    \n",
    "    # Drop the mainclass column, as it is now superfluous.\n",
    "    dfall.drop(columns=['mainclass'], inplace=True)\n",
    "    \n",
    "    # Add an etype column\n",
    "    #dfall['etype'] = dfall['subclass'].replace(subclasses, etypes)\n",
    "    \n",
    "    # Add columns to assign a percentage for each subclass\n",
    "    for subclass in subclasses_for_ML:\n",
    "        dfall[subclass] = 0\n",
    "    \n",
    "    # But set column for actual subclass to 100%  \n",
    "    \"\"\"\n",
    "    for index, row in dfall.iterrows():            \n",
    "        # Set dfall['h'] to 100 if dfall['subclass']=='h', etc\n",
    "        dfall.loc[index, row['subclass']] = 100        \n",
    "    \"\"\"\n",
    "    # replace row operations above\n",
    "    for subclass in subclasses_for_ML:\n",
    "        dfall.loc[dfall['subclass'] == subclass, subclass] = 100\n",
    "        \n",
    "    # Add a new_subclass column\n",
    "    dfall['new_subclass'] = dfall['subclass']\n",
    "\n",
    "    # Add weight column. I will give really clear events higher weight when I process them\n",
    "    dfall['weight']=3 # weight for events I have not visually checked\n",
    "    \n",
    "    # Add column that records if event is checked\n",
    "    dfall['checked']=False\n",
    "    \n",
    "    # Add column that records if event is marked for splitting\n",
    "    dfall['split']=False    \n",
    "    \n",
    "    # Add column that records if event is marked for deletion\n",
    "    dfall['delete']=False\n",
    "    \n",
    "    # Add column that records if event should be ignored\n",
    "    # Ignore any events longer than 1-minute, as they are likely to contain multiple events \n",
    "    # or just be unhelpful for classifying short signals which are more common\n",
    "    # SCAFFOLD - the twin column no longer seems to exist\n",
    "    #dfall['ignore'] = dfall['twin']>max_duration\n",
    "    dfall['ignore'] = False\n",
    "    \n",
    "    # Now we have a catalog dataframe we can work with. Let's save this.\n",
    "    dfall.to_csv(catalogfile)\n",
    "    \n",
    "    return dfall\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_STATION0HYP(station0hypfile):\n",
    "    \"\"\" file sample\n",
    "    012345678901234567890123456 \n",
    "      MWNH1644.53N 6211.45W 407\n",
    "      MWNL1644.53N 6211.45W 407\n",
    "      MWZH1644.53N 6211.45W 407\n",
    "      MWZL1644.53N 6211.45W 407\n",
    "      0123456789012345678901234 \n",
    "    \"\"\"\n",
    "    station_locations = []\n",
    "    if os.path.exists(station0hypfile):\n",
    "        fptr = open(station0hypfile,'r')\n",
    "        for line in fptr:\n",
    "            line = line.strip()\n",
    "            if len(line)==25:\n",
    "                station = line[0:4]\n",
    "                latdeg = line[4:6]\n",
    "                if latdeg != '16':\n",
    "                    print(line)\n",
    "                    continue\n",
    "                latmin = line[6:8]\n",
    "                latsec = line[9:11]\n",
    "                hemisphere = line[11]\n",
    "                if hemisphere == 'N':\n",
    "                    latsign = 1\n",
    "                elif hemisphere == 'S':\n",
    "                    latsign = -1 \n",
    "                else:\n",
    "                    print(line)\n",
    "                    continue\n",
    "                londeg = line[13:15]\n",
    "                lonmin = line[15:17]\n",
    "                lonsec = line[18:20]\n",
    "                lonsign = 1\n",
    "                if line[20]=='W':\n",
    "                    lonsign = -1\n",
    "                elev = line[21:25].strip()\n",
    "                station_dict = {}\n",
    "                station_dict['name'] = station\n",
    "                station_dict['lat'] = (float(latdeg) + float(latmin)/60 + float(latsec)/3600) * latsign\n",
    "                station_dict['lon'] = (float(londeg) + float(lonmin)/60 + float(lonsec)/3600) * lonsign\n",
    "                station_dict['elev'] = float(elev)\n",
    "                \n",
    "                station_locations.append(station_dict)\n",
    "        fptr.close()\n",
    "        return pd.DataFrame(station_locations)\n",
    "    \n",
    "    \n",
    "    \n",
    "def _select_best_events(df, allowed_subclasses, N=100, exclude_checked=True):\n",
    "    # When we are iterating, we want to exclude the checked events, else we are repeating our work.\n",
    "    # When we return our final list, we do not want to exclude checked events\n",
    "    all_subclasses = df['new_subclass'].unique()\n",
    "    best_events_dict = {}\n",
    "    for subclass in all_subclasses:\n",
    "        if subclass in allowed_subclasses:\n",
    "            #print('\\nProcessing subclass=%s' % subclass)\n",
    "            is_subclass =  df['new_subclass']==subclass\n",
    "            df_subclass = df[is_subclass]\n",
    "            \n",
    "            # mechanism to weight out checked events\n",
    "            df_subclass['include'] = 1 - df_subclass['ignore']\n",
    "            if exclude_checked:\n",
    "                print('Excluding checked events')\n",
    "                df_subclass['include'] = df_subclass['include'] * (1 - df_subclass['checked'])\n",
    "            \n",
    "            if len(df_subclass.index)>0:\n",
    "                # we use three criteria for ranking events. detection_quality has the largest magnitude, but can be missing, so we also add snr, \n",
    "                # and finally quality as a tie-braker, since it has a small range for events that have made it this far\n",
    "                df_subclass['sortcol'] = df_subclass['quality'] # always present\n",
    "                if 'snr' in df_subclass.columns:\n",
    "                    df_subclass['sortcol'] = df_subclass['sortcol'] + df_subclass['snr']\n",
    "                if 'detection_quality' in df_subclass.columns:\n",
    "                    df_subclass['sortcol'] = df_subclass['sortcol'] + df_subclass['detection_quality']                    \n",
    "                df_subclass['sortcol'] = df_subclass['sortcol'] * df_subclass['weight'] * df_subclass['include']\n",
    "                df_subclass.drop(columns=['include'], inplace=True)\n",
    "\n",
    "                L = len(df_subclass.index)\n",
    "                H = int(min([L, N]))\n",
    "                print('Selecting %d events of type %s from a total of %d' % (H, subclass, L))\n",
    "                df_subclass.sort_values(by=['sortcol'], ascending=False, inplace=True)\n",
    "                df_subclass.drop(columns=['sortcol'], inplace=True)\n",
    "                df_subclass = df_subclass.head(H)\n",
    "                if 'bandratio_[1.0_6.0_11.0]' in df_subclass.columns:\n",
    "                    df_subclass.rename(columns = {'bandratio_[1.0_6.0_11.0]':'band_ratio'}, inplace = True)\n",
    "                best_events_dict[subclass]=df_subclass\n",
    "\n",
    "    return best_events_dict\n",
    "    \n",
    "def get_fingerprints(dfall, allowed_subclasses, N=100, exclude_checked=False):\n",
    "    \"\"\"\n",
    "    All we do right now is a dataframe describe, so we return the stats of each column.\n",
    "    \n",
    "    wdf = DescrStatsW(df.x, weights=df.wt, ddof=1) \n",
    "    \n",
    "    \"\"\"\n",
    "    #df = best_events_dict.groupby(\"subclass\")\n",
    "    fingerprints = {}\n",
    "    best_events_dict = _select_best_events(dfall, allowed_subclasses, N=N, exclude_checked=exclude_checked)\n",
    "    for subclass in allowed_subclasses:\n",
    "        if subclass in best_events_dict.keys(): \n",
    "            print('Computing fingerprint for subclass ',subclass)\n",
    "            #df_subclass = df.get_group(subclass)\n",
    "            df_subclass = best_events_dict[subclass]     \n",
    "            fingerprints[subclass] = df_subclass[[ 'peaktime', \n",
    "                'kurtosis', 'medianF', 'peakF', 'bw_min', 'bw_max', 'band_ratio']].describe()\n",
    "            #print(fingerprints[subclass])\n",
    "        else:\n",
    "            fingerprints[subclass]=pd.read_csv('fingerprint_%s.csv' % subclass)\n",
    "    return fingerprints\n",
    "\n",
    "\"\"\"    \n",
    "from statsmodels.stats.weightstats import DescrStatsW    \n",
    "def get_weighted_fingerprints(dfall, subclasses_for_ML, N=300, exclude_checked=False):\n",
    "\n",
    "    fingerprints = {}\n",
    "    best_events_dict = _select_best_events(dfall, subclasses_for_ML, N=N, exclude_checked=exclude_checked)\n",
    "    for subclass in subclasses_for_ML:\n",
    "        if subclass in best_events_dict.keys(): \n",
    "            print('Computing fingerprint for subclass ',subclass)\n",
    "            thisdf = best_events_dict[subclass]\n",
    "            if len(thisdf.index)>30:\n",
    "                statsdf = pd.DataFrame()\n",
    "                statsdf['statistic'] = ['mean', 'std', '25%', '50%', '75%']\n",
    "                statsdf.set_index(['statistic'], inplace = True)\n",
    "\n",
    "                for col in [ 'peaktime', 'kurtosis', 'medianF', 'peakF', 'bw_min', 'bw_max', 'band_ratio']:\n",
    "                    # compute mean, std, median, 25% percentile, 75% percentile\n",
    "                    wdf = DescrStatsW(thisdf[col], weights=thisdf['weight'].astype(float)*thisdf[subclass].astype(float), ddof=1)\n",
    "                    p = [0.25,0.50,0.75]\n",
    "                    q  = wdf.quantile(p) \n",
    "                    statsdf.loc['mean', col] = wdf.mean\n",
    "                    statsdf.loc['std', col] = wdf.std\n",
    "                    statsdf.loc['50%', col] = q[p[1]]\n",
    "                    statsdf.loc['25%', col] = q[p[0]]\n",
    "                    statsdf.loc['75%', col] = q[p[2]]\n",
    "\n",
    "                fingerprints[subclass] = statsdf\n",
    "                print(fingerprints[subclass])\n",
    "    return fingerprints     \n",
    "\"\"\"\n",
    " \n",
    "    \n",
    "\n",
    "def save_fingerprints(fingerprints, allowed_subclasses):\n",
    "    for subclass in allowed_subclasses:\n",
    "        if subclass in fingerprints.keys(): \n",
    "            fingerprints[subclass].to_csv('fingerprint_%s.csv' % subclass)\n",
    "            \n",
    "\n",
    "def _select_next_event(dfall, subclasses_for_ML):\n",
    "    \"\"\" The goal here is just to pick the next event of a particular subclass from the unchecked events \"\"\"\n",
    "    checked = dfall[dfall['checked']==True]\n",
    "    unchecked = dfall[dfall['checked']==False]\n",
    "    subclasses = subclasses_for_ML.copy()\n",
    "    \n",
    "    tryagain = True\n",
    "    while tryagain and len(subclasses)>0:\n",
    "    \n",
    "        # Check how many we have of each class\n",
    "        counts = None\n",
    "        nextclass = 'r'\n",
    "        mincounts = 99999\n",
    "\n",
    "        for subclass in subclasses:\n",
    "            dfs = checked[checked['new_subclass']==subclass]\n",
    "            counts=len(dfs.index)\n",
    "            if counts<mincounts:\n",
    "                mincounts=counts\n",
    "                nextclass=subclass\n",
    "        print('next event is of subclass = %s' % nextclass)\n",
    "\n",
    "        is_subclass = unchecked['subclass']==nextclass\n",
    "        df = unchecked[is_subclass]\n",
    "\n",
    "        # mechanism to weight out checked events\n",
    "        df = df[df['ignore']==False]\n",
    "\n",
    "        if len(df.index)>0:\n",
    "            # we use three criteria for ranking events. detection_quality has the largest magnitude, but can be missing, so we also add snr, \n",
    "            # and finally quality as a tie-braker, since it has a small range for events that have made it this far\n",
    "            df['sortcol'] = df['quality'] # always present\n",
    "            if 'snr' in df.columns:\n",
    "                df['sortcol'] = df['sortcol'] + df['snr']\n",
    "            if 'detection_quality' in df.columns:\n",
    "                df['sortcol'] = df['sortcol'] + df['detection_quality']                    \n",
    "            df['sortcol'] = df['sortcol'] * df['weight']\n",
    "            #df.drop(columns=['include'], inplace=True)\n",
    "            df.sort_values(by=['sortcol'], ascending=False, inplace=True)\n",
    "            df.drop(columns=['sortcol'], inplace=True)\n",
    "            df = df.head(1)\n",
    "            if 'bandratio_[1.0_6.0_11.0]' in df.columns:\n",
    "                df.rename(columns = {'bandratio_[1.0_6.0_11.0]':'band_ratio'}, inplace = True)\n",
    "            return df\n",
    "        else:\n",
    "            subclasses.remove(nextclass)\n",
    "    return None\n",
    "\n",
    "from obspy.core import read\n",
    "def _deconvolve_instrument_response(st):\n",
    "    for tr in st:\n",
    "        this_inv = None\n",
    "        need_to_correct = False\n",
    "        if 'units' in tr.stats:\n",
    "            if 'units'=='Counts':\n",
    "                need_to_correct = True\n",
    "        else:\n",
    "            need_to_correct = True\n",
    "        if need_to_correct:           \n",
    "            if bool_correct_data: # try to find corresponding station XML\n",
    "                this_inv = load_mvo_inventory(tr, '/Users/thompsong/DATA/MVO/CAL')\n",
    "        if this_inv and tr.stats['units'] == 'Counts':\n",
    "            tr.remove_response(inventory=this_inv, output=\"VEL\")\n",
    "            tr.stats['units'] = 'm/s'\n",
    "            add_to_trace_history(tr, 'deconvolved')\n",
    "        #else:\n",
    "        #    st.remove(tr)\n",
    "\n",
    "\n",
    "def add_station_locations(st, station_locationsDF):\n",
    "    for tr in st:\n",
    "        df = station_locationsDF[station_locationsDF['name'] == tr.stats.station]\n",
    "        df = df.reset_index()\n",
    "        if len(df.index)==1:\n",
    "            row = df.iloc[0]\n",
    "            tr.stats['lat'] = row['lat']\n",
    "            tr.stats['lon'] = row['lon']\n",
    "            tr.stats['elev'] = row['elev']\n",
    "        else:\n",
    "            print('Found %d matching stations' % len(index))\n",
    "            \n",
    "def plot_amplitude_locations(st):\n",
    "    plt.figure()\n",
    "    x = []\n",
    "    y = []\n",
    "    s = []\n",
    "    for tr in st:\n",
    "        if 'lon' in tr.stats:\n",
    "            x.append(tr.stats.lon)\n",
    "            y.append(tr.stats.lat)\n",
    "            s.append(np.max(np.absolute(tr.data)))\n",
    "    if len(s)>0:\n",
    "        maxs = np.max(s)\n",
    "        s = s/maxs * 50\n",
    "        plt.scatter(x, y, s, facecolors='none', edgecolors='r');\n",
    "        for i, label in enumerate(tr.stats.station):\n",
    "            #if tr.stats.channel[-1]=='Z':\n",
    "            plt.text(x[i]*1.01, y[i]*1.01, label)\n",
    "\n",
    "        volcano_dict = {}\n",
    "        volcano_dict['name']='VOLC'\n",
    "        volcano_dict['lat']=16.7166638\n",
    "        volcano_dict['lon']=-62.1833326\n",
    "        volcano_dict['elev']=1000\n",
    "        plt.scatter(volcano_dict['lon'], volcano_dict['lat'], 300, marker='*')\n",
    "        plt.show();\n",
    "        \n",
    "def _guess_subclass(row, fingerprints, subclasses_for_ML):\n",
    "    chance_of = {}\n",
    "    for item in subclasses_for_ML:\n",
    "        chance_of[item]=0.0\n",
    "    \n",
    "    params = ['peaktime', 'kurtosis', 'medianF', 'peakF', 'bw_min', 'bw_max', 'band_ratio']\n",
    "    for subclass in fingerprints.keys():\n",
    "        fingerprint = fingerprints[subclass]\n",
    "        #fingerprint.reset_index(inplace=True)\n",
    "        #print(fingerprint.columns)\n",
    "        for param in params:\n",
    "            thisval = row[param]\n",
    "            \n",
    "            # test against mean+/-std\n",
    "            meanval = fingerprint[param]['mean']\n",
    "            stdval = fingerprint[param]['std']\n",
    "            minus1sigma = meanval - stdval\n",
    "            plus1sigma = meanval + stdval\n",
    "            \n",
    "            if thisval > minus1sigma and thisval < plus1sigma:\n",
    "                weight = 1.0 - abs(thisval-meanval)/stdval\n",
    "                chance_of[subclass] += weight\n",
    "                \n",
    "            # test against 25-75% percentile\n",
    "            medianval = fingerprint[param]['50%']\n",
    "            val25 = fingerprint[param]['25%']\n",
    "            val75 = fingerprint[param]['75%']\n",
    "            if thisval > val25 and thisval < val75:\n",
    "                if thisval < medianval:\n",
    "                    weight = 1.0 - (medianval-thisval)/(medianval-val25)\n",
    "                else:\n",
    "                    weight = 1.0 - (thisval-medianval)/(val75-medianval)\n",
    "                chance_of[subclass] += weight\n",
    "    \n",
    "    print('The event is classified as %s, but here are our guesses:' % row['subclass'])\n",
    "    total = 0\n",
    "    for subclass in chance_of.keys():\n",
    "        total += chance_of[subclass]\n",
    "    for subclass in chance_of.keys():\n",
    "        if total>0:\n",
    "            print('%s: %.0f' % (subclass, 100*chance_of[subclass]/total), end=', ')\n",
    "            \n",
    "            \n",
    "def qc_event(dfall, subclasses_for_ML, seisan_subclasses, fingerprints, SEISAN_DATA, station_locationsDF):\n",
    "    df = _select_next_event(dfall, subclasses_for_ML)\n",
    "    subclass = df.iloc[0]['new_subclass']\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        clear_output(wait=True)\n",
    "        picklepath = os.path.join(SEISAN_DATA, row.path.replace('./WAV','PICKLE') + '.pickle')\n",
    "        \n",
    "        if os.path.exists(picklepath):\n",
    "            \n",
    "            # plot whole event file\n",
    "            st = read(picklepath) #.select(station='MBWH')\n",
    "            st.filter('bandpass', freqmin=0.5, freqmax=25.0, corners=4)\n",
    "            _deconvolve_instrument_response(st)\n",
    "\n",
    "            # compute ampeng to show later\n",
    "            traceids = []\n",
    "            energies = []\n",
    "            amplitudes = []\n",
    "            for tr in st:\n",
    "                amp = np.max(np.absolute(tr.data))\n",
    "                energy = np.sum(np.square(tr.data))/tr.stats.sampling_rate\n",
    "                traceids.append(tr.id)\n",
    "                amplitudes.append(amp)\n",
    "                energies.append(energy)\n",
    "            dfenergy = pd.DataFrame()\n",
    "            dfenergy['traceID']=traceids\n",
    "            dfenergy['amplitude']=amplitudes\n",
    "            dfenergy['energy']=energies\n",
    "            dfenergy.sort_values(by=['amplitude'],ascending=False,inplace=True)\n",
    "            suggested_weight = None\n",
    "            if st[0].stats.units == 'Counts':\n",
    "                suggested_weight = int(np.log10(dfenergy.iloc[0]['energy']/2)) # works with uncorrected data in Counts only\n",
    "            \n",
    "            # plot all\n",
    "            st.plot(equal_scale=False);\n",
    "\n",
    "            # also plot a fixed 30-seconds around the peaktime\n",
    "            starttime = st[0].stats.starttime + row['peaktime']-10\n",
    "            endtime = starttime + 20\n",
    "            st.trim(starttime=starttime, endtime=endtime, pad=True, fill_value=None)\n",
    "            st.plot(equal_scale=False)                  \n",
    "\n",
    "            # Show a map of amplitude distribution\n",
    "            try:\n",
    "                add_station_locations(st, station_locationsDF)\n",
    "                plot_amplitude_locations(st)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # TEXT OUTPUT STARTS HERE\n",
    "            print(' ')\n",
    "            print('Checked events currently:')\n",
    "            #print(dfall[dfall['checked']==True].groupby('new_subclass').size())\n",
    "            print(row)\n",
    "            print(' ')\n",
    "            print('Loaded %s' % picklepath)\n",
    "            \n",
    "            # station amplitudes and energies\n",
    "            #print(dfenergy)\n",
    "\n",
    "            # trace df metrics\n",
    "            csvpath = picklepath.replace('.pickle', '.csv')\n",
    "            tracedf = pd.read_csv(csvpath)\n",
    "            if 'bandratio_[1.0_6.0_11.0]' in tracedf.columns:\n",
    "                tracedf.rename(columns = {'bandratio_[1.0_6.0_11.0]':'band_ratio'}, inplace = True)\n",
    "            tracedf.sort_values(by=['peakamp'], ascending=False, inplace=True)\n",
    "            print(' ')\n",
    "            print(tracedf[['id', 'medianF', 'bw_min', 'peakF', 'bw_max', 'band_ratio', 'kurtosis']])\n",
    "\n",
    "            # fingerprint guesses\n",
    "            print(' ')\n",
    "            _guess_subclass(row, fingerprints, subclasses_for_ML)\n",
    "            print(suggested_weight)\n",
    "\n",
    "            # Input\n",
    "            checked = False\n",
    "            print(' ')\n",
    "            print('RECLASSIFY')\n",
    "            print('Valid subclasses according to VOLCANO.DEF are: ', seisan_subclasses, end = ' ' )\n",
    "            print('e.g. l, 75, h, 25, 6')\n",
    "            #print('To enter percentage probabilities, e.g. 75% l, 25%h, enter l, 75, h, 25')\n",
    "            #print('Optionally add a weight [0-9] too with a trailing integer, e.g. l, 75,  h, 25, 5')\n",
    "            #print('Or:\\n\\ts = mark event for splitting')\n",
    "            #print('\\td = mark event for deletion')\n",
    "            #print('\\ti = ignore event')\n",
    "            #print('\\tq = quit')\n",
    "            print('Or s=split, d=delete, i=ignore, q=quit')\n",
    "\n",
    "            try:                       \n",
    "                new_subclass = input('\\t ?') \n",
    "                if not new_subclass:\n",
    "                    new_subclass = subclass\n",
    "                if new_subclass == 'q': \n",
    "                    return df, True\n",
    "                if new_subclass == 's':\n",
    "                      df.loc[index, 'split'] = True\n",
    "                      checked = True\n",
    "                if new_subclass == 'i':\n",
    "                      df.loc[index, 'ignore'] = True \n",
    "                      checked = True\n",
    "                if new_subclass == 'd':\n",
    "                      df.loc[index, 'delete'] = True \n",
    "                      checked = True\n",
    "                if not checked:                         \n",
    "                    if not ',' in new_subclass: # convert to a subclass, percentage string\n",
    "                        new_subclass = new_subclass + ', 100'\n",
    "                    spl = new_subclass.split(',') # split string to subclass probability list \n",
    "                    if len(spl) % 2 == 1:\n",
    "                        df.loc[index, 'weight'] = int(spl.pop())\n",
    "                    spd = {spl[a].strip():spl[a + 1] for a in range(0, len(spl), 2)} # subclass probability dict\n",
    "                    for key in subclasses_for_ML:\n",
    "                        if key in spd.keys():\n",
    "                            df.loc[index, key] = int(spd[key])\n",
    "                            print(key, int(spd[key]) )\n",
    "                        else:\n",
    "                            df.loc[index, key] = 0\n",
    "                    keymax = max(spd, key=spd.get)\n",
    "                    print('new_subclass = ',keymax)\n",
    "                    df.loc[index, 'new_subclass']=keymax  \n",
    "                    checked = True\n",
    "                if checked:\n",
    "                    df.loc[index, 'checked']=True\n",
    "            except:\n",
    "                print('Input may have been faulty. Skipping event')\n",
    "                return False\n",
    "                    \n",
    "    return df, False   \n",
    "\n",
    "\n",
    "\n",
    "def remove_marked_events(df):  \n",
    "    dfsubset = df\n",
    "    n_all = len(dfsubset.index)\n",
    "    dfsubset = dfsubset[dfsubset['delete']==False]\n",
    "    n_after_delete = len(dfsubset.index)\n",
    "    dfsubset = dfsubset[dfsubset['ignore']==False]\n",
    "    n_after_ignore = len(dfsubset.index)\n",
    "    dfsubset = dfsubset[dfsubset['split']==False]\n",
    "    n_after_split = len(dfsubset.index)\n",
    "    n_delete = n_all - n_after_delete\n",
    "    n_ignore = n_after_delete - n_after_ignore\n",
    "    n_split = n_after_ignore - n_after_split\n",
    "    print('Removed events: Marked to:')\n",
    "    print('- split ', n_split)\n",
    "    print('- delete ', n_delete)\n",
    "    print('- ignore ', n_ignore)\n",
    "    print('Catalog down from %d to %d events' % (n_all, n_after_split))\n",
    "    print(' ')  \n",
    "    return dfsubset\n",
    "\n",
    "\n",
    "\n",
    "def _merge_dataframes(df_dict, accepted_subclasses):\n",
    "    frames = []\n",
    "    for subclass in accepted_subclasses:\n",
    "        if subclass in df_dict.keys():\n",
    "            frames.append(df_dict[subclass]) \n",
    "    return pd.concat(frames, sort=True) \n",
    "\n",
    "def _count_by_subclass(df):\n",
    "    checked = df[df['checked']==True]\n",
    "    unchecked = df[df['checked']==False]\n",
    "    \n",
    "    print('Event counts:')\n",
    "    \n",
    "    if len(checked.index)>0:\n",
    "        print('Checked events: %d' % len(checked.index) )\n",
    "        checked_by_subclass = checked.groupby(\"new_subclass\")\n",
    "        print(checked_by_subclass['path'].count())\n",
    "        \n",
    "    if len(unchecked.index)>0:\n",
    "        print('Unchecked events: %d' % len(unchecked.index) )\n",
    "        unchecked_by_subclass = unchecked.groupby(\"new_subclass\")\n",
    "        print(unchecked_by_subclass['path'].count()) \n",
    "\n",
    "    print('Events by weight / quality threshold')\n",
    "    print(df.groupby('weight')['path'].count())\n",
    "\n",
    "def to_AAA(df, subclasses_for_ML, outfile, SEISAN_DATA, ignore_extra_columns=True, copy_to_path=None):\n",
    "    \"\"\"\n",
    "    create output file for AAA\n",
    "    \"\"\" \n",
    "    included_subclasses = subclasses_for_ML.copy()\n",
    "    minweight = 0\n",
    "    df = df[df['weight']>=minweight]\n",
    "    df = df[df['checked']==True]\n",
    "    \n",
    "    print(' ') \n",
    "    print('Now we have the following number of events by subclass:')\n",
    "    L = []\n",
    "    for i, subclass in enumerate(subclasses_for_ML):\n",
    "        df_subclass = df[df['new_subclass']==subclass]\n",
    "        L.append(len(df_subclass.index))\n",
    "        print('- %s: %d' % (subclass, L[i]))\n",
    "    \n",
    "    minnumevents = 10\n",
    "    removed_subclasses = []\n",
    "    for subclass in df['new_subclass'].unique():\n",
    "        if subclass in included_subclasses:\n",
    "            df_subclass = df[df['new_subclass']==subclass]\n",
    "            if len(df_subclass.index) < minnumevents:\n",
    "                print('Eliminating subclass %s' % subclass)\n",
    "                included_subclasses.remove(subclass)\n",
    "                removed_subclasses.append(subclass)\n",
    "            \n",
    "    print('The subclasses for machine learning are %s.' % ''.join(included_subclasses) )\n",
    "    print('Removed subclasses %s' % ''.join(removed_subclasses) )\n",
    "    df.sort_values(by='filetime',inplace=True)\n",
    "    \n",
    "    frames = []\n",
    "    df_dict = {}\n",
    "    for subclass in included_subclasses:\n",
    "        df_dict[subclass] = df[df['new_subclass']==subclass]\n",
    "    dfAAA = _merge_dataframes(df_dict, included_subclasses)    \n",
    "    \n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    df.sort_values(by='filetime',inplace=True)\n",
    "    for i, row in df.iterrows():\n",
    "        row['new_subclass'] = row['new_subclass'].strip()\n",
    "        if row['new_subclass'] in included_subclasses:\n",
    "            df_list.append(row)\n",
    "    df = pd.DataFrame(df_list)\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Here is the FINAL list of events by subclass and whether they have been checked:')\n",
    "    _count_by_subclass(dfAAA)    \n",
    "    \n",
    "    #df.rename(columns = {'twin':'duration'}, inplace = True)\n",
    "    dfAAA.rename(columns = {'twin':'length'}, inplace = True)\n",
    "    dfAAA['f0']=0.5\n",
    "    dfAAA['f1']=25.0   \n",
    "    \n",
    "    \"\"\"\n",
    "    confidence_threshold = int(input('What is the minimum confidence percentage (e.g. 50) you wish to use ? This should be an integer.'))\n",
    "    if confidence_threshold:\n",
    "        good_indices = []\n",
    "        for subclass in subclasses_for_ML:\n",
    "            good_indices.expand(df[subclass] >= confidence_threshold)\n",
    "        df = df.iloc[good_indices]\n",
    "    print(df[['filetime', 'new_subclass']])\n",
    "    \"\"\"\n",
    "    \n",
    "    # subset and rename columns for output\n",
    "    if ignore_extra_columns:\n",
    "        dfAAA=dfAAA[['new_subclass','year','month','day','hour','minute','second','length','path']]     \n",
    "    dfAAA.rename(columns = {'new_subclass':'class'}, inplace = True)\n",
    "    \n",
    "    \n",
    "    if copy_to_path:\n",
    "        thisdir = os.getcwd()\n",
    "        os.chdir(SEISAN_DATA)\n",
    "        for i, row in dfAAA.iterrows():\n",
    "            subclass = row['class']\n",
    "            wavfile = row['path'] # relative path like ./WAV/DB/YYYY/MM/WAVfile\n",
    "            newdir = os.path.join(copy_to_path, subclass)\n",
    "            newfile = os.path.join(newdir, os.path.basename(wavfile))\n",
    "            dfAAA.loc[i, 'path']=newfile\n",
    "            if not os.path.exists(newdir):\n",
    "                os.mkdir(newdir)\n",
    "            if not os.path.exists(newfile):\n",
    "                print('cp %s %s' % (wavfile, newfile) )\n",
    "                os.system('cp %s %s' % (wavfile, newfile) )\n",
    "        print('WAVfiles copied to ',copy_to_path)\n",
    "        os.chdir(thisdir)\n",
    "            \n",
    "    dfAAA.to_csv(outfile)\n",
    "    print('Catalog CSV saved to ',outfile)\n",
    "    \n",
    "    \n",
    "    \n",
    "def report_checked_events(dfall, subclasses_for_ML):\n",
    "    # Sanity check against AAA writer\n",
    "    df = dfall.copy()\n",
    "    df = df[df['checked']==True]\n",
    "    print('total checked events = %d' % len(df.index))\n",
    "    df = df[df['ignore']==False]\n",
    "    df = df[df['delete']==False]\n",
    "    df = df[df['split']==False]\n",
    "    print('total classified events = %d' % len(df.index))\n",
    "    frames = []\n",
    "    for subclass in subclasses_for_ML:\n",
    "        dfs = df[df['new_subclass']==subclass]\n",
    "        print(subclass, len(dfs.index))\n",
    "        frames.append(dfs)\n",
    "    newdf = pd.concat(frames)\n",
    "\n",
    "    L0 = len(newdf.index)\n",
    "    print('total events matching ML subclasses = %d' % L0)\n",
    "\n",
    "    differentdf = newdf[newdf['subclass']!=newdf['new_subclass']]\n",
    "    L1 = len(differentdf.index)\n",
    "    print('total reclassified events = %d' % L1)\n",
    "\n",
    "    samedf = newdf[newdf['subclass']==newdf['new_subclass']]\n",
    "    L2 = len(samedf.index)\n",
    "    print('total already correctly classified events = %d' % L2)\n",
    "\n",
    "    print('Error rate = %.1f%%' % (L1*100/L0))\n",
    "\n",
    "    print(L0, L1+L2)\n",
    "\n",
    "    #newdf.sort_values(by=['weight'],inplace=True,ascending=False)\n",
    "    #print(newdf[['subclass', 'R', 'r', 'e', 'l', 'h', 't', 'new_subclass', 'weight']].to_string())\n",
    "\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 100 events of type r from a total of 124771\n",
      "Selecting 100 events of type e from a total of 13347\n",
      "Selecting 100 events of type t from a total of 7759\n",
      "Selecting 100 events of type l from a total of 27206\n",
      "Selecting 100 events of type h from a total of 41209\n",
      "Selecting 100 events of type R from a total of 128\n",
      "Selecting 10 events of type D from a total of 10\n",
      "Computing fingerprint for subclass  D\n",
      "Computing fingerprint for subclass  R\n",
      "Computing fingerprint for subclass  r\n",
      "Computing fingerprint for subclass  e\n",
      "Computing fingerprint for subclass  l\n",
      "Computing fingerprint for subclass  h\n",
      "Computing fingerprint for subclass  t\n",
      "next event is of subclass = D\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clear_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3ba4c67ae7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# manually QC the next event. each time we choose the class with least checked examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mone_event_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqc_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclasses_for_ML\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseisan_subclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfingerprints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEISAN_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstation_locationsDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_event_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# now we must merge this back into dfall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c415bfd4785c>\u001b[0m in \u001b[0;36mqc_event\u001b[0;34m(dfall, subclasses_for_ML, seisan_subclasses, fingerprints, SEISAN_DATA, station_locationsDF)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mpicklepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEISAN_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./WAV'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PICKLE'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clear_output' is not defined"
     ]
    }
   ],
   "source": [
    "SEISAN_DATA = os.path.join( os.getenv('HOME'),'DATA','MVO')\n",
    "DB = 'MVOE_'\n",
    "volcanodefcsv = 'CSVfiles/volcano_def.csv'\n",
    "subclass_mapping = read_volcano_def(volcanodefcsv) # subclasses allowed for classification\n",
    "seisan_subclasses = subclass_mapping['subclass'].values.tolist() # append('g') as needed, it is not an allowed subclass\n",
    "#seisan_etypes = subclass_mapping['etype'].values.tolist()\n",
    "subclasses_for_ML = ['D', 'R', 'r', 'e', 'l', 'h', 't'] # subclasses allowed for Machine Learning\n",
    "outfile = 'MVOE_catalog_hal.csv'\n",
    "\n",
    "if os.path.exists(outfile):\n",
    "    dfall = pd.read_csv(outfile) # how do i ignore the index?\n",
    "    # do the following until I learn how to ignore index. otherwise it adds a new column on each load.\n",
    "    dfall = dfall[['filetime', 'Fs', 'bandratio_[0.8_4.0_16.0]',\n",
    "       'bandratio_[1.0_6.0_11.0]', 'bw_max', 'bw_min', 'calib',\n",
    "       'cft_peak_wmean', 'cft_std_wmean', 'coincidence_sum', 'day',\n",
    "       'detection_quality', 'energy', 'hour', 'kurtosis', 'medianF', 'minute',\n",
    "       'month', 'noise_level', 'num_gaps', 'num_traces', 'offtime', 'ontime',\n",
    "       'path', 'peakA', 'peakF', 'peakamp', 'peaktime', 'percent_availability',\n",
    "       'quality', 'sample_lower_quartile', 'sample_max', 'sample_mean',\n",
    "       'sample_median', 'sample_min', 'sample_rms', 'sample_stdev',\n",
    "       'sample_upper_quartile', 'second', 'sfile', 'signal_level', 'skewness',\n",
    "       'snr', 'starttime', 'subclass', 'trigger_duration', 'year', 'D', 'R',\n",
    "       'r', 'e', 'l', 'h', 't', 'new_subclass', 'weight', 'checked', 'split',\n",
    "       'delete', 'ignore']]\n",
    "    # removed twin as it is missing\n",
    "else:\n",
    "    master_event_catalog = 'MVOE_catalog_original_hal.csv'\n",
    "    \n",
    "    # SCAFFOLD - the twin column no longer seems to exist\n",
    "    dfall = build_master_event_catalog(SEISAN_DATA, DB, master_event_catalog, subclasses_for_ML)\n",
    "\n",
    "station0hypfile = os.path.join(SEISAN_DATA, 'DAT', 'STATION0_MVO.HYP')\n",
    "station_locationsDF = parse_STATION0HYP(station0hypfile)\n",
    "\n",
    "###\n",
    "#fingerprints = get_weighted_fingerprints(dfall, subclasses_for_ML, N=300, exclude_checked=False)\n",
    "#one_event_df, quit = qc_event(dfall, subclasses_for_ML, seisan_subclasses, fingerprints, SEISAN_DATA, station_locationsDF)\n",
    "\n",
    "iterate_again = True # changed this back to do the loop\n",
    "while iterate_again:\n",
    "\n",
    "    # get/update the fingerprints of each event class\n",
    "    #fingerprints = get_weighted_fingerprints(dfall, subclasses_for_ML, N=100, exclude_checked=False)\n",
    "    fingerprints = get_fingerprints(dfall, subclasses_for_ML, N=100, exclude_checked=False)\n",
    "    save_fingerprints(fingerprints, subclasses_for_ML)\n",
    "    \n",
    "    # manually QC the next event. each time we choose the class with least checked examples\n",
    "    one_event_df, quit = qc_event(dfall, subclasses_for_ML, seisan_subclasses, fingerprints, SEISAN_DATA, station_locationsDF)\n",
    "    if isinstance(one_event_df, pd.DataFrame):\n",
    "        # now we must merge this back into dfall\n",
    "        dfall.sort_index(inplace=True)\n",
    "        dfall.update(one_event_df)  \n",
    "    \n",
    "        # save the data  \n",
    "        dfall.to_csv(outfile, index=False)\n",
    "    else:\n",
    "        iterate_again=False\n",
    "    if quit:\n",
    "        iterate_again=False \n",
    "# remove events we marked for deletion, splitting or to ignore\n",
    "dfsubset = remove_marked_events(dfall)\n",
    "\n",
    "aaa_infile = 'MVOE_catalog_reclassified_hal.csv' \n",
    "to_AAA(dfsubset, subclasses_for_ML, aaa_infile, SEISAN_DATA, ignore_extra_columns=False)\n",
    "report_checked_events(dfall, subclasses_for_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When I work on hal, I will want to build a full catalog \n",
    "# from all the event CSV files. But I would lose my checked events.\n",
    "# So this is how I took care of that before.\n",
    "# 1. Move the catalog file to a new name, e.g. MVOE_catalog_previous.csv\n",
    "# 2. Move the original catalog file also, e.g. MVOE_catalog_original_previous.csv\n",
    "# 3. Run the code above but with iterate=False\n",
    "# 4. Now we should have a new MVOE_catalog_original.csv. \n",
    "#    Copy that to MVOE_catalog.csv\n",
    "# 5. Now we need to update the newest catalog using the oldest. \n",
    "#    Any rows with matching file times should be replaced\n",
    "print(dfall.path)\n",
    "\n",
    "oldcatfile = 'MVOE_catalog_previous.csv'\n",
    "newcatfile = 'MVOE_catalog.csv'\n",
    "newcat = pd.read_csv(newcatfile)\n",
    "newcat.update(pd.read_csv(oldcatfile))\n",
    "newcat.to_csv('MVOE_catalog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'MVOE_catalog.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "#df = dfall[dfall['filetime']=='2003-05-14T06:04:14.000000Z']\n",
    "#df = dfall[dfall['filetime']=='1901-03-06T21:13:28.040000Z']\n",
    "df = dfall[dfall['filetime']=='2001-11-06T21:13:28.040000Z']\n",
    "print(len(dfall))\n",
    "#df = dfall.iloc[0]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfall['filetime'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it turns out that multiple Sfiles sometimes point to the same WAVfile\n",
    "# this leads to non-unique filetime entries\n",
    "\n",
    "# Indeed we now see there is a many-to-many relationship between (WAVfile) path and Sfile.\n",
    "\n",
    "# reawav_MVOE_YYYYDD.csv: fields include sfile, (WAV) path and (WAV) filetime\n",
    "# ^ this already establishes a link between every Sfile and DSN WavFile\n",
    "\n",
    "# Better might be...\n",
    "# Mapping.CSV: Sfile, SfileTime, DSNWavfile, DSNWavFileTime, ASNWavfile, ASNWavFileTime\n",
    "\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "LIBpath = os.path.join( os.getenv('HOME'),'src','kitchensinkGT', 'LIB')\n",
    "sys.path.append(LIBpath)\n",
    "import seisan_classes\n",
    "SEISAN_DATA = os.path.join( os.getenv('HOME'),'DATA','MVO')\n",
    "DB = 'MVOE_'\n",
    "\n",
    "def sfile2spath(sfile):\n",
    "    parts = sfile.split('.S')\n",
    "    YYYY = parts[1][0:4]\n",
    "    MM = parts[1][4:6]\n",
    "    spath = os.path.join(SEISAN_DATA, 'REA', DB, YYYY, MM, sfile) \n",
    "    return spath    \n",
    "\n",
    "catfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(catfile)\n",
    "\n",
    "print('Number of rows %d' % len(dfall))\n",
    "print('Number of unique filetimes %d' % len(dfall['filetime'].unique()))\n",
    "print('Number of unique paths %d' % len(dfall['path'].unique()))\n",
    "\n",
    "for i,row in dfall.iterrows():\n",
    "    if i>0:\n",
    "        if row['delete']==True:\n",
    "            continue\n",
    "        if row['filetime']==lastrow['filetime']:\n",
    "            rows=[lastrow, row]\n",
    "            \n",
    "            # Fix the S-file path. Sometimes we have two S-files that point to same DSN MVO WAVfile.\n",
    "            # We have to examine both S-files, and determine what name they should have based on the WAVfiles\n",
    "            # they point to. The key is the first WAVfile in time. Then we ultimately drop the later S-file.\n",
    "            predicted_sfilepath = \"\"\n",
    "            first_wavfile = None\n",
    "            for r in rows:\n",
    "                sfilepath = sfile2spath(r['sfile'])\n",
    "                sfileobj = seisan_classes.Sfile(sfilepath, use_mvo_parser=True)\n",
    "                for wavfile in sfileobj.wavfiles:\n",
    "                    if not first_wavfile:\n",
    "                        first_wavfile = wavfile\n",
    "                    if first_wavfile.filetime > wavfile.filetime:\n",
    "                        first_wavfile = wavfile\n",
    "            predicted_sfilepath, was_found = first_wavfile.find_sfile(mainclass=sfileobj.mainclass[0])  \n",
    "            dfall.loc[lasti, 'sfile'] = os.path.basename(predicted_sfilepath)\n",
    "            \n",
    "            # Fix the subclass\n",
    "            if row['subclass']!=lastrow['subclass']:\n",
    "                sfileobj = seisan_classes.Sfile(sfilepath, use_mvo_parser=True)\n",
    "                dfall.loc[lasti, 'mainclass'] = sfileobj.mainclass\n",
    "                dfall.loc[lasti, 'subclass'] = sfileobj.subclass\n",
    "                \n",
    "            # If either event was checked, assume the new_subclass is correct\n",
    "            keep = lasti\n",
    "            if dfall.loc[lasti, 'checked'] == False:\n",
    "                dfall.loc[lasti, 'new_subclass'] = sfileobj.subclass\n",
    "            if dfall.loc[i, 'checked'] == True:\n",
    "                dfall.loc[lasti, 'new_subclass'] = dfall.loc[i, 'new_subclass']\n",
    "                for subclass in ['R', 'r', 'e', 'l', 'h', 't']:\n",
    "                    dfall.loc[lasti, subclass] = dfall.loc[i, subclass]\n",
    "                dfall.loc[lasti, 'checked'] = True\n",
    "                dfall.loc[lasti, 'ignore'] = dfall.loc[i, 'ignore']             \n",
    "\n",
    "            # Mark the duplicate for removal    \n",
    "            dfall.loc[i, 'delete'] = 2\n",
    "            \n",
    "            # Print the modified rows of the dataframe  \n",
    "            print(dfall.loc[[lasti, i], ['sfile','subclass','new_subclass', 'checked','delete']])\n",
    "            \n",
    "    lastrow=row\n",
    "    lasti=i\n",
    "    \n",
    "# Remove duplicated rows previously marked for deletion\n",
    "dfall = dfall[dfall['delete']!=2]\n",
    "print('Number of rows %d' % len(dfall))\n",
    "\n",
    "outfile = catfile.replace('all', 'unique')\n",
    "#dfall.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we want to figure out how to do this for the new, bigger catalog. And then merge the two catalogs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
