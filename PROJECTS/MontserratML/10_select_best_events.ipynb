{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montserrat event selector for Machine Learning\n",
    "The aim of this code is to find the best N events of each type, and create a corresponding CSV file and data structure for entry into Alexis' and Marielle's AAA codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked events: 150\n",
      "new_subclass\n",
      " t     1\n",
      "R      1\n",
      "e     17\n",
      "h     33\n",
      "l     25\n",
      "m      1\n",
      "n      1\n",
      "r     34\n",
      "t     34\n",
      "u      2\n",
      "w      1\n",
      "Name: path, dtype: int64\n",
      "Unchecked events: 17346\n",
      "new_subclass\n",
      "2        1\n",
      "M        7\n",
      "U       14\n",
      "e      690\n",
      "g      659\n",
      "h     3126\n",
      "l     1258\n",
      "m        4\n",
      "n      606\n",
      "r    10300\n",
      "s       11\n",
      "t      670\n",
      "Name: path, dtype: int64\n",
      "How many events of each subclass do you want in training+test datasets?30\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy.core import read\n",
    "import sys\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "\n",
    "LIBpath = os.path.join( os.getenv('HOME'),'src','kitchensinkGT', 'LIB')\n",
    "sys.path.append(LIBpath)\n",
    "from libseisGT import mulplt\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "# Use 3 decimal places in output display\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "# Don't wrap repr(DataFrame) across additional lines\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "# Set max rows displayed in output to 25\n",
    "pd.set_option(\"display.max_rows\", 25)\n",
    "\n",
    "np.set_printoptions(linewidth=160)\n",
    "\"\"\"\n",
    "\n",
    "def read_volcano_def():\n",
    "    filepath = './volcano_def.csv'\n",
    "    subclass_df = pd.read_csv(filepath)\n",
    "    subclass_df.columns = subclass_df.columns.str.lstrip()\n",
    "    return subclass_df\n",
    "    \n",
    "def build_master_event_catalog(csvdir, seisandbname, catalogfile, subclasses_for_ML):\n",
    "    # load all the year/month CSV files\n",
    "    csvfiles = glob(os.path.join(csvdir, 'reawav_%s??????.csv' % seisandbname))\n",
    "    frames = []\n",
    "    for csvfile in csvfiles:\n",
    "        df = pd.read_csv(csvfile)\n",
    "        frames.append(df) \n",
    "    dfall = pd.concat(frames, sort=True)\n",
    "    dfall.set_index('filetime', inplace=True) # we will need this later to remerge\n",
    "    dfall.sort_index(inplace=True)\n",
    "    \"\"\"\n",
    "    for index, row in dfall.iterrows():\n",
    "        \n",
    "        # For simplicity, copy 'D' and 'R' mainclass to subclass\n",
    "        if row['mainclass'] in ['D', 'R']: # Do I need L here too?\n",
    "            dfall.loc[index, 'subclass']=row['mainclass']\n",
    "    \"\"\" \n",
    "    # replace loop above\n",
    "    for mainclass in ['R', 'D']:\n",
    "        dfall.loc[dfall['mainclass'] == mainclass, 'subclass'] = mainclass\n",
    "    \n",
    "    # Drop the mainclass column, as it is now superfluous.\n",
    "    dfall.drop(columns=['mainclass'], inplace=True)\n",
    "    \n",
    "    # Add an etype column\n",
    "    #dfall['etype'] = dfall['subclass'].replace(subclasses, etypes)\n",
    "    \n",
    "    # Add columns to assign a percentage for each subclass\n",
    "    for subclass in subclasses_for_ML:\n",
    "        dfall[subclass] = 0\n",
    "    \n",
    "    # But set column for actual subclass to 100%  \n",
    "    \"\"\"\n",
    "    for index, row in dfall.iterrows():            \n",
    "        # Set dfall['h'] to 100 if dfall['subclass']=='h', etc\n",
    "        dfall.loc[index, row['subclass']] = 100        \n",
    "    \"\"\"\n",
    "    # replace row operations above\n",
    "    for subclass in subclasses_for_ML:\n",
    "        dfall.loc[dfall['subclass'] == subclass, subclass] = 100\n",
    "        \n",
    "    # Add a new_subclass column\n",
    "    dfall['new_subclass'] = dfall['subclass']\n",
    "\n",
    "    # Add weight column. I will give really clear events higher weight when I process them\n",
    "    dfall['weight']=3 # weight for events I have not visually checked\n",
    "    \n",
    "    # Add column that records if event is checked\n",
    "    dfall['checked']=False\n",
    "    \n",
    "    # Add column that records if event is marked for splitting\n",
    "    dfall['split']=False    \n",
    "    \n",
    "    # Add column that records if event is marked for deletion\n",
    "    dfall['delete']=False\n",
    "    \n",
    "    # Add column that records if event should be ignored\n",
    "    # Ignore any events longer than 1-minute, as they are likely to contain multiple events \n",
    "    # or just be unhelpful for classifying short signals which are more common\n",
    "    dfall['ignore'] = dfall['twin']>60 \n",
    "    \n",
    "    # Now we have a catalog dataframe we can work with. Let's save this.\n",
    "    dfall.to_csv(catalogfile)\n",
    "    \n",
    "    return dfall\n",
    "\n",
    "def _count_by_subclass(df):\n",
    "    checked = df[df['checked']==True]\n",
    "    unchecked = df[df['checked']==False]\n",
    "    \n",
    "    if len(checked.index)>0:\n",
    "        print('Checked events: %d' % len(checked.index) )\n",
    "        checked_by_subclass = checked.groupby(\"new_subclass\")\n",
    "        print(checked_by_subclass['path'].count())\n",
    "        \n",
    "    if len(unchecked.index)>0:\n",
    "        print('Unchecked events: %d' % len(unchecked.index) )\n",
    "        unchecked_by_subclass = unchecked.groupby(\"new_subclass\")\n",
    "        print(unchecked_by_subclass['path'].count()) \n",
    "        \n",
    "\n",
    "def _select_best_events(df, allowed_subclasses, N=100, exclude_checked=True):\n",
    "    # When we are iterating, we want to exclude the checked events, else we are repeating our work.\n",
    "    # When we return our final list, we do not want to exclude checked events\n",
    "    all_subclasses = df['new_subclass'].unique()\n",
    "    best_events_dict = {}\n",
    "    for subclass in all_subclasses:\n",
    "        if subclass in allowed_subclasses:\n",
    "            #print('\\nProcessing subclass=%s' % subclass)\n",
    "            is_subclass =  df['new_subclass']==subclass\n",
    "            df_subclass = df[is_subclass]\n",
    "            \n",
    "            # mechanism to weight out checked events\n",
    "            #df_subclass['include'] = 1 - df_subclass['ignore']\n",
    "            df_subclass['include'] = not df_subclass['ignore']\n",
    "            if exclude_checked:\n",
    "                print('Excluding checked events')\n",
    "                df_subclass['include'] = df_subclass['include'] * (1 - df_subclass['checked'])\n",
    "            \n",
    "            if len(df_subclass.index)>0:\n",
    "                # we use three criteria for ranking events. detection_quality has the largest magnitude, but can be missing, so we also add snr, \n",
    "                # and finally quality as a tie-braker, since it has a small range for events that have made it this far\n",
    "                df_subclass['sortcol'] = df_subclass['quality'] # always present\n",
    "                if 'snr' in df_subclass.columns:\n",
    "                    df_subclass['sortcol'] = df_subclass['sortcol'] + df_subclass['snr']\n",
    "                if 'detection_quality' in df_subclass.columns:\n",
    "                    df_subclass['sortcol'] = df_subclass['sortcol'] + df_subclass['detection_quality']                    \n",
    "                df_subclass['sortcol'] = df_subclass['sortcol'] * df_subclass['weight'] * df_subclass['include']\n",
    "                df_subclass.drop(columns=['include'], inplace=True)\n",
    "\n",
    "                L = len(df_subclass.index)\n",
    "                H = int(min([L/3, N]))\n",
    "                print('Selecting %d events of type %s from a total of %d' % (H, subclass, L))\n",
    "                df_subclass.sort_values(by=['sortcol'], ascending=False, inplace=True)\n",
    "                df_subclass.drop(columns=['sortcol'], inplace=True)\n",
    "                df_subclass = df_subclass.head(H)\n",
    "                best_events_dict[subclass]=df_subclass\n",
    "\n",
    "    return best_events_dict \n",
    "\n",
    "def get_weighted_fingerprints(dfall, subclasses_for_ML, N=300, exclude_checked=False):\n",
    "    \"\"\"\n",
    "    SCAFFOLD: how do I modify this to use the probabilistic subclasses and weight column to weight the fingerprints?\n",
    "    All we do right now is a dataframe describe, so we return the stats of each column.\n",
    "    But to weight the statistics we might need to loop over each column, which could be slow.\n",
    "    \n",
    "    wdf = DescrStatsW(df.x, weights=df.wt, ddof=1) \n",
    "    \n",
    "    \"\"\"\n",
    "    #df = best_events_dict.groupby(\"subclass\")\n",
    "    fingerprints = {}\n",
    "    best_events_dict = _select_best_events(dfall, subclasses_for_ML, N=N, exclude_checked=exclude_checked)\n",
    "    for subclass in subclasses_for_ML:\n",
    "        if subclass in best_events_dict.keys(): \n",
    "            print('Computing fingerprint for subclass ',subclass)\n",
    "            thisdf = best_events_dict[subclass]\n",
    "            if len(thisdf.index)>30:\n",
    "                statsdf = pd.DataFrame()\n",
    "                statsdf['statistic'] = ['mean', 'std', '25%', '50%', '75%']\n",
    "                statsdf.set_index(['statistic'], inplace = True)\n",
    "\n",
    "                for col in [ 'peaktime', 'kurtosis', 'medianF', 'peakF', 'bw_min', 'bw_max', 'band_ratio']:\n",
    "                    # compute mean, std, median, 25% percentile, 75% percentile\n",
    "                    wdf = DescrStatsW(thisdf[col], weights=thisdf['weight'].astype(float)*thisdf[subclass].astype(float), ddof=1)\n",
    "                    p = [0.25,0.50,0.75]\n",
    "                    q  = wdf.quantile(p) \n",
    "                    statsdf.loc['mean', col] = wdf.mean\n",
    "                    statsdf.loc['std', col] = wdf.std\n",
    "                    statsdf.loc['50%', col] = q[p[1]]\n",
    "                    statsdf.loc['25%', col] = q[p[0]]\n",
    "                    statsdf.loc['75%', col] = q[p[2]]\n",
    "\n",
    "                fingerprints[subclass] = statsdf\n",
    "                #print(fingerprints[subclass])\n",
    "    return fingerprints    \n",
    "\n",
    "def get_fingerprints(dfall, allowed_subclasses, N=300, exclude_checked=True):\n",
    "    \"\"\"\n",
    "    All we do right now is a dataframe describe, so we return the stats of each column.\n",
    "    \n",
    "    wdf = DescrStatsW(df.x, weights=df.wt, ddof=1) \n",
    "    \n",
    "    \"\"\"\n",
    "    #df = best_events_dict.groupby(\"subclass\")\n",
    "    fingerprints = {}\n",
    "    best_events_dict = _select_best_events(dfall, allowed_subclasses, N=N, exclude_checked=exclude_checked)\n",
    "    for subclass in allowed_subclasses:\n",
    "        if subclass in best_events_dict.keys(): \n",
    "            print('Computing fingerprint for subclass ',subclass)\n",
    "            #df_subclass = df.get_group(subclass)\n",
    "            df_subclass = best_events_dict[subclass]     \n",
    "            fingerprints[subclass] = df_subclass[[ 'peaktime', \n",
    "                'kurtosis', 'medianF', 'peakF', 'bw_min', 'bw_max', 'band_ratio']].describe()\n",
    "            #print(fingerprints[subclass])\n",
    "    return fingerprints\n",
    "  \n",
    "def _merge_dataframes(df_dict, accepted_subclasses):\n",
    "    frames = []\n",
    "    for subclass in accepted_subclasses:\n",
    "        if subclass in df_dict.keys():\n",
    "            frames.append(df_dict[subclass]) \n",
    "    return pd.concat(frames, sort=True)     \n",
    "\n",
    "def _guess_subclass(row, fingerprints, subclasses_for_ML):\n",
    "    chance_of = {}\n",
    "    for item in subclasses_for_ML:\n",
    "        chance_of[item]=0.0\n",
    "    \n",
    "    params = ['peaktime', 'kurtosis', 'medianF', 'peakF', 'bw_min', 'bw_max', 'band_ratio']\n",
    "    for subclass in fingerprints.keys():\n",
    "        fingerprint = fingerprints[subclass]\n",
    "        #fingerprint.reset_index(inplace=True)\n",
    "        #print(fingerprint.columns)\n",
    "        for param in params:\n",
    "            thisval = row[param]\n",
    "            \n",
    "            # test against mean+/-std\n",
    "            meanval = fingerprint[param]['mean']\n",
    "            stdval = fingerprint[param]['std']\n",
    "            minus1sigma = meanval - stdval\n",
    "            plus1sigma = meanval + stdval\n",
    "            \n",
    "            if thisval > minus1sigma and thisval < plus1sigma:\n",
    "                weight = 1.0 - abs(thisval-meanval)/stdval\n",
    "                chance_of[subclass] += weight\n",
    "                \n",
    "            # test against 25-75% percentile\n",
    "            medianval = fingerprint[param]['50%']\n",
    "            val25 = fingerprint[param]['25%']\n",
    "            val75 = fingerprint[param]['75%']\n",
    "            if thisval > val25 and thisval < val75:\n",
    "                if thisval < medianval:\n",
    "                    weight = 1.0 - (medianval-thisval)/(medianval-val25)\n",
    "                else:\n",
    "                    weight = 1.0 - (thisval-medianval)/(val75-medianval)\n",
    "                chance_of[subclass] += weight\n",
    "    \n",
    "    print('The event is classified as %s, but here are our guesses:' % row['subclass'])\n",
    "    total = 0\n",
    "    for subclass in chance_of.keys():\n",
    "        total += chance_of[subclass]\n",
    "    for subclass in chance_of.keys():\n",
    "        if total>0:\n",
    "            print('subclass: %s, points = %f, probability = %3.0f%%' % (subclass, chance_of[subclass], 100*chance_of[subclass]/total))\n",
    "\n",
    "\n",
    "def qc_best_events(best_events_dict, seisan_subclasses, subclasses_for_ML, fingerprints):\n",
    "    \n",
    "    for subclass in subclasses_for_ML:\n",
    "        #if subclass=='u' or subclass=='n': # we don't care about these events\n",
    "        #    continue\n",
    "        #print('Processing ',subclass)\n",
    "        if subclass in best_events_dict.keys():\n",
    "            df = best_events_dict[subclass]\n",
    "            print('Processing %d events of type %s' % (len(df.index), subclass) )\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                #picklepath = os.path.join(SEISAN_DATA, row.path.replace('WAV','PICKLE') + '.pickle')\n",
    "                picklepath = os.path.join(SEISAN_DATA, row.path.replace('WAV','PICKLE2') + '.pickle')\n",
    "                print('Loading %s' % picklepath)\n",
    "                if os.path.exists(picklepath):\n",
    "                    \n",
    "                    st = read(picklepath) #.select(station='MBWH')\n",
    "                    st.plot();\n",
    "                    #mulplt(st)\n",
    "                    starttime = st[0].stats.starttime + row['peaktime']-10\n",
    "                    endtime = starttime + 20\n",
    "                    st.trim(starttime=starttime, endtime=endtime, pad=True, fill_value=None)\n",
    "                    st.plot()\n",
    "                    \n",
    "                    # Hints\n",
    "                    csvpath = picklepath.replace('.pickle', '.csv')\n",
    "                    tracedf = pd.read_csv(csvpath)\n",
    "                    print(tracedf[['id', 'peaktime', 'kurtosis', 'medianF', 'peakF','bw_min', 'bw_max', 'band_ratio', 'kurtosis', 'energy']])\n",
    "                    _guess_subclass(row, fingerprints, subclasses_for_ML)\n",
    "                    \n",
    "                    # Input\n",
    "                    checked = False\n",
    "                    print('Please reclassify the event.')\n",
    "                    print('Valid subclasses are: ', seisan_subclasses )\n",
    "                    print('To enter percentage probabilities, e.g. 75% l, 25%h, enter l, 75, h, 25')\n",
    "                    print('Optionally add a weight [0-9] too with a trailing integer, e.g. l, 75,  h, 25, 5')\n",
    "                    print('Or:\\n\\ts = mark event for splitting')\n",
    "                    print('\\td = mark event for deletion')\n",
    "                    print('\\ti = ignore event')\n",
    "                    \n",
    "                    try:                       \n",
    "                        new_subclass = input('\\t ?') \n",
    "                        if not new_subclass:\n",
    "                            new_subclass = subclass\n",
    "                        if new_subclass == 's':\n",
    "                              df.loc[index, 'split'] = True\n",
    "                              checked = True\n",
    "                        if new_subclass == 'i':\n",
    "                              df.loc[index, 'ignore'] = True \n",
    "                              checked = True\n",
    "                        if new_subclass == 'd':\n",
    "                              df.loc[index, 'delete'] = True \n",
    "                              checked = True\n",
    "                        if not checked:                         \n",
    "                            if not ',' in new_subclass: # convert to a subclass, percentage string\n",
    "                                new_subclass = new_subclass + ', 100'\n",
    "                            spl = new_subclass.split(',') # split string to subclass probability list \n",
    "                            if len(spl) % 2 == 1:\n",
    "                                df.loc[index, 'weight'] = int(spl.pop())\n",
    "                            spd = {spl[a]:spl[a + 1] for a in range(0, len(spl), 2)} # subclass probability dict\n",
    "                            for key in subclasses_for_ML:\n",
    "                                if key in spd.keys():\n",
    "                                    df.loc[index, key] = int(spd[key])\n",
    "                            keymax = max(spd, key=spd.get)\n",
    "                            df.loc[index, 'new_subclass']=keymax  \n",
    "                            checked = True\n",
    "                        if checked:\n",
    "                            df.loc[index, 'checked']=True\n",
    "                    except:\n",
    "                        print('Input may have been faulty. Skipping event')\n",
    "                        pass\n",
    "                        \n",
    "                    \n",
    "            best_events_dict[subclass] = df\n",
    "    #return _merge_dataframes(best_events_dict, best_events_dict.keys())\n",
    "    return best_events_dict\n",
    "\n",
    "def to_AAA(df, subclasses_for_ML, outfile, confidence_threshold=None, ignore_extra_columns=True):\n",
    "    \"\"\"\n",
    "    create output file for AAA\n",
    "    \"\"\" \n",
    "    #df.rename(columns = {'twin':'duration'}, inplace = True)\n",
    "    df.rename(columns = {'twin':'length'}, inplace = True)\n",
    "    #df['f0']=None\n",
    "    #df['f1']=None\n",
    "    df['f0']=0.5\n",
    "    df['f1']=25.0   \n",
    "    \n",
    "    for subclass in subclasses_for_ML:\n",
    "        if confidence_threshold:\n",
    "            df = df[df[subclass] >= confidence_threshold]\n",
    "            \n",
    "    dfAAA_list = []\n",
    "    for i, row in df.iterrows():\n",
    "        row['new_subclass'] = row['new_subclass'].strip()\n",
    "        if row['new_subclass'] in subclasses_for_ML:\n",
    "            dfAAA_list.append(row)\n",
    "    dfAAA = pd.DataFrame(dfAAA_list)\n",
    "    \n",
    "    if ignore_extra_columns:\n",
    "        dfAAA=dfAAA[['new_subclass','year','month','day','hour','minute','second','length','path']] \n",
    "        \n",
    "    dfAAA.rename(columns = {'new_subclass':'class'}, inplace = True)\n",
    "    dfAAA.to_csv(outfile)\n",
    "\n",
    "##############################################################################\n",
    "SEISAN_DATA = os.path.join( os.getenv('HOME'),'DATA','MVO')\n",
    "DB = 'MVOE_'\n",
    "subclass_mapping = read_volcano_def() # subclasses allowed for classification\n",
    "#print(subclass_mapping.columns)\n",
    "seisan_subclasses = subclass_mapping['subclass'].values.tolist() # append('g') as needed, it is not an allowed subclass\n",
    "#seisan_etypes = subclass_mapping['etype'].values.tolist()\n",
    "subclasses_for_ML = ['D', 'R', 'r', 'e', 'l', 'h', 't'] # subclasses allowed for Machine Learning\n",
    "outfile = 'catalog_all.csv'\n",
    "\n",
    "if os.path.exists(outfile):\n",
    "    dfall = pd.read_csv(outfile)\n",
    "else:\n",
    "    master_event_catalog = 'catalog_all_original.csv'\n",
    "    dfall = build_master_event_catalog(SEISAN_DATA, DB, master_event_catalog, subclasses_for_ML)\n",
    "\n",
    "# how many events of each type do we want?\n",
    "_count_by_subclass(dfall)\n",
    "choice = input('How many events of each subclass do you want in training+test datasets?')\n",
    "N = int(choice)\n",
    "\n",
    "iterate_again = False # changed this back to do the loop\n",
    "while iterate_again:\n",
    "\n",
    "    # get/update the fingerprints of each event class\n",
    "    #fingerprints = get_fingerprints(dfall, SUBCLASSES, N=300, exclude_checked=False)  \n",
    "    fingerprints = get_weighted_fingerprints(dfall, subclasses_for_ML, N=300, exclude_checked=False)\n",
    "\n",
    "    # for further classification select best unchecked events of each subclass based on quality_index\n",
    "    best_events_dict = _select_best_events(dfall, subclasses_for_ML, N=N, exclude_checked=True)\n",
    "    \n",
    "    # manually QC the data\n",
    "    best_events_dict = qc_best_events(best_events_dict, seisan_subclasses, subclasses_for_ML, fingerprints)\n",
    "    \n",
    "    # save and summarize the data\n",
    "    allbest = _merge_dataframes(best_events_dict, best_events_dict.keys())\n",
    "    allbest.sort_index(inplace=True)\n",
    "    dfall.sort_index(inplace=True)\n",
    "    dfall.update(allbest)    \n",
    "    dfall.to_csv(outfile)\n",
    "    _count_by_subclass(dfall)\n",
    "    \n",
    "    choice = input('Do you want to iterate again (y/n)?')\n",
    "    if choice[0]=='n':\n",
    "        iterate_again=False\n",
    "\n",
    "\n",
    "dfchecked = dfall[dfall['checked']==True]\n",
    "dfchecked = dfchecked[dfall['delete']==False]\n",
    "dfchecked = dfchecked[dfall['ignore']==False]\n",
    "dfchecked = dfchecked[dfall['split']==False]\n",
    "\n",
    "aaa_infile = 'aaa_labelled_events.csv'\n",
    "to_AAA(dfchecked, subclasses_for_ML, aaa_infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
