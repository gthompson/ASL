{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some tools for tooling around with the dfall dataframe\n",
    "The aim of this code is to find the best N events of each type, and create a corresponding CSV file and data structure for entry into Alexis' and Marielle's AAA codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we plot the first 1-minute of each selected event file, padding if needed\n",
    "# We do this mainly to see the variable onset time of signals within event files\n",
    "import pandas as pd\n",
    "aaa_infile = 'aaa_labelled_events.csv' \n",
    "dfAAA = pd.read_csv(aaa_infile)\n",
    "for i, row in dfAAA.iterrows():\n",
    "    st = read(row['path'])\n",
    "    st = st.select(station='MBLG', component='Z')\n",
    "    if len(st):\n",
    "        stime = st[0].stats.starttime\n",
    "        st.trim(starttime=stime,endtime=stime+60,pad=True,fill_value=0)\n",
    "        st.plot();\n",
    "        print('subclass = %s' % row['class'])\n",
    "        #dummy = input('ENTER to see next signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if Pickle files are corrected\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from obspy.core import read, UTCDateTime\n",
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "df = dfall.copy()\n",
    "df.sort_values(by=['trigger_duration'],ascending=False)\n",
    "for i,row in df.iterrows():\n",
    "    abpath =row['path'].replace('./WAV', '/Users/thompsong/DATA/MVO/PICKLE') + '.pickle'\n",
    "    st = read(abpath)\n",
    "    for tr in st:\n",
    "        print(tr.stats)\n",
    "    st.plot()\n",
    "    dummy = input('ENTER to see next event, or q to quit')  \n",
    "    if dummy=='q':\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if there is a relationship between detection window length and length of file for different subclasses\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from obspy.core import read, UTCDateTime\n",
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "df = dfall.copy()\n",
    "for subclass in ['r','e','l','h','t']:\n",
    "    df0 = df[df['new_subclass']==subclass]\n",
    "    df0 = df0[df0['trigger_duration']>0]\n",
    "    #print(df_subclass.columns)\n",
    "    print(subclass)\n",
    "    print(df0[['twin','trigger_duration']].describe())\n",
    "    for i,row in df0.iterrows():\n",
    "        abpath =row['path'].replace('./', '/Users/thompsong/DATA/MVO/')\n",
    "        st = read(abpath)\n",
    "        st = st.select(station='MBLG', component='Z')\n",
    "        \n",
    "        if len(st)==0:\n",
    "            st = st.select(station='MBWH', component='Z')\n",
    "        if len(st)==0:\n",
    "            tr = st[0]\n",
    "            st = Stream()\n",
    "            st.append(tr)\n",
    "        #st.plot(equal_scale=False)\n",
    "        st.normalize()\n",
    "        plt.plot(st[0].times(), st[0].data)\n",
    "        ontime = UTCDateTime.strptime(row['ontime'], format='%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "        offtime = UTCDateTime.strptime(row['offtime'], format='%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "        filetime = UTCDateTime.strptime(row['filetime'], format='%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "        plt.vlines([ontime-filetime, offtime-filetime],-1,1,'r')\n",
    "        plt.ylabel(st[0].id)\n",
    "        plt.title('subclass = %s ' % subclass)\n",
    "        plt.xlabel('Time (s)')\n",
    "        \n",
    "        plt.show()\n",
    "        dummy = input('ENTER to see next event, or q to quit')  \n",
    "        if dummy=='q':\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the events that I have marked for splitting\n",
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "df = dfall.copy()\n",
    "df = df[df['split']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.tips()\n",
    "fig = px.box(df, x='time', y='total_bill')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "df = dfall.copy()\n",
    "df = df[df['checked']==True]\n",
    "df = df[df['ignore']==False]\n",
    "df = df[df['delete']==False]\n",
    "df = df[df['split']==False]\n",
    "frames = []\n",
    "for subclass in ['R', 'r', 'e', 'l', 'h', 't']:\n",
    "    dfs = df[df['new_subclass']==subclass]\n",
    "    print(subclass, len(dfs.index))\n",
    "    frames.append(dfs)\n",
    "newdf = pd.concat(frames)\n",
    "for index, row in newdf.iterrows():\n",
    "    print(row[['filetime', 'subclass', 'R', 'r', 'e', 'l', 'h', 't', 'new_subclass']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing that the input parsing works\n",
    "df = dfall.copy()\n",
    "\n",
    "new_subclass = 't, 50, h, 49, 6'\n",
    "spl = new_subclass.split(',') # split string to subclass probability list \n",
    "if len(spl) % 2 == 1:\n",
    "    weight = int(spl.pop())\n",
    "    print(weight)\n",
    "spd = {spl[a].strip():spl[a + 1] for a in range(0, len(spl), 2)} # subclass probability dict\n",
    "print(spd)\n",
    "print(spd.keys())\n",
    "for key in subclasses_for_ML:\n",
    "    if key in spd.keys():\n",
    "        val = int(spd[key])\n",
    "    else:\n",
    "        val = 0\n",
    "    print(key, val)\n",
    "keymax = max(spd, key=spd.get)\n",
    "print('new_subclass = ',keymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am guessing a lot of the e and R events are being ignored because of the 60-s length limit.\n",
    "# Remove that limit.\n",
    "# So just look for checked events that are ignored and have a length of twin>60. And turn ignore to False.\n",
    "df = dfall.copy()\n",
    "df = df[df['checked']==True]\n",
    "df = df[df['ignore']==True]\n",
    "df = df[df['delete']==False]\n",
    "df = df[df['split']==False]\n",
    "df = df[df['twin']>=60.0]\n",
    "print(df.groupby('new_subclass').size())\n",
    "df['ignore']=False\n",
    "dfall2 = dfall.copy()\n",
    "dfall2.update(df)\n",
    "print(dfall2[dfall2['checked']==True].groupby('new_subclass').size())\n",
    "print(dfall2.iloc[0])\n",
    "\n",
    "dfall2.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipping unchecked events of length>60 from ignored to not ignored\n",
    "df = dfall.copy()\n",
    "df = df[df['checked']==False]\n",
    "df = df[df['ignore']==True]\n",
    "df = df[df['twin']>=60.0]\n",
    "print(df.groupby('new_subclass').size())\n",
    "df['ignore']=False\n",
    "dfall2 = dfall.copy()\n",
    "dfall2.update(df)\n",
    "print(dfall2[dfall2['checked']==False].groupby('new_subclass').size())\n",
    "#print(dfall2.iloc[0])\n",
    "\n",
    "dfall2.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I also need to apply a 100 year correction to any data from 1901! print(dfall.iloc[0])\n",
    "# map is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17496\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2cee76aa1694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdfall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/AAA/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   6715\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6717\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   6718\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6719\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/AAA/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m    561\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/AAA/lib/python3.8/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "outfile = 'catalog_all.csv'\n",
    "dfall = pd.read_csv(outfile)\n",
    "print(len(dfall.index))\n",
    "print(dfall.groupby('subclass').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
