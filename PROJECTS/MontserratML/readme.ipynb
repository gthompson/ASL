{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba54429-ada7-4116-8068-4e17e68b0974",
   "metadata": {},
   "source": [
    "# Preparing a set of labelled events from the Montserrat seismic event catalog.\n",
    "\n",
    "## Overview\n",
    "\n",
    "To conduct supervised machine learning, we need to prepare a set of labelled events. The Montserrat seismic event catalog is stored as a Seisan database, but event classifications are unreliable, which is the main motivation for attempting supervised learning. Since a Seisan event database is awkward to work with, we first convert it to a more convenient format for Python. We take 3 main steps:\n",
    "\n",
    "1. Convert Seisan database to ObsPy & pandas friendly formats. \n",
    "2. Compute metrics on each waveform.\n",
    "3. Reclassify events to build a list of events with verified classifications.\n",
    "\n",
    "## Converting Seisan database to ObsPy and Pandas\n",
    "\n",
    "This includes:\n",
    "\n",
    "1. Converting Seisan waveform (WAV) files to MiniSEED.\n",
    "2. Seisan S-files to ObsPy Catalog objects and CSV files. S-files are in __[Nordic format](http://isc-mirror.iris.washington.edu/standards/nordic/)__\n",
    "3. Seisan calibration and RESP files to stationXML.\n",
    "    \n",
    "`01_seisandb_classifications`: translates VOLCANO.DEF if it exists.\n",
    "\n",
    "`02_index_seisandb`: Create two indexes. One that maps all Sfiles to their corresponding WAVfiles, indicating if they exist. And another mapping all WAVfiles to their corresponding S-files. This is trickier because WAV files might not have the same time as the S-file. But the goal should be that we can easily identify S-files without existing WAV files, and vice versa. The index file creator can also detect the presence of other files including PNG files, PICKLE files, corrected/enhanced MSEED files, and trace level and event level CSV files. 5. Implement an index for each event in the Seisan database. This should log every Sfile, every WAV file, and have links to and from each. Thus, when any WAV file is examined, the corresponding Sfile can be found. And vice versa. It should also track the existence of other products, like log files, PNG files, Pickle files, CSV files, and track whether an event has been reviewed, and its corresponding classification percentages and weight.index the S-files and WAV files in both directions (sfile_index.csv). the S-file index file contains paths for up to 2 WAV files (when data are corrected, these links are updated) and bools that indicate if they exist, and columns for the original mainclass and subclass and a columns for each acceptable mainclass and subclass (the latter from volcano_def.csv) and a corresponding percentage probability, a weight, and a checkbox marking if the event has been reclassified, and another checkbox if it needs to be split.  and create a separate index of calibration files (wavfile_index.csv). finally, make an index of all traceIDs found in WAV files, calibration files, and S-files (originalTraceID_index.csv). Since those in the calibration files and particularly S-files are not fully qualified NSLC, these should be selected from the available traceIDs from the WAV file. Also record the earliest and latest occurrence of each traceID. a separation index of all mainclass/subclass is also given, with a count of each. this can be optionally restricted to just S-files that map to a WAV file. it should be easy to see which WAV files have not been registered, or which S-files have no linked or existing WAV files.\n",
    "\n",
    "`03_seisandb2counts`: produce counts of how many times each traceID appears as a function of time (events). also how many S-files have no corresponding WAV-files, and vice versa.\n",
    "\n",
    "`04_seisandb_fixtraceid`. translate all traceIDs in the index using a given function. this updates originalTraceID_index.csv. After execution, this file should be manually checked and can be hand edited before it is applied in later steps.\n",
    "\n",
    "`05_qc_seisanwav`: quality check the WAV files in the seisan DB. Optionally apply this to linked WAV files only, and/or certain traceIDs only. update the index files. There is no output - Trace.stats.metrics are updated, but Trace objects can optionally be saved to Pickle files, or Trace.stats.metrics can be saved to traceCSV files.\n",
    "\n",
    "`06_seisancal2stationxml`: attempt to convert all Seisan calibration and RESP files in the CAL directory to stationXML. TraceIDs will be fixed according to originalTraceID_index.csv. Then assemble them into either a single stationXML file, or one per trace id, stored back in the CAL directory. create a new index fixedTraceID_index.csv that maps each fixed TraceID to the corresponding stationXML file. \n",
    "\n",
    "`07_correct_seisandb`: attempt to apply instrument corrections from stationXML files to translate traces from WAV files. by default, omit any traces that have quality=0. by default, only retain corrected traces. optionally save them as MSEED or Pickle files. update the standard Miniseed metrics.\n",
    "\n",
    "## Computing metrics\n",
    "10_seisandb_ampengfft: compute various amplitude, energy and frequency metrics. this updates Trace.stats.metrics.\n",
    "11_seisandb_detectevents: optionally detect events within each WAV file.\n",
    "12_seisandb_plot_seismograms: optionally create PNG files of corrected seismic and infrasound traces.\n",
    "13_seisandb_plot_spectrograms: optionally create PNG files of corrected seismic and infrasound spectrograms.\n",
    "    \n",
    "## Selecting best events\n",
    "The following steps are undertaken to improve the original manual classifications to prepare the Seisan database for supervised machine learning. These steps are necessary because the original event classifications are unreliable, classification is difficult so it makes more sense to apply a probability for each event class, and each event file may also contain several events, or may be garbage. So what we do is iteratively select the top N events of each class, manually reclassify them giving percentage probabilities for each class (while marking bad events for deleted, or multi-event files for splitting). We can also generate/update fingerprints for each class, which can help guide the manual classification, or be used to automatically classify events.\n",
    "    \n",
    "60_select_events: select the top N events of each class or accepted volcano subclass. A file called volcano_def.csv is required for the latter to be available. By default, only unchecked events are loaded. To choose the top unchecked events, the quality is used. Optionally, checked events can be picked instead. To choose the top checked events, the probability is used, and a minimum threshold given (default: 50%). In either case, a set of class fingerprints is generated. These are stored in two CSV files: checked_fingerprints.csv and unchecked_fingerprints.csv.\n",
    "\n",
    "    function fingerprint_events: using the selected events, compute fingerprints. for unchecked events, the original subclass is used. for checked events, the probabilistic subclasses are used and weighted using the weight column (see 61).\n",
    "    \n",
    "61_reclassify_events_manually: manually (re)classify the selected events. a probability can be given for each mainclass or accepted volcano subclass. a weight can also be given, indicating how clear the signal is. or the event can be marked for splitting because it is judged to contain multiple events. All processed events are marked as checked unless skipped.\n",
    "\n",
    "62_reclassify_events_with_fingerprints: automatically reclassify the selected events. the probability for each class will be the result of comparing to event metrics to the fingerprints.   \n",
    "\n",
    "63_write_AAA_input_file: looks through the entire sfile_index.csv, extracts only the top probability N events of each mainclass/subclass that have been checked (and not marked for splitting or deletion). Not sure how to use the weight. Or does it just use the selected events?\n",
    "\n",
    "For selecting events:\n",
    "1. Enable user to manually split event files into separate events.\n",
    "2. Enable \"noise\" and \"unknown\" classifications. Implement a \"weight\" for each classification, from 0-9. Unchecked events are given a weight of 3. Very good examples can be given a weight of 9 and therefore weighted more strongly in the fingerprints.\n",
    "3. Enable user to assign a probability to each classification. Keep asking for further classifications until probability reaches 1.0 (including unknown and noise).\n",
    "4. \n",
    "- (a) Implement classification using single characters as 'T'='teleseism', 'R'='regional', 'r'='rockfall/PDC', 'e'='long-period + rockfall', 'l'='long-period', 'h'='hybrid', 'm'='tremor', 't'='volcano-tectonic', 'n'='noise', 'u'='unknown'. \n",
    "- (b) A classification code should be followed by ', p' where p is a percentage (an integer from 1-100). If omitted, percentage defaults to 100.\n",
    "- (c) A percentage should be followed by ', w' where w is a weight. If omitted, weight defaults to 3. \n",
    "- (d) To accept an existing classification with probability 100% and weight 3 <ENTER>. \n",
    "- (e) To accept the probabilities from the fingerprints, enter 'f'.\n",
    "- (f) To split an event, 's' (do not enter a percentage or weight). This will open a separate window to clip out each event.\n",
    "- (g) If an event is garbage, 'd' should delete it and all corresponding files, including from the indexes.\n",
    "- (h) 'k' skips to the next event.\n",
    "    \n",
    "Valid entries:\n",
    "    'h' # 100% hybrid, weight 3\n",
    "    'h, 100' # 100% hybrid, weight 3\n",
    "    'h, 100, 9' # 100% hybrid, weight 9\n",
    "    'h, 50, l, 50' # 50% hybrid, 50% long-period, weight 3\n",
    "    's' # mark for splitting\n",
    "    'd' # mark for deletion\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "## Problems with the Montserrat seismic event catalog:\n",
    "1. WAV files often contain multiple events.\n",
    "2. Events are often not clearly identifiable as any event class.\n",
    "3. Events can appear to be candidates for two or more event classes.\n",
    "4. There are a lot of events to process.\n",
    "\n",
    "## Solutions - implement eev.py\n",
    "1. Make processing easy.\n",
    "2. Allow user to skip to next event, or return to previous event.\n",
    "\n",
    "## To do\n",
    "Testing:\n",
    "    \n",
    "- Test whether a pickle file is larger than a corresponding Miniseed and pickle file just containing metrics. Or can metrics all be stored in a CSV, using tracemetrics2csv and csv2tracemetrics? I could then wrap this with libseisGT.StreamWrite and libseisGT.StreamRead.\n",
    "- Test whether writing each trace to a separate Miniseed file, and loading them again, is significantly slower than saving a whole stream to miniseed and loading it again.\n",
    "- Troubleshoot why traces are not being corrected.\n",
    "- Allow ampengfft and spectrograms to be computed on corrected traces\n",
    "- A separate process needs to be divised for splitting marked events. Run processWAV to get updated metrics.\n",
    "\n",
    "The event-level CSV file could track:\n",
    "- Sfile path\n",
    "- percentages for each classification\n",
    "- weight\n",
    "- overall Stream metrics, averaged from trace metrics\n",
    "- WAV DSN path\n",
    "- WAV ASN path\n",
    "- For each trace in each WAV:\n",
    " - trace ID\n",
    " - trace quality\n",
    " - trace weight\n",
    " - trace deleted (if quality or weight are zero)\n",
    " - other trace metrics\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ceaa1-808e-4fb9-80bd-8d897bdbe7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
