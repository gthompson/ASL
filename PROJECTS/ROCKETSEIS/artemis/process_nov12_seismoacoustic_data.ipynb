{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9deb14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generically plot rocket launch data, given start and end dates\n",
    "# Base start and end times on XLS file - including whether landers\n",
    "# Assume data are in SDS archive\n",
    "# Need a consistent way to store stationXML files too - currently in Fieldwork/Response folder\n",
    "# Try to download any additional RShake data available, just in case\n",
    "# Could also try to download any Florida seismic stations not at KSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad61110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read event times\n",
    "import os, obspy\n",
    "import pandas as pd\n",
    "HOME = os.getenv('HOME')\n",
    "xlsdir = os.path.join(HOME, 'Dropbox', 'PROFESSIONAL', 'RESEARCH', \\\n",
    "                        '3_Project_Documents', 'NASAprojects', \\\n",
    "                        '201602 Rocket Seismology', 'tracking_launches_recorded')\n",
    "xlsfile = os.path.join(xlsdir, 'All_KSC_Rocket_Launches.xlsx')\n",
    "df = pd.read_excel(xlsfile, sheet_name='launches') #, parse_dates=['Date','Time'])\n",
    "#print(df.columns)\n",
    "\n",
    "# Process dataframe\n",
    "df = df[['Date', 'Time', 'Rocket_Payload','SLC','Stations Recorded','Returned to KSC']] # subset to critical columns\n",
    "df['SLC'] = df['SLC'].astype(str) # turn numeric SLC like '40' to string\n",
    "df['DateTimeString'] = df['Date'].astype(str) + 'T' + df['Time'].astype(str)\n",
    "df['DateTime'] = pd.to_datetime(df['DateTimeString'])\n",
    "#df.info()\n",
    "\n",
    "# Choose event\n",
    "print(\"Select event: \")\n",
    "c=1\n",
    "for c, launchtime in enumerate(df['DateTime']):\n",
    "    print(c, launchtime)\n",
    "choice = int(input(\"\\nChoice ?  \"))\n",
    "\n",
    "launchtime = obspy.UTCDateTime(df.iloc[choice]['DateTime'])\n",
    "print('You chose ', launchtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from SDS archive for this event\n",
    "import os, glob, obspy, sys, datetime\n",
    "print('Current working directory is %s' % os.getcwd())\n",
    "HOME = os.getenv('HOME')\n",
    "sys.path.append(os.path.join(HOME, 'src','kitchensinkGT','LIB'))\n",
    "from libseisGT import read_sds\n",
    "\n",
    "# SDS_TOP is path where YYYY directories reside. SDS directory structure is like YYYY/NET/STATION/CHANNEL.D\n",
    "# Check if Lacie DATA drive connected. Otherwise, try Dropbox\n",
    "SDS_TOP = '/Volumes/DATA/SDS'\n",
    "if not os.path.isdir(SDS_TOP):\n",
    "    SDS_TOP = os.path.join(HOME, 'Dropbox', 'DATA', 'SDS')\n",
    "print('Looking for data at: ', SDS_TOP)\n",
    "print(os.listdir(SDS_TOP))\n",
    "#startt = obspy.UTCDateTime(2022,11,12,16,6,0)\n",
    "launchtime = obspy.UTCDateTime(launchtime)\n",
    "print(launchtime)\n",
    "st = read_sds(SDS_TOP, launchtime-60, launchtime+120, skip_low_rate_channels=True, nslc_tuples=None )\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3bdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct data\n",
    "import USF_instrument_responses as USF\n",
    "USF.correctUSFstations(st)\n",
    "for tr in st:\n",
    "    print(tr.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCAFFOLD\n",
    "# Get origin lat/lon by looking up SLC coordinates from dataframe\n",
    "#olat = (28 + 37/60 + 38/3600) \n",
    "#olon = -(80 + 37/60 + 15/3600)\n",
    "# Compute distances by looking up station coordinates from inventories\n",
    "distances={'MTEGL': 1500, 'S39A1': 3080, 'BCHH2': 7500, 'BCHH4': 7500, 'R1E5E': 12800}\n",
    "from libTonga import get_distance_vector, order_traces_by_distance\n",
    "for tr in st:\n",
    "    tr.stats.distance = distances[tr.stats.station]\n",
    "r = get_distance_vector(st)\n",
    "st = order_traces_by_distance(st, r, assert_channel_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(HOME, 'src','GitHub','icewebPy'))\n",
    "import IceWeb\n",
    "TOPDIR = os.path.join(HOME, 'WORK')\n",
    "#DATADIR = os.path.join(TOPDIR, 'DATA')\n",
    "#XMLDIR = os.path.join(TOPDIR, 'stationXML')\n",
    "FIGDIR = os.path.join(TOPDIR, 'FIGURES')\n",
    "if not os.path.isdir(FIGDIR):\n",
    "    os.makedirs(FIGDIR)\n",
    "\n",
    "# Plot infrasound\n",
    "infrasound_st = st.select(channel='HD?') \n",
    "infrasound_st.plot(equal_scale=True, outfile=os.path.join(FIGDIR,'infrasound.png'))\n",
    "infrasound_st.plot(outfile=os.path.join(FIGDIR,'record_section_infrasound.png'), type='section', orientation='horizontal', norm_method='stream')\n",
    "#infrasound_st.write(INFRAfile, 'pickle')\n",
    "spobj_infrasound = IceWeb.icewebSpectrogram(stream=infrasound_st)\n",
    "sgramfile = os.path.join(FIGDIR,'specgram_infrasound_unscaled.png')\n",
    "spobj_infrasound.plot(outfile=sgramfile, dbscale=True, equal_scale=False, fmax=50.0, add_colorbar=False)\n",
    "print('infrasound plotted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic_VEL_st = st.select(channel='?H?')\n",
    "seismic_VEL_st.plot(equal_scale=True, outfile=os.path.join(FIGDIR,'seismic_VEL.png'))\n",
    "seismic_VEL_st.select(component='Z').plot(outfile=os.path.join(FIGDIR,'record_section_seismic_VEL_Z.png'), type='section', orientation='horizontal', norm_method='stream')\n",
    "#seismic_VEL_st.write(VELfile, 'pickle')\n",
    "spobj_seismic_VEL = IceWeb.icewebSpectrogram(stream=seismic_VEL_st.select(component='Z'))\n",
    "sgramfile = os.path.join(FIGDIR,'specgram_seismic_VEL_scaled.png')\n",
    "spobj_seismic_VEL.plot(outfile=sgramfile, dbscale=True, equal_scale=True, fmax=50.0, add_colorbar=False)\n",
    "print('seismic velocity plotted - verticals only')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762dc42a",
   "metadata": {},
   "source": [
    "# CRAP\n",
    "Everything below here is junk code that needs sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e793e1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# libraries and paths\n",
    "import os, glob, obspy, sys, datetime\n",
    "print('Current working directory is %s' % os.getcwd())\n",
    "HOME = os.getenv('HOME')\n",
    "sys.path.append(os.path.join(HOME, 'src','kitchensinkGT','LIB'))\n",
    "import USF_instrument_responses as USF\n",
    "from libTonga import plot_record_section, order_traces_by_id, get_distance_vector, order_traces_by_distance\n",
    "sys.path.append(os.path.join(HOME, 'src','GitHub','icewebPy'))\n",
    "import IceWeb\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams[\"figure.figsize\"] = [19.2, 10.8]\n",
    "#mpl.rcParams[\"figure.autolayout\"] = True\n",
    "#mpl.rc('font', size=24)\n",
    "from libseisGT import inventory2traceid, get_FDSN_inventory, get_FDSN_Stream, read_sds\n",
    "\n",
    "# SDS_TOP is path where YYYY directories reside. SDS directory structure is like YYYY/NET/STATION/CHANNEL.D\n",
    "# Check if Lacie DATA drive connected. Otherwise, try Dropbox\n",
    "SDS_TOP = '/Volumes/DATA/SDS'\n",
    "if not os.path.isdir(SDS_TOP):\n",
    "    SDS_TOP = os.path.join(HOME, 'Dropbox', 'DATA', 'SDS')\n",
    "end\n",
    "\n",
    "# new functions\n",
    "def summarize_st(st):\n",
    "    print('Summarizing')\n",
    "    for tr in st:\n",
    "        print('%s, %10.0f counts/(%s), %e %s p2p, %7.0f m' % \\\n",
    "              (tr.id, tr.stats.sensitivity, tr.stats.units, \\\n",
    "               max(tr.data)-min(tr.data), tr.stats.units, tr.stats.distance)  )          \n",
    "\n",
    "\n",
    "\n",
    "startt = obspy.UTCDateTime(2022,11,12,16,6,0)\n",
    "st = sds2stream(SDS_TOP, startt-60, startt+120 )\n",
    "\n",
    "print(st)\n",
    "\n",
    "st.select(channel='H??').plot(equal_scale=False)\n",
    "\n",
    "SDS_TOP=\".\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df78f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def correct_local_rshake_data(trace_id):\n",
    "    (net, sta, loc, chan) = trace_id.split('.')\n",
    "    sdsfile = os.path.join(DATADIR, '%s.D.2022.320' % trace_id)\n",
    "    xmlfile = os.path.join(XMLDIR, '%s.xml' % sta)\n",
    "    correctedfile = os.path.join(DATADIR, '%s.D.2022.320_corrected.pkl' % trace_id)\n",
    "    inv = None\n",
    "    try:\n",
    "        if os.path.isfile(xmlfile):\n",
    "            inv = obspy.read_inventory(xmlfile)\n",
    "        else:\n",
    "            inv = obspy.read_inventory('https://fdsnws.raspberryshakedata.com/fdsnws/station'\\\n",
    "                '/1/query?network=%s&station=%s&level=resp&format=xml' % (net, sta))\n",
    "            inv = get_FDSN_inventory(fdsnClient, row['time'], stationXmlFile, NETWORK, \\\n",
    "                LATITUDE, LONGITUDE, searchRadiusDeg, PRE_TRIGGER_SECS, POST_TRIGGER_SECS )\n",
    "    startt = row['time'] - PRE_TRIGGER_SECS\n",
    "    endt = row['time'] + POST_TRIGGER_SECS    \n",
    "        stR = get_FDSN_Stream(fdsnClient, trace_ids, rawfile, startt, endt )            \n",
    "            inventory.write(xmlfile, format='STATIONXML')\n",
    "    except:\n",
    "        print('No inventory found online or at %s' % xmlfile)\n",
    "    else:\n",
    "        st = obspy.read(sdsfile)\n",
    "        st.attach_response(inv)\n",
    "        st.filter('highpass', freq=0.5, corners=2, zerophase=True)\n",
    "        st = st.remove_response()\n",
    "        st.trim(starttime=stime,endtime=etime);\n",
    "        st.merge(fill_value=0)\n",
    "        for tr in st:\n",
    "            print('Data for %s already corrected' % tr.id)\n",
    "            tr.stats.corrected = True # this will prevent gain being reapplied later            \n",
    "        print('Writing %s' % correctedfile)\n",
    "        st.write(correctedfile, 'pickle')\n",
    "\n",
    "        \n",
    "# Constants\n",
    "olat = (28 + 37/60 + 38/3600) \n",
    "olon = -(80 + 37/60 + 15/3600)\n",
    "stime = obspy.UTCDateTime(2022,11,12,16,6,0)\n",
    "etime = stime + 360\n",
    "distances={'MTEGL': 1500, 'S39A1': 3080, 'BCHH2': 7500, 'BCHH4': 7500, 'R1E5E': 12800}\n",
    "TOPDIR = os.path.join(HOME, 'Movies', 'Artemis')\n",
    "SDS_TOP=\n",
    "DATADIR = os.path.join(TOPDIR, 'DATA')\n",
    "XMLDIR = os.path.join(TOPDIR, 'stationXML')\n",
    "FIGDIR = os.path.join(TOPDIR, 'FIGURES')\n",
    "\n",
    "# Original data files - Note: R1E5E_fixed.mseed has already been instrument corrected \n",
    "rshake_trace_id = 'AM.R1E5E.00.EHZ'\n",
    "rshake_file = os.path.join(DATADIR, '%s.D.2022.320_corrected.pkl' % rshake_trace_id)\n",
    "if not os.path.isfile(rshake_file):\n",
    "    correct_local_rshake_data(rshake_trace_id)    \n",
    "USFrawfiles = [os.path.join(DATADIR,'S39A1.seed'), os.path.join(DATADIR,'BCHH2.seed'), \\\n",
    "               os.path.join(DATADIR,'BCHH4.seed'), rshake_file] #'../DATA/R1E5E_fixed.mseed']\n",
    "paul_minute_files = glob.glob(os.path.join(DATADIR,'paul/*.ms'))\n",
    "paul_rawfile = os.path.join(DATADIR,'XA.MTEGL.pkl')\n",
    "USF_rawfile = os.path.join(DATADIR, 'USF_raw.pkl')\n",
    "\n",
    "# Processed\n",
    "USF_correctedfile = os.path.join(DATADIR, 'USF_corrected.pkl')\n",
    "VELfile = os.path.join(DATADIR, 'ALL_seismic_VEL.pkl')\n",
    "ACCfile = os.path.join(DATADIR, 'ALL_seismic_ACC.pkl')\n",
    "INFRAfile = os.path.join(DATADIR, 'ALL_infrasound.pkl')\n",
    "\n",
    "print('Setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3fcc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read RSHAKE data & correct it\n",
    "from obspy.clients.fdsn import Client\n",
    "rs = Client(base_url='https://fdsnws.raspberryshakedata.com/') \n",
    "\n",
    "def correct_local_rshake_data(trace_id):\n",
    "    (net, sta, loc, chan) = trace_id.split('.')\n",
    "    sdsfile = os.path.join(DATADIR, '%s.D.2022.320' % trace_id)\n",
    "    xmlfile = os.path.join(XMLDIR, '%s.xml' % sta)\n",
    "    correctedfile = os.path.join(DATADIR, '%s.D.2022.320_corrected.pkl' % trace_id)\n",
    "    inv = None\n",
    "    try:\n",
    "        if os.path.isfile(xmlfile):\n",
    "            inv = obspy.read_inventory(xmlfile)\n",
    "        else:\n",
    "            inv = obspy.read_inventory('https://fdsnws.raspberryshakedata.com/fdsnws/station'\\\n",
    "                '/1/query?network=%s&station=%s&level=resp&format=xml' % (net, sta))\n",
    "            inventory.write(xmlfile, format='STATIONXML')\n",
    "    except:\n",
    "        print('No inventory found online or at %s' % xmlfile)\n",
    "    else:\n",
    "        st = obspy.read(sdsfile)\n",
    "        st.attach_response(inv)\n",
    "        st.filter('highpass', freq=0.5, corners=2, zerophase=True)\n",
    "        st = st.remove_response()\n",
    "        st.trim(starttime=stime,endtime=etime);\n",
    "        st.merge(fill_value=0)\n",
    "        for tr in st:\n",
    "            print('Data for %s already corrected' % tr.id)\n",
    "            tr.stats.corrected = True # this will prevent gain being reapplied later            \n",
    "        print('Writing %s' % correctedfile)\n",
    "        st.write(correctedfile, 'pickle')\n",
    "\n",
    "# set the station name and download the response information\n",
    "trace_ids = ['AM.R1E5E.00.EHZ']        \n",
    "for trace_id in trace_ids: \n",
    "    correct_local_rshake_data(trace_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4161bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw SEED files from USF stations\n",
    "USF_st = obspy.Stream()\n",
    "if os.path.exists(USF_rawfile):\n",
    "    USF_st = obspy.read(USF_rawfile)\n",
    "else:\n",
    "    for rawfile in USFrawfiles:\n",
    "        if os.path.isfile(rawfile):\n",
    "            if rawfile[-3:]=='pkl':\n",
    "                this_st = obspy.read(rawfile, 'pickle')\n",
    "            else:\n",
    "                this_st = obspy.read(rawfile, 'mseed')\n",
    "            this_st.merge(fill_value=1)\n",
    "            for tr in this_st:\n",
    "                #print(tr.id)\n",
    "                #if tr.stats.network == 'AM':\n",
    "                #    print('Data for %s already corrected' % tr.id)\n",
    "                #    tr.stats.corrected = True # this will prevent gain being reapplied later\n",
    "                USF_st.append(tr)\n",
    "    USF_st.write(USF_rawfile, 'pickle') \n",
    "print('USF seismic data loaded/written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb506626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply calibration corrections - but would be better to create full inventory stationXML files\n",
    "USF_st = obspy.read(USF_rawfile)\n",
    "USF_stc = USF_st.copy()\n",
    "USF.correctUSFstations(USF_stc)            \n",
    "print('USF stations calibration corrected')\n",
    "\n",
    "# For plotting purposes, reset calib to 1. Other ObsPy messes it up.\n",
    "for tr in USF_stc:\n",
    "    tr.stats['sensitivity']=tr.stats.calib\n",
    "    tr.stats.calib = 1.0 # this is just so that plots are not messed up. ObsPy applies tr.stats.calib before plotting\n",
    "    tr.stats.distance = distances[tr.stats.station]\n",
    "for tr in USF_stc:\n",
    "    if tr.stats.network == 'AM':\n",
    "        # RS and RB response files are here https://manual.raspberryshake.org/bru2.html#instrument-response-files\n",
    "        tr.stats.calib = 1.0\n",
    "        if tr.stats.channel[1]=='D':\n",
    "            tr.stats['sensitivity'] = 56000 # counts per Pa, see email 2020/07/01 and https://manual.raspberryshake.org/_downloads/SpecificationsforBoom_SnB.pdf\n",
    "            tr.stats.units = 'Pa'\n",
    "        elif tr.stats.channel[1]=='H':\n",
    "            tr.stats['sensitivity'] = 353090000 # counts per m/s, see https://www.gempageoservices.com/wp-content/uploads/2017/10/Specifications-RaspberryShake-3D.pdf\n",
    "            tr.stats.units = 'm/s' \n",
    "\n",
    "        if not 'corrected' in tr.stats or not tr.stats.corrected: # use tr.stats.corrected = True\n",
    "            print('Correcting %s' % tr.id)\n",
    "            tr.data = tr.data/tr.stats['sensitivity'] \n",
    "            tr.stats.corrected = True\n",
    "r = get_distance_vector(USF_stc)\n",
    "USF_stc = order_traces_by_distance(USF_stc, r, assert_channel_order=True)\n",
    "USF_stc.trim(starttime=stime, endtime=etime)\n",
    "print(USF_stc)            \n",
    "USF_stc.write(USF_correctedfile, 'pickle')                 \n",
    "print('USF data corrected/trimmed/written. Calibs fixed to 1.0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFRASOUND\n",
    "\n",
    "# Remove bad infrasound channels\n",
    "infrasound_st = USF_stc.select(channel='HD?')\n",
    "for tr in infrasound_st.select(id=\"FL.BCHH2.10.HD5\"):\n",
    "    infrasound_st.remove(tr)  \n",
    "for tr in infrasound_st.select(id=\"FL.BCHH4.00.HD3\"):\n",
    "    infrasound_st.remove(tr)  \n",
    "    \n",
    "# Plot infrasound\n",
    "infrasound_st.plot(equal_scale=True, outfile=os.path.join(FIGDIR,'artemis_infrasound.png'))\n",
    "infrasound_st.plot(outfile=os.path.join(FIGDIR,'artemis_record_section_infrasound.png'), type='section', orientation='horizontal', norm_method='stream')\n",
    "infrasound_st.write(INFRAfile, 'pickle')\n",
    "spobj_infrasound = IceWeb.icewebSpectrogram(stream=infrasound_st)\n",
    "sgramfile = os.path.join(FIGDIR,'specgram_infrasound_unscaled.png')\n",
    "spobj_infrasound.plot(outfile=sgramfile, dbscale=True, equal_scale=False, fmax=50.0, add_colorbar=False)\n",
    "print('infrasound plotted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a274ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEISMIC VELOCITY\n",
    "\n",
    "seismic_VEL_st = USF_stc.select(channel='?H?')\n",
    "seismic_VEL_st.plot(equal_scale=True, outfile=os.path.join(FIGDIR,'artemis_seismic_VEL.png'))\n",
    "seismic_VEL_st.select(component='Z').plot(outfile=os.path.join(FIGDIR,'artemis_record_section_seismic_VEL_Z.png'), type='section', orientation='horizontal', norm_method='stream')\n",
    "seismic_VEL_st.write(VELfile, 'pickle')\n",
    "spobj_seismic_VEL = IceWeb.icewebSpectrogram(stream=seismic_VEL_st.select(component='Z'))\n",
    "sgramfile = os.path.join(FIGDIR,'specgram_seismic_VEL_scaled.png')\n",
    "spobj_seismic_VEL.plot(outfile=sgramfile, dbscale=True, equal_scale=True, fmax=50.0, add_colorbar=False)\n",
    "print('seismic velocity plotted - verticals only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAUL's/NASA ACCELEROMETER    \n",
    "    \n",
    "# load Paul's station & make plots\n",
    "if os.path.exists(paul_rawfile):\n",
    "    paulst = obspy.read(paul_rawfile, 'pickle')\n",
    "else:\n",
    "    paulst = obspy.Stream()\n",
    "    for file in paul_minute_files:\n",
    "        paulst = paulst + obspy.read(file, 'mseed')\n",
    "    paulst.merge(fill_value=0)\n",
    "    paulst.write(paul_rawfile, 'pickle')    \n",
    "print('Paul seismic data loaded/written')      \n",
    "\n",
    "# Add a fake calibration to Paul's station. Assumed value\n",
    "paulcalib = 10000000 \n",
    "paulst.trim(starttime=stime, endtime=etime)\n",
    "for tr in paulst:\n",
    "    tr.data = tr.data/paulcalib\n",
    "    tr.calib = 1.0\n",
    "    tr.stats.sensitivity = paulcalib\n",
    "    tr.stats.units = 'm/s^2'\n",
    "    tr.stats.distance = distances[tr.stats.station]\n",
    "print('Paul station gain corrected assuming %d counts per m/s^2' % paulcalib)\n",
    "summarize_st(paulst)\n",
    "paulst.plot(equal_scale=True, outfile=os.path.join(FIGDIR,'artemis_seismic_paul.png'))\n",
    "spobj_paul = IceWeb.icewebSpectrogram(stream=paulst)\n",
    "sgramfile = os.path.join(FIGDIR,'specgram_paul_scaled.png')\n",
    "spobj_paul.plot(outfile=sgramfile, dbscale=True, equal_scale=True, fmax=400.0, add_colorbar=False)\n",
    "print('Paul station spectrogram plotted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCELERATION\n",
    "USF_stc = obspy.read(USF_correctedfile, 'pickle')\n",
    "\n",
    "# differentiate USF velocity channels & add Paul channesl\n",
    "seismic_ACC_st = USF_stc.select(channel='?H?').differentiate()\n",
    "for tr in seismic_ACC_st:\n",
    "    tr.stats.channel = tr.stats.channel[0] + 'N' + tr.stats.channel[2]\n",
    "    tr.stats.units = 'm/s^2'\n",
    "    \n",
    "for tr in paulst:\n",
    "    seismic_ACC_st.append(tr)\n",
    "r = get_distance_vector(seismic_ACC_st)\n",
    "seismic_ACC_st = order_traces_by_distance(seismic_ACC_st, r, assert_channel_order=True)\n",
    "summarize_st(seismic_ACC_st)\n",
    "        \n",
    "# plot and save\n",
    "seismic_ACC_st.plot(equal_scale=True, outfile=os.path.join(FIGDIR,'artemis_seismic_ACC.png'))\n",
    "seismic_ACC_st.select(component='Z').plot(outfile=os.path.join(FIGDIR,'artemis_record_section_seismic_ACC_Z.png'), type='section', orientation='horizontal', norm_method='stream')\n",
    "seismic_ACC_st.write(ACCfile, 'pickle')\n",
    "spobj_seismic_ACC = IceWeb.icewebSpectrogram(stream=seismic_ACC_st.select(component='Z'))\n",
    "sgramfile = os.path.join(FIGDIR,'specgram_seismic_ACC_scaled.png')\n",
    "spobj_seismic_ACC.plot(outfile=sgramfile, dbscale=True, equal_scale=True, fmax=50.0, add_colorbar=False)\n",
    "print('seismic acceleration plotted - verticals only') \n",
    "\n",
    "mseed_acc = os.path.join(DATADIR,'acceleration.mseed')\n",
    "seismic_ACC_st.select(component='Z').write(mseed_acc, 'mseed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a1f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgramfiletif = os.path.join(FIGDIR,'specgram_seismic_ACC_scaled.tif')\n",
    "spobj_seismic_ACC.plot(outfile=sgramfiletif, dbscale=True, equal_scale=True, fmax=50.0, add_colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9732797",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgramfile = os.path.join(FIGDIR,'specgram_seismic_ACC_scaled.tif')\n",
    "spobj_seismic_ACC.plot(outfile=sgramfiletif, dbscale=True, equal_scale=True, fmax=50.0, add_colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fca264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csvfile = os.path.join(TOPDIR, 'rocketlaunches.csv')\n",
    "\n",
    "df1 = pd.read_csv(csvfile)\n",
    "df1.drop(['Landed', 'Notes', 'Time_UTC'], axis=1, inplace=True)\n",
    "df1 = df1[['Date', 'Rocket_Payload','SLC']]\n",
    "\n",
    "\n",
    "csvfile2 = os.path.join(TOPDIR, 'KSC_rocket_launches_MASTER_2016_2019.csv')\n",
    "df2 = pd.read_csv(csvfile2)\n",
    "df2['Rocket_Payload']=df2['ROCKET TYPE']+'|'+df2['PAYLOAD']\n",
    "df2.drop(['ROCKET TYPE', 'PAYLOAD'], axis=1, inplace=True)\n",
    "\n",
    "df2.rename(columns = {'DATE':'Date'}, inplace = True)\n",
    "\n",
    "df3 = pd.concat([df2, df1], ignore_index=True)\n",
    "df3['Date']=pd.to_datetime(df3['Date'])\n",
    "#print(df3)\n",
    "df3.to_csv(os.path.join(TOPDIR,'All_KSC_Rocket_Launches.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = os.path.join(TOPDIR, 'All_KSC_Rocket_Launches_2016_2022.csv')\n",
    "\n",
    "dfall = pd.read_csv(csvfile)\n",
    "dfall['Date']=pd.to_datetime(dfall['Date'])\n",
    "\n",
    "import datetime\n",
    "def year_fraction(date):\n",
    "    start = datetime.date(date.year, 1, 1).toordinal()\n",
    "    year_length = datetime.date(date.year+1, 1, 1).toordinal() - start\n",
    "    return date.year + float(date.toordinal() - start) / year_length\n",
    "\n",
    "\n",
    "decyear=[]\n",
    "for i in range(len(dfall.index)):\n",
    "    decyear.append(year_fraction(dfall.iloc[i]['Date']))\n",
    "dfall['decyear']=decyear\n",
    "occur = dfall.groupby(['SLC']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fd245",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(occur)\n",
    "#type(occur)\n",
    "df3['cum_count'] = df3.groupby('SLC').cumcount()\n",
    "#print(df3)\n",
    "print(occur.keys())\n",
    "for thisSLC in occur.keys():\n",
    "    df3[thisSLC] = (df['SLC'] == thisSLC).cumsum()\n",
    "print(df3)\n",
    "df3.to_csv(os.path.join(TOPDIR,'All_KSC_Rocket_Launches_cumcount.csv'), index=False)\n",
    "'''\n",
    "\n",
    "for thisSLC in occur.keys():\n",
    "    count = 0\n",
    "    count_list = []\n",
    "    for i in range(len(dfall.index)):\n",
    "        if dfall.iloc[i]['SLC']==thisSLC:\n",
    "            count+=1\n",
    "        count_list.append(count)\n",
    "    dfall[thisSLC]=count_list\n",
    "print(dfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d03450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfall['Other']=dfall['39B']+dfall['46']+dfall['L1011']\n",
    "cols=occur.keys()\n",
    "print(cols)\n",
    "cols = ['37B', '39A', '40', '41', 'Other']\n",
    "#    decyear.append(year_fraction(df3.iloc[i]['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d05578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [19.2, 10.8]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rc('font', size=24)\n",
    "plt.style.use('dark_background')\n",
    "COLOR = 'white'\n",
    "plt.rcParams['text.color'] = COLOR\n",
    "plt.rcParams['axes.labelcolor'] = COLOR\n",
    "plt.rcParams['xtick.color'] = COLOR\n",
    "plt.rcParams['ytick.color'] = COLOR\n",
    "fig = plt.figure(figsize=(19.2,10.8))\n",
    "fig.set_dpi(100)\n",
    "ax = dfall.plot.area(x='decyear',y=cols) # y=occur.keys())\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title('Cumulative number of launches by launch complex')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of launches')\n",
    "plt.savefig(os.path.join(FIGDIR,'cum_launches_opaque.png'), transparent=False, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff02c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(FIGDIR,'cum_launches_opaque.png'), transparent=False, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191fe9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
